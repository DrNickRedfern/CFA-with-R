[{"path":"index.html","id":"computational-film-analysis-with-r","chapter":"Computational Film Analysis with R","heading":"Computational Film Analysis with R","text":"Welcome Computational Film Analysis R.  online, interactive book intended first course application computational methods questions form style cinema using R programming language. aimed previous experience computational film analysis prior knowledge statistics, data science programming R required assumed. chapter discusses underlying methodological concepts range approaches analysing sound, colour, editing, shot types, lexical texts motion pictures depth demonstrates implementation R reading book researchers able design execute computational film analysis research projects.  book includes interactive content user explore, including embedded video files, interactive plots, tables, visualisations. Interactive content identified ‚òùÔ∏è üëà.","code":""},{"path":"index.html","id":"about-the-author","chapter":"Computational Film Analysis with R","heading":"About the author","text":"name Nick Redfern teaching researching film since 2001. taught film, media, television studies Manchester Metropolitan University, University Central Lancashire, Leeds Trinity University. work computational film analysis published Journal Japanese Korean Cinema, Statistica, Journal Data Science, Post Script, Digital Scholarship Humanities, Umanistica Digitale, Sound, Music, Moving Image, Humanities Bulletin, Acta Universitatis Sapientiae, Film Media Studies, Art Perception. can read computational film analysis blog https://computationalfilmanalysis.wordpress.com.  https://orcid.org/0000-0002-7821-2404","code":""},{"path":"index.html","id":"about-this-book","chapter":"Computational Film Analysis with R","heading":"About this book","text":"","code":""},{"path":"index.html","id":"citation","chapter":"Computational Film Analysis with R","heading":"Citation","text":"cite book please use:Redfern, Nick (2022) Computational Film Analysis R (version 0.9.010). https://doi.org/10.5281/zenodo.7074521.Original date Publication: 13 September 2022","code":""},{"path":"index.html","id":"license","chapter":"Computational Film Analysis with R","heading":"License","text":"work licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\ncode book made available MIT license free use research teaching provided book cited source.","code":""},{"path":"index.html","id":"disclaimer","chapter":"Computational Film Analysis with R","heading":"Disclaimer","text":"book features embedded video content purposes education research. infringement copyright intended. use embedded video content taken imply producers way endorse content book.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Computational Film Analysis with R","heading":"Acknowledgements","text":"","code":""},{"path":"index.html","id":"publications","chapter":"Computational Film Analysis with R","heading":"Publications","text":"book draws research previously published asRedfern, N. (2013) Film style narration Rashomon, Journal Japanese Korean Cinema 5 (1‚Äì2): 21‚Äì36. https://doi.org/10.1080/17564905.2013.10820070Redfern, N. (2014) Comparing shot length distributions motion pictures using dominance statistics, Empirical Studies Arts 32 (2): 257‚Äì273. https://doi.org/10.2190/EM.32.2.gRedfern, N. (2014) Quantitative Methods Study Film, University Glasgow, 14-15 May 2014.Redfern, N. (2015) Exploratory data analysis film form: editing structure Friday Thirteenth (1980), Post Script: Essays Film Humanities 34 (2-3): 71‚Äì83.Redfern, N. (2015) time contour plot: graphical analysis film soundtrack. Sound Screen, University West London, 20 November 2015. https://doi.org/10.5281/zenodo.6354341Redfern, N. (2021) Colour palettes US film trailers: comparative analysis movie barcodes, Umanistica Digitale 10: 251-270. http://doi.org/10.6092/issn.2532-8816/12468Redfern, N. (2022) Computational Analysis Film Audio, Manchester Metropolitan University, 28 April 2022. http://doi.org/10.5281/zenodo.6472560Redfern, N. (2022) Analysing motion picture cutting rates, Wide Screen 9 (1): 1-29.","code":""},{"path":"index.html","id":"code","chapter":"Computational Film Analysis with R","heading":"Code","text":"book uses css, html, JavaScript CodeConvey.","code":""},{"path":"preface.html","id":"preface","chapter":"1 Preface","heading":"1 Preface","text":"","code":""},{"path":"preface.html","id":"who-or-what-is-a-filmmaker","chapter":"1 Preface","heading":"1.1 Who (or what) is a filmmaker?","text":"Computer science contributed film production fifty years since digital image processing first used Westworld (Michael Crichton, 1973) pixelate frames point--view gunslinger android. Digital technologies now involved every aspect film production, images sounds captured computers, still call ‚Äòcameras‚Äô ‚Äòrecorders,‚Äô use specialised software writing screenplays, editing, visual effects, music composition. Animated films now produced solely using computers. Films distributed computers streaming services digital cinema packages can read computers theatres (still call ‚Äòprojectors‚Äô). home, audiences watch films computers, may smart televisions, desktop/laptop pcs, tablets, even mobile phones. result half century‚Äôs innovation computer science film industry filmmaking now computer-based art form.application artificial intelligence (AI) tools informed data analysis motion pictures next step technological development cinema.AIs used generate original screenplays films. 2018, Visual Voice utilised IBM‚Äôs Watson suite AI tools applications develop screenplay sixty-second promotional Lexus training neural network fifteen years‚Äô worth luxury adverts, advert directed Kevin Macdonald (Ibrahim, 2018). Benjamin, long short-term memory recurrent neural network machine intelligence trained screenplays science fiction films 1980s 1990s, generated original screenplay Sunspring (2016), experimental short science fiction film directed Oscar Sharp (Video 1.1). 2017, Benjamin collaborated team human writers produce script another short film, ‚Äôs Game (2017); wrote, directed, performed, scored third short, Zone (2018), editing footage public domain films, face-swapping human actors filmmaker‚Äôs database footage, inserting spoken voices read Benjamin‚Äôs script, writing music film (Machkovech, 2018).\nVideo 1.1: Sunspring (2016) science fiction film original script created Benjamin artificial intelligence. ‚òùÔ∏è\nEditing one first filmmaking practices benefit introduction digital technologies EditDroid Lucasfilm released 1984 one key areas automation filmmaking. 2016 producers Morgan, science fiction-horror film ‚Äòhybrid biological organism capacity autonomous decision making sophisticated emotional responses‚Äô, approached IBM support making trailer film using Watson artificial intelligence system. Video 1.2 shows trailer along interviews filmmakers researchers involved. researchers IBM trained model using machine learning trailers 100 horror films segmenting trailers individual scenes performing audio-visual analyses scene order understand associated sentiments aesthetics (e.g., lighting, framing, location, etc.). results used Watson select ten scenes Morgan closely matched features scenes training set trailers. scenes handed filmmaker turned trailer.\nVideo 1.2: Morgan trailer produced IBM. ‚òùÔ∏è\nteam IBM concluded systems ‚Äòeliminated need decision process scenes selected first place taken considerable time effort. expedited process substantially‚Äô (J. R. Smith et al., 2017). However, actual piecing together Morgan trailer done filmmaker made set decisions include trailer, pacing, transitions, aesthetic features. extent, impact data science workflow trailer production significant; impact actual filmmaking process limited.2017, team researchers Stanford University Adobe, developers non-linear editing software Premiere Pro, demonstrated idiomatic editing software capable making aesthetics decisions film edited (Leake et al., 2017). software allows filmmakers build customised editing styles controlling range visual timing parameters (maintaining visibility speaker, avoiding jump cuts, mirroring screen positions actors, intensifying emotion, etc.) determine desired style applied collection takes scene generate dialogue scenes handful seconds. Video 1.3 demonstrates application approach editing scene.\nVideo 1.3: Stanford/Adobe idiomatic editing system. ‚òùÔ∏è\nediting system uses data science variety ways. Computer vision techniques used identify screen positions actors scale shot, OpenFace algorithm employed detect track faces within shots; audio levels modelled using average root mean square energy soundtrack. Edited sequences generated using Hidden Markov Models represent different idioms, maximise likelihood conditional probability distributions set matrices define state space model based parameters selected filmmaker. system also incorporates results data-driven research film style models: example, default settings parameter controlling overlap audio image tracks J- L-cuts incorporated software based Barry Salt‚Äôs research editing practices relationship sounds images (Salt, 2011).problem-solution model film style takes filmmakers rational agents make decisions tell story, symbolically convey abstract themes viewer, emotionally affect viewer, deploy stylistic devices pure decoration, drawing craft traditions preserve favoured sets practices, practices filmmakers can replicate, revise, reject (Bordwell, 2005a: 34-35, 254-257). Innovations style arise filmmakers encounter new problem situations require novel solutions; stylistic continuities stabilised filmmakers must repeatedly solve problems particular solutions prove valuable time. Solutions emerge ‚Äòmetaphysical Kunstwollen drive cyclical historical forces patterns situations artists make decisions result traits works produce‚Äô (Burnett, 2008: 139), resulting form motion picture arising ‚Äòensemble choices‚Äô intended achieve objectives filmmaker (Carroll, 1998). New technologies create new opportunities filmmakers create new craft traditions. examples illustrate, application data science filmmaking already generating new workflows producers, writers, editors, expand future impact creative roles.data science comes play greater role film production, stylistic decisions made filmmakers increasingly focused model select parameter values set achieve desired objectives. Alternatively, time taken generate edited sequence reduced matter seconds, filmmakers may produce multiple versions scene select one feel best meets objectives select elements different versions combined final edit. example, Kristen Stewart‚Äôs short film Come Swim (2017) employed Neural Style Transfer map style impressionistic painting director onto video image, redrawing scenes desired style controlled set parameters used realise ‚Äòun-realness‚Äô image filmmakers desired became quantified parameter u (Joshi et al., 2017).range models filmmakers can select , turn, depend decisions data scientists models potentially useful filmmakers parameters available manipulation technological decisions computer scientists design implement algorithms run models part filmmaking technologies available filmmakers. example Stanford/Adobe editing system demonstrates, development data-driven filmmaking technologies explicitly codifies set existing aesthetic norms editing dialogue scenes idioms makes norms available filmmakers set choices. factors shape evolution style time new models added dropped filmmaking technologies.introduction AI-powered editing also creates new possibilities editing scenes presently inconceivable filmmakers. 2016 AlphaGo, computer program employing advanced search trees deep neural networks, defeated leading human player Go, highly complex boardgame originating China 3000 years ago (Koch, 2016). moves made AlphaGo surprised leading human Go players (notably, move 37 game two), going centuries traditional thinking, appearing error revealed part coherent successful strategy. ways AI-powered editing system interpret set aesthetic norms, potentially expanding constricting ensemble choices available filmmakers, remain discovered.Grodal et al. (2005: 7) write , ‚Äòauthor defined creative human agency, given film may produced many different agencies: authors, directors, scriptwriters, actors, cinematographers, et al.‚Äô operating within given craft tradition set constraints. New data-driven filmmaking technologies mean network collaborative agents must expanded include new categories filmmakers ‚Äì , data computer scientists ‚Äì involved making decisions style film. Furthermore, application AI technologies filmmaking, understanding authorship cinema must also include role played non-human agents making decisions takes use combine shots sequence. idea filmmaker must therefore expand beyond traditional concepts creative agents, represented Morgan director Luke Scott filmmaker Zef Cota, include computers like Watson researchers, like Benoit Huet, develop models computers run (Figure 1.1).\nFigure 1.1: idea filmmaker looks like evolve beyond traditionally conceived filmmakers (& B) include artificially intelligent machines (C) data scientists (D) develop models filmmakers artificial intelligence systems. ‚òùÔ∏è\n","code":""},{"path":"preface.html","id":"computational-analyses-of-film-form-and-style","chapter":"1 Preface","heading":"1.2 Computational analyses of film form and style","text":"cinema becomes increasingly data-driven art form, studies art film.Since 2010 increase number studies utilising quantitative computational methods analyse form style cinema driven availability range software tools, including Cinemetrics (Tsivian, 2009), VIAN (Halter et al., 2019), ELAN (Wittenburg et al., 2006), ANVIL (Kipp, 2014), Advene (Aubert & Pri√©, 2005), packages modules programming languages Python R, Distant Viewing Toolkit (T. Arnold & Tilton, 2020) chromaR (Buonocore, 2019). however significant obstacles addressed looking future computational film analysis.Burghardt et al. (2020) present computer-aided analysis moving images interdisciplinary dialogue builds bridges researchers computer science media informatics, Film Media Studies, film archivists. However, data-driven analyses film form film style can broken two groups conducted two largely distinct groups researchers, little overlap topics study methods use. one hand, film scholars, apply statistical methods shot length shot scale data; , , digital humanists, apply computational methods analysing colour texts (e.g., subtitles, screenplays, etc.). even possible make distinction groups located, quantitative films scholars largely based English-speaking countries, digital humanists studying motion pictures tend based continental Europe.Quantitative methods applied questions film style 50 years. Salt (1992: 18) cites example Elias Boudinot Stockton discussed structure motion pictures quantitative terms 1910s Thompson (2005: 117) reports German critic Georg Otto Stindt published analysis shot length data 1926. 1962, Herbert Birett (1962) proposed study film placed scientific footing suggested features film style, shot lengths shot types, analysed statistically, following proposal Birett (1988) Birett (1993). Salt‚Äôs 1974 article ‚ÄòStatistical style analysis motion pictures‚Äô (Salt, 1974) introduced quantitative analyses shot lengths shot types mainstream Film Studies, also goal placing analysis film style objective scientific basis subjective hermeneutics contemporary film theory.Despite long history, applications quantitative methods questions film style within Film Studies remain limited narrow range formal elements, focusing almost exclusively editing cinematography analyse shot lengths shot scales. little methodological innovation nearly half century research quantitative analysis film style 2022 looks lot like 1974, authors going beyond quoting average shot lengths. methodological innovation occurred past decade largely driven researchers disciplines Film Studies, archaeology (Mike Baxter), psychology (James Cutting), linguistics (Peter Grzybek), computer science information engineering (Michele Svanera, Sergio Benini, Mattia Savardi, Alberto Signoroni).studies employing quantitative computational methods elements film style beyond shot lengths, colour texts (Burghardt et al., 2016), necessary look research audio-visual media coming digital humanities. Applications computational tools questions film form style within digital humanities much shorter history still lag areas traditionally focus field given origins literary linguistic computing (Sittel, 2017). However, longer accurate state digital humanities ignored cinema (Burghardt et al., 2020; Pustu-Iren et al., 2020). important recognise digital humanists different perspective film scholars regarding computational analysis film. Digital humanists share goal making humanities disciplines Film Studies scientific (however may defined), though often charge laid (see, example, Konnikova, 2012). Rather, seek integrate quantitative analysis existing qualitative methods better analyse motion pictures , often presented interdisciplinary research, remain grounded humanities, sharing principles perspectives non-digital humanist researchers embracing methodological opportunities computer-assisted analysis.Although two groups researchers applying quantitative computational methods study film, aspects film style remain almost entirely overlooked. Neither film scholars digital humanists devoted much attention analysis sound motion pictures. sustained body work area , even comprises handful articles presentations (Redfern, 2015b, 2020b, 2020c, 2021c, 2022). Similarly, studies movement cinema ‚Äì movement frame camera movement ‚Äì far (see Cutting, 2016 one studies include motion variable).\nComputational media aesthetics multimedia content\nanalysis\n\nBeyond humanities considerable body literature \nfields computational media aesthetics (Adams, 2003; Dorai & Venkatesh, 2002) multimedia\ncontent analysis (Gong & Xu, 2007; Hanjalic et al., 2006; Li & Kuo, 2013; Wang et al., 2000) applying\ncomputational methods analyse low-level formal elements (colours,\ntextures, shapes, motion, etc.) extracted multimedia texts; develop\nmodels bridge semantic gap\nelements semantic content text; apply\nmodels classify existing texts facilitate production \nnew media artefacts. However, research little impact \nunderstanding cinema development methods tools \ncomputational quantitative studies Film Studies digital\nhumanities. example, research underpins multimedia\nrecommender systems capable suggesting films based analysis \nfeatures extracted directly text (Deldjoo et al., 2020; Yadav & Vishwakarma, 2020) \nimpact genre theory Film Studies even though viewers interact\ndaily systems customers users Netflix, YouTube, \ncontent providers. Similarly, affective video content analysis\n(Baveye et al., 2018; Hanjalic & Li-Qun Xu, 2005; Yazdani et al., 2013; Yi et al., 2020), aims automatically\nextract emotions elicited videos assess effectively \nfilm creates desired emotion audience, ignored \ncognitive film theorists studying emotion cinema.\nDespite long history growing body research, Heftberger (2018: 62) argues computational analysis film form film style still infancy. long-standing antipathy two cultures sciences humanities sustains perceived incompatibility computational methods humanities research, making progress slow despite increasing importance data-driven methods film industry. little knowledge quantitative computational methods among scholars Film Studies specifically humanities general. Statistical literacy part film education vast majority film scholars lack ability understand concepts, methods, results computational studies presented editors reviewers unable adequately evaluate research publication readers unable read research critically. Fewer still possess required levels statistical competence necessary design conduct research using quantitative methods (Redfern, 2013a).Lev Manovich argues thatWe want humanities scholars capable using data-analysis visualisation software daily work, order able combine quantitative qualitative methods work (quoted Heftberger, 2018: 14).However, opportunities humanities scholars develop capabilities rare. Film Studies degrees rarely include computational film analysis syllabi, , lack skills contribute daily business research, film scholars rarely find home digital humanities research centres. Even though number studies employing computational methods growing, comprise small part research cinema, within Film Studies without.basic level, lack materials support like learn apply computational methods first principles. existing instructional texts suitable task. Buckland & Elsaesser (2002) contains basic errors presentation statistical terms methods degree completely avoided lest reader become thoroughly confused elementary concepts variable, bar chart, normal distribution. Mike Baxter‚Äôs Notes Cinemetric Data Analysis (Baxter, 2014) now almost decade old still useable, much code old-fashioned demonstrate key features contemporary practices R, projects, tidy data, relative paths, pipes. Baxter provides extensive discussions topics related analysing shot length shot scale data, keeping traditional focus statistical analyses Film Studies address sound, colour, text.aim book provide accessible introduction researchers, teachers, students like learn teach computational film analysis, presenting modern introduction computational analysis film form film style covers wide range formal stylistic elements motion pictures, demonstrating methods can contribute understanding cinema implement using statistical programming language R (R Core Team, 2021).","code":""},{"path":"preface.html","id":"outline","chapter":"1 Preface","heading":"1.3 Outline","text":"start introducing computational film analysis empirical approach understanding form style motion pictures.Chapter 2 defines computational film analysis describes common features computational film analysis project, also placing CFA relation digital humanities study film.Absolute beginners strongly recommended read Chapter 3 attempting subsequent chapters, working knowledge R can pick choose rest book according needs interests.Chapter 3 introduces three key tools used book: R, RStudio, FFmpeg. provides basic knowledge needed understand use tools subsequent chapters.rest book demonstrates computational analyses different aspects style cinema series case studies introduce analytical concepts methods context. examples used book include previously unpublished analyses, along case studies drawn published research demonstrate concepts underpinning work.Chapter 4 covers methods analysing sound cinema, including spectrogram, normalised amplitude power envelope, covers practical issues format audio files analysis.Chapter 5 shows perform analyses colour cinema, including frame extraction, colour spaces models, cluster analysis.Chapter 6 explores different methods analysing editing data, description comparison shot length distributions.Chapter 7 explores methods time series analysis shot length data analysing cutting rates motion pictures.Chapter 8 demonstrates ways analysing shot scale data, including multivariate analysis shot type data.Chapter 9 introduces analysis different cinema‚Äôs lexical texts (screenplays subtitles) cinema, including sentiment analysis cluster analysis.","code":""},{"path":"CFA.html","id":"CFA","chapter":"2 Computational film analysis","heading":"2 Computational film analysis","text":"Definitions like maps: help explore ground; substitutes exploration.  Brian Aldiss (1973: 3)","code":""},{"path":"CFA.html","id":"what-is-computational-film-analysis","chapter":"2 Computational film analysis","heading":"2.1 What is computational film analysis?","text":"Computational film analysis employs methods tools statistics, data science, information visualisation, computer science order understand form style cinema.Computational film analysis post-disciplinary complex. say, field inquiry belonging digital humanities study film exists collection interconnected systems weaving together specialist domain knowledge phenomenon interest humanities (cinema); knowledge design, execution, validation research projects; competencies use humanistic quantitative methods collect analyse data; competencies application computational tools process data. definition, computational film analysis research project engaged every element complex (Figure 2.1), though project engage element different ways.\nFigure 2.1: complex computational film analysis. ‚òùÔ∏è\nordering elements complex. Every CFA project begins (1) framing questions cinema, (2) operationalized achievable research project defining sample studied, concepts researcher interested variables represent , variables measured. requires (3) selection appropriate set humanistic quantitative methods data collection, analysis, reporting; (4) selection set computational tools implement methods. Applying computational tools data (5) result set outputs (statistical summaries, data visualizations, models, etc.) (6) must interpreted context methods selected. Interpreting results (7) supply answers research questions making possible (8) judge meaning results satisfy researcher‚Äôs curiosity cinema.ordering CFA project begins ends domain knowledge. questions answers CFA belong study film. context questions answers meaning. questions can answered engaging elements complex order. moves stages one four Figure 2.2, representation film becomes abstract turned data capable analysed computer. Returning steps five eight Figure 2.2, representation film becomes concrete leading us make empirical statements cinema. Every CFA project necessarily begins ends cinema.\nFigure 2.2: process computational film analysis. ‚òùÔ∏è\n2015 conference paper ‚Äòtime contour plot: graphical analysis film soundtrack‚Äô (Redfern, 2015b) illustrates process practice. (1) interested understanding sound design short horror film Behold Noose (Jamie Brooks, 2014) created frightening experience viewer. chose focus (2) temporal organisation soundtrack relationship structure film, changes sound energy different scales (event, scene, segment), dynamic relationship sound silence. methods selected (3) spectrogram short-time Fourier transform soundtrack normalised aggregated power envelope, (4) implemented using tuneR seewave packages statistical programming language R. results applying methods (5) visual representations structure soundtrack (6) interpreted given understanding quantitative methods employed order (7) describe analyse key features soundtrack selected relevant. (8) explain filmmakers organised aspect film style order impact audience. method covered Chapter 4 book.Computational film analysis statistical analysis film style. Although CFA employs statistical methods, inherits broader outlook possible quantitative methods computational tools data science embrace exploratory data analysis, statistical modelling, machine learning, data visualisation, computer programming order tell story data. CFA falls within scope greater data science described (Donoho, 2017) comprises tasks data gathering, preparation, exploration; data representation transformation; computing data; data modelling; data visualisation presentation; science data science.","code":""},{"path":"CFA.html","id":"where-does-computational-film-analysis-fit-part-i","chapter":"2 Computational film analysis","heading":"2.2 Where does computational film analysis fit? Part I","text":"Burdick et al. (2012: 122) define digital humanities broadly ‚Äònew modes scholarship institutional units collaborative, transdisciplinary, computationally-engaged research, teaching, publication‚Äô ‚Äòless unified field array convergent practices explore universe print longer primary medium knowledge produced disseminated.‚Äô concretely, term ‚Äòdigital humanities‚Äô describes two related areas research: analysis culture (broadly variously defined) using digital toolkits enable researchers search retrieve texts databases, automate analyses, detect patterns structures within texts; study impact digital technologies humanities research (Masson, 2017). perspective digital humanists, objectives digital humanities analysing culture non-digital humanities, though methods . Despite common goal digital non-digital humanists, history resistance digital humanities regards illegitimate way understanding culture uses ‚Äòdigital toolkits.‚Äôestrangement digital non-digital humanities occurs uses ‚Äòhumanities‚Äô refer different thing. digital humanist, humanities object study digital refers nature methods applied object analysis. example, Silke Schwandt defines digital humanities ‚Äògrowing field within Humanities dealing application digital methods humanities research one hand well addressing questions influence digital practices research practices within different humanities disciplines ‚Äô (Schwandt, 2020: 7, emphasis). Non-digital humanists accept position conceive humanities object inquiry way knowing world distinguishable natural sciences (Osborne, 2015: 16). Just term scientific method defines empirical method knowledge production characterised canonical form inquiry relatively stable across scientific disciplines, term humanities method defines historicist-perspectivist mode knowledge generation held equally valid across humanities disciplines (Kn√∂chelmann, 2019). ‚Äòhigh humanist‚Äô stance insists humanities ‚Äòsui generis autonomous field inquiry, approachable means special sensitivity produced humanistic training ‚Äô (Slingerland, 2008: 2). methodologies used digital humanists come sciences ‚Äì computer science, data science, statistics, etc. ‚Äì knowledge produce product scientific method , therefore, scientific knowledge. definition, knowledge produced digital humanities part humanist knowledge culture produced wrong way knowing.difference digital non-digital humanities can expressed difference disciplinarity. Jan Parker argues disciplines ‚Äòcommunities practice‚Äô characterised ‚Äònon-generic epistemological models:‚Äômany disciplines, surely, defining, quintessential element core process: underpinning unifying activity gives discipline distinctive tone value. Humanities disciplines core critical, mutual engagement humanities texts (Parker, 2002: 379).Given definition disciplinarity, clear see non-digital Humanists reject application digital methods legitimate way knowing failing participate ‚Äòunderpinning unifying activity‚Äô even though methods may enable researchers engage ‚Äì mutually critically ‚Äì humanities texts. digital humanities contribute understanding human society culture, participate humanities way knowing, set practices knowledge produced, conformed, implemented, preserved, reproduced, institutionalised regulated universities, scholarly societies, publications (Post, 2009).Robert Post argues , unlike sciences, humanities resistant methodological institutional reformulations challenge disciplinary boundaries, new emerging fields assimilated existing methodologies:genuine puzzle humanities seem easily transcend traditional disciplinary methods like textual exegesis literary criticism, analytics philosophy, narratives history, cultural hermeneutics anthropology. ‚Ä¶ fact proved surprisingly difficult generate stable enduring new disciplinary formations humanities. proliferation new disciplines sciences depends part fact new domains knowledge often require new techniques knowledge acquisition, methodology necessarily changes step subject matter studied. humanities, contrast, new domains knowledge quite regularly assimilated traditional disciplinary methods (Post, 2009: 757-758).digital humanities emerged enduring area research teaching, quite stable one, assimilated existing humanities disciplines smoothly. Indeed, assimilation digital methods humanities actively resisted (Allington et al., 2016; Da, 2019; Fish, 2019; Greetham, 2012; Konnikova, 2012).critique disciplinarity areas sciences humanities focussed limits specialisation, blinds researchers broader context phenomena exist; focusing narrow topic fails deal complexity real world, imposing past approach onto present limits possibilities create breakthroughs, discounts ignores alternative ways knowing (Osborne, 2015; Repko et al., 2019; Wilson, 2009). Andrew Sayer (2000) argues arises disciplines parochial, unable pose questions beyond limits strongly policed, imperialist, claiming intellectual territories irrespective work others.perennially recommended solution go beyond boundaries, defeating parochial imperialist imperatives disciplinarity, engage research either multidisciplinary, bringing together researchers range disciplines collaborate drawing disciplinary expertise, interdisciplinary, integrating knowledge methods different disciplines, transdisciplinary, create unity intellectual frameworks bound disciplinary perspectives (Stember, 1991). approaches response reductionist disciplinary specialisation grounded dichotomous ‚Äòeither-‚Äô thinking dominant Western intellectual tradition maintains distinction sciences humanities (Newell, 2010: 360; M. Smith, 2017: 1-5). However, multidisciplinary research remains firmly grounded disciplinary specialisation participants, interdisciplinary research remain tied disciplinary thinking. Louis Menand, points outwhen ask interdisciplinarity important, often answer interdisciplinarity solves problem disciplinarity. seems non sequitur. Interdisciplinarity simply disciplinarity raised higher power. escape disciplinarity; scholarly pedagogical ratification disciplinarity. ‚Äôs disciplinarity academics want get rid , call new order interdisciplinarity (Menand, 2010: 96-97).Attempting leave behind disciplinarity, transdisciplinary emergent phenomenon, arising context-specific interactions different forms practice generate new ways knowing capable understanding hybrid, multidimensional nature reality transcend academic disciplinary structures (McGregor, 2015; Thompson Klein, 2004). However, difficult identify point transdisciplinary emerges distinguishing inter- transdisciplinarity challenging may possible hindsight; emerges /transdisciplinary may become institutionalised time new discipline, generating new academic structures clearly defined boundaries. potential danger digital humanities becomes institutionalised just another discipline, professional societies, journals, conferences, degree programmes, university departments ‚Äì already exist ‚Äì define (imperially) police (parochially) limits. reasons, prefer use term transdisciplinary reference digital humanities, though seen , digital humanities researchers describe work terms.Randi Gray Kristensen Ryan Claycomb describe post-disciplinarity, alternative formulation transdisciplinary, goal critique disciplinarity:anti-disciplinary actively challenges taxonomic effort organise, categorise, delimit knowledge discrete, sanctioned modes inquiry, post-disciplinary (rough cognate, transdisciplinary) functions discrete models longer matter. construct perhaps simplistic narrative progression, might suggest disciplinarity come critique anti-disciplinary stance, initially gave rise current rhetoric savour multi- interdisciplinary approaches, approaches respectively combine cross methodologies, still adhere significance disciplines first place ‚Ä¶ post-disciplinary, , perhaps utopian goal anti-disciplinary critique, state knowledge production draws potential multi- interdisciplinary inquiry without professional surveillance institutional policing remains implicit forms modes (Kristensen & Claycomb, 2010: 5).utopian ideal, post-disciplinarity described form resistance co-opted existing disciplinary formulations ‚Äòachieve maintain [] radical intellectual freedom‚Äô (Kristensen & Claycomb, 2010: 6) ‚Äòliberated mode imagination curiosity advances academic freedom flexibility see, , anew‚Äô (Pernecky, 2020: 6, original emphasis). Post-disciplinarity open, democratic, creative relationship knowledge made possible breaking social structures cultural practices define disciplines:[Post-disciplinarity] extends questioning conventional norms processes knowledge production, dissemination, communication; invitation debate genres received privileged position scholarly activities; challenges established view scope limits possible, relevant, desirable, even credible academic terms (Pernecky et al., 2016: 36).Post-disciplinarity mean abandoning methodological rigour purposeless eclecticism. Rather grounded recognition different ways knowing complex diverse phenomena, one method able answer questions wish ask, answer questions researchers freedom select combine different methodologies creatively disobediently without fear discipline. goal research produce original knowledge world; perpetuate disciplines.Discarding disciplinarity requires us find new term describe digital humanities diverse field inquiry. term prefer complex, etymology lies Latin complecti, means ‚Äòembrace‚Äô ‚Äòentwine,‚Äô roots com, meaning ‚Äò, together‚Äô plectere, meaning ‚Äòbraid.‚Äô Alvila (2014: 197 ff.) argues ‚Äòcomplex‚Äô therefore two key characteristics, one derived ‚Äòbraid‚Äô ‚Äòfold‚Äô involves type interaction parts, arising ‚Äòembrace,‚Äô ‚Äòencircle,‚Äô ‚Äòencompass,‚Äô conveys sense global shape identity, convey ‚Äòquality objects whose identity meaning emerges interaction two parts make unit entity without losing individuality.‚Äôdigital humanities, , post-disciplinary complex, defined sanctified way knowing culture possibilities knowing different ways. characterised question-driven rather methods-driven research, care provenance methods used answer questions ‚Äì requirement enable us answer whatever questions wish ask culture. digital humanities, illegitimate ways knowing, methods drawn sciences freely purposefully combined non-digital humanities. researcher want know different things human society culture, methodological combinations unique questions posed complex digital humanities remade anew time, entwining embracing new strands research. can discipline-defining core process circumstances much variation among digital humanities research projects digital non-digital humanities research. Nevertheless, digital humanities living identity complex embracing different ways knowing enduring ever-changing, arises myriad methodological combinations distinguish disciplinary humanities. constant remaking digital humanities absence ‚Äòunderpinning unifying activity‚Äô armours digital humanities fate disciplinarity ‚Äì though danger ever present.manifestation digital humanities applying computational, data-driven methods questions form style motion pictures, computational film analysis inherits qualities post-disciplinary complex. piece research employing computational approaches film style create unique arrangement methods response different questions asked researchers, combining humanistic modes inquiry (historical research, philosophical approaches, narrative inquiry, textual analysis, etc.) methods drawn wide range research fields, including psychology (Cutting, 2021), bioacoustics (Redfern, 2020c), biology (May & Shamir, 2019), climate science (Redfern, 2014c), linguistics (Grzybek & Koch, 2012), quantitative economics (Redfern, 2020a), computer science machine learning (√Ålvarez et al., 2019), many others, without worrying source methods. diversity approaches inoculates CFA disciplinarity, though common shared goal: desire contribute understanding art motion pictures.","code":""},{"path":"CFA.html","id":"where-does-computational-film-analysis-fit-part-ii","chapter":"2 Computational film analysis","heading":"2.3 Where does computational film analysis fit? Part II","text":"computational film analysis fit Film Studies? answer simple: . place CFA within discipline Film Studies. mourned loss recognised opportunity re-imagine study film post-disciplinary complex unlimited methodological scope.Contemporary debates future Film Studies academic discipline largely focussed changing meaning ‚Äòfilm‚Äô emergence new, digital technologies motion picture production distribution challenge medium specificity cinema (Carroll, 2003: 1-9; Flaxman, 2012; Rodowick, 2008). Rick Altman, example, refers ‚Äòpost-Film Studies world‚Äô arising ontological uncertainty surrounding nature ‚Äòfilm‚Äô object (R. Altman, 2009). point, however, Altman (indeed film scholar) propose changing disciplinary nature Film Studies. Claims interdisciplinarity made different screen media studied alongside one another ‚Äì example, film television ‚Äì even though methods used studying different screen media self-historicist-perspectivist ways knowing characteristic discipline Film Studies. ‚Äòpost-Film Studies world‚Äô Altman envisions represents attempt negotiate meaning ‚Äòfilm‚Äô Film Studies leaving ‚Äòstudies‚Äô Film Studies untouched. ‚ÄòFilm‚Äô may take new meanings combined nouns (television/media/games/etc.), discipline persists. goal redefining ‚Äòfilm‚Äô ensure continuity Film Studies.Similarly, calls reform Film Studies, eyes scholars become corrupted lost way, intended reinvigorate rather challenge disciplinarity. example, essay ‚Äòstudy cinema?‚Äô, Valentina Vitali argued Film Studies paid high price institutionalisation discipline becoming, view, instrumentalised part culture industries seeks critique. Film Studies dissolved replaced bya discipline fully conscious role critical (opposed formatting) operation, body theories knowledge awake question productivity studying cinema medium particular discursive cultural form (Vitali, 2005: 288, emphasis).Vitali‚Äôs objective reformulation discipline, point challenge disciplinarity Film Studies. Similarly, David Bordwell No√´l Carroll sought encourage application new approaches understanding cinema sweeping away ‚Äòdecades sedimented dogma‚Äô Grand Theory dominated discipline early-1970s mid-1990s replacing ‚Äòtheories theorising; problem-driven research middle-level scholarship; responsible, imaginative, lively inquiry‚Äô nonetheless left Film Studies largely intact, re-constructing discipline (Bordwell & Carroll, 1996: xvii).Personally, comfortable word ‚Äòfilm‚Äô use without difficulty refer arts works mode aesthetic experience well understood converse (yet meet anyone understand phrase ‚Äòwatch film‚Äô), even though longer depends film medium , extent, become skeuomorphic. disciplinary part Film Studies see obstacle future ‚Äì ‚Äòstudies‚Äô ‚Äòfilm‚Äô ‚Äì inhibits ways others can know cinema. Re-constructing re-formulating Film Studies retaining core disciplinarity change fact. shift Film Studies study film shift disciplinary post-disciplinary, methods-driven research question-driven research, disciplinarity complexity. Film Studies discipline; film domain study, like digital humanities, post-disciplinary complex.Film Studies approach studying film emerged Europe North America late-1960s. 1968 edition Cinema Journal, journal newly named Society Cinema Studies, editors declared ‚Äòsearching best approach, discipline‚Äô (quoted Ellis & MaCann (1982: viii, emphasis)). close relationship method discipline: knowledge part discipline Film Studies knowledge produced characteristic approach discipline, determined critical analysis grounded historicist-perspectivist modes drawn humanities disciplines. Film Studies conceived, parochially imperially, way knowing cinema grounded humanities-based methods inquiry (‚Äòbest approach‚Äô), self-nominated group scholars asserting ownership (‚Äòdiscipline‚Äô) programme research cinema. Grieveson & Wasson (2008: xiii) point fact Society Cinema Studies born Society Cinematologists, dropping old name represented conscious rejection scientific approaches cinema wholehearted embrace humanities-based methods.irony . Film Studies emerged time ‚Äòstudies‚Äô programmes higher education, Women‚Äôs Studies, Gender Studies, Environmental Studies, etc., conceived part anti-bureaucratic interdisciplinary movement principle disciplinarity grounded broad-based scepticism universality particular method (Menand, 2001). Film Studies, however, conceived singular field focussed limited range methods establish clearly differentiated discipline within humanities (even though methods drawn English literature, art history, philosophy) enabled new discipline become institutionalised university departments. benefit many researchers students ‚Äì including ‚Äì highly successful achieving goal. However, came cost. David Bordwell argues Film Studies ‚Äògot wrong foot methodologically. Instead framing questions, competing theories might responded common concern enlightenment, film academics embraced doctrine-driven conception research‚Äô (Bordwell, 2005b: xi, original emphasis). institutionalisation Film Studies admitted single class methods legitimate, cutting research economists, sociologists, psychologists use methods, might usefully contribute understanding cinema complexity answering questions Films Studies attempts answer. theoretical methodological justification .‚Äòparadigmatic chauvinism‚Äô (Sternberg, 2007) doctrine-driven approach exhibited Film Studies creates several problems. First, restricting study film single way knowing cinema limits answers may find; constrains range possible questions can ask. ‚Äòbest approach‚Äô one helps researchers answer questions wish ask cinema irrespective origins. Second, alternative methodologies ‚Äì researchers employ ‚Äì marginalised despite fact may better suited answering questions cinema. Consequently, questions viewed important conform pre-selected methods rather selecting methods based importance question. can add , developed North America Europe, Film Studies grounded Western intellectual tradition humanities limited single class methods marginalises alternative approaches ‚Äì researchers employ ‚Äì grounded non-Western methodologies pedagogies (Akande, 2020). Third, film researchers film students receive narrow training knowledge production, perpetuating methodological monopoly adequately prepare deal real world problems , fourth, researchers unable change direction response exhaustion methodological fads, emergence new phenomena, demands non-academic, stakeholders. example, problems acknowledged Ian Christie, observed BFI symposium research policymaking British film industry 2011 thatmeaningful research audio-visual field increasingly needs multiplicity skills disciplines [‚Ä¶] problem style funding moment [‚Ä¶] single principal investigator one discipline one institution (British Film Institute, 2011: 9).Christie went express opinion Film Studies failed address important issues impose research agenda, part generated much qualitative research provide limited range answers limited range questions.Computational film analysis fit Film Studies participate ‚Äòunderpinning unifying activity‚Äô discipline, namely critical analysis film texts means humanities-based methods. Nonetheless, CFA goals Film Studies scholars using humanities methods: understand form style motion pictures account functions. CFA contribute understanding cinema necessary forego disciplinarity Film Studies embrace study film post-disciplinary complex right. thinking film object inquiry rather discipline free mundane activity reproducing something called ‚ÄòFilm Studies‚Äô ask questions want ask cinema answer .study film comprises four distinct related areas: industrial analysis, textual analysis, ethnographic analysis, cognitive-psychological analysis (see Table 2.1). Films analysed institutionally produced commercial commodities function cultural artefacts inscribed meanings consumed interpreted audiences whose experiences cinema predicated cognitive-psychological processes perception comprehension. complexity film object inquiry requires methodological diversity match. single method class methods capable answering questions can ask cinema. answer question posed one area often require methods derived one others. example, order answer question film style, ‚Äòcontinuity editing emerge dominant style Hollywood cinema?‚Äô (textual analysis), find answer understanding film industry adopted style mode producing films narrationally economically efficient (industrial analysis; see Staiger (1985: 178-180)) understanding system editing exploits viewer‚Äôs ability construct coherent spatial understanding scene (cognitive-psychological analysis; see Anderson (1996: 99-103)). insist irreducibility film phenomenon. Individual questions may answered application specific method within single area; understand cinema diverseness, necessary range methods choose.\nTable 2.1: four areas study film.\napproach film complex object inquiry methodological openness demands, researching cinema naturally includes quantitative computational methods, naturally includes historical text-based methods, film scholars faced economic, textual, sociological, scientific data much computationally tractable. application quantitative computational methods, combination humanistic modes inquiry, becomes possible answer far broader range questions economics film industry, patterns style form motion pictures, audiences‚Äô behaviours attitudes, experience make sense cinema (Redfern, 2013a). methods used come economics, marketing, sociology, psychology, neuroscience; provenance less interest fact can contribute understanding cinema. , goal produce knowledge cinema rather perpetuate discipline Film Studies. , students study film; Film Studies.ideas filmmaker change, ideas future design film education programmes analysis become part filmmaking. present, filmmaking courses focus core skills direction, cinematography, editing, sound design, producing; data-driven AIs come reshape , cases, replace filmmakers performing activities, new type filmmaking education required based craft traditions filmmaking also computer data sciences.One approach Data-Driven Creativity Project (DDCP) Curtin University, Australia, teaches students filmmakers‚Äô stylistic decisions impact audiences‚Äô attention, arousal, emotions using data derived eye-tracking, skin conductance level, facial recognition studies (Bender & Sung, 2020). Adopting CFA approach can extend project allow students create analyse data researchers filmmakers, can grow beyond filmmakers viewers act ‚Äòpractical psychologists‚Äô (Bordwell, 2012) become cinematically scientifically experimental researchers data-driven filmmakers.Another way forward develop degree programmes designed meet goals combining computational film analysis, students learning film form film style developing data science knowledge build models motion picture aesthetics, computer science, students learn data structures algorithms allow engineer software implement models developed using CFA production motion pictures.Finally, film scholars going continue work analysing film style film form, need able think computationally understand algorithms create works art. critique data-driven filmmaking without understanding data science. critical operations capable analysing limitations biases algorithms data used train (Kearns & Roth, 2020) without understanding data used film industry. Data-driven filmmaking requires data-driven studies film.","code":""},{"path":"CFA.html","id":"analysing-films-computationally","chapter":"2 Computational film analysis","heading":"2.4 Analysing films computationally","text":"","code":""},{"path":"CFA.html","id":"analysis","chapter":"2 Computational film analysis","heading":"2.4.1 Analysis","text":"discussion aesthetics music, Roger Scruton (1997: 396-398) describes analytical process building bridge formal structure artwork aesthetic experience affords construction ‚Äòcritical narrative‚Äô enables us experience work art, experience things differently , thereby, help us enjoy work art. Analysis differentiated forms writing film , unlike criticism, engage value judgements; , unlike hermeneutics, produce subjective interpretations depend extrinsic factors. Rather, focussed work uses description account formal structure (), aim explain functions () operations () text. Computational film analysts comprehend structure text computational means, goal remains production critical narrative explains film group films analysis.Computational film analysis complements extends existing methods used analysis film style form bring alternative perspective bear cinema, encouraging scholars think new ways old problems conceive novel questions answered (Ramsay, 2012a). CFA adopts bottom-data-driven approach alternative top-doctrine-driven approach Film Studies allows scholars discover unexpected relationships potentially explicable patterns form style rather constrained priori interpretations film (Redfern, 2015a). digital toolkits CFA make possible move smoothly different scales analysis individual pixels scenes sequences whole films beyond groups films, allowing researchers look frame extraordinary detail view corpus distance (T. Arnold & Tilton, 2019).Carroll (1998) describes two modes stylistic formal analysis motion pictures: classificatory mode, uses descriptive stylistics relationally differentiate groups films based presence absence set formal devices; explanatory mode, accounts significant use stylistic formal devices single film deconstructing functions devices realizing purposes film.Computational methods can applied analytical modes. classificatory mode aligns application data mining, detection patterns, relationships, anomalies within large data sets, machine learning, automated construction models data minimal human intervention, questions film style level corpus, analysing films based colour (Flueckiger, 2017), plot structure (Del Vecchio et al., 2020), editing (Cutting et al., 2010), social networks (Weng et al., 2009), shot scales (Svanera et al., 2019), visual content (May & Shamir, 2019).explanatory mode aligns textual analysis single films address functional questions explain film possesses formal stylistic qualities , capable shifting micro level individual moments film, meso level scenes sequences, macro level whole film describe explain use colour film (Stutz, 2021), sound design trailer (Redfern, 2021c), relationship multiple elements film‚Äôs style narration (Redfern, 2013b).adopting methods used data-driven creation motion pictures, CFA can bring new methods analysis motion pictures. Prose Storyboarding Language developed Ronfard et al. (2015) builds research developing artificial intelligence approaches film production provide formal language describing motion pictures shot--shot using simple syntax limited vocabulary based film production working practices intended readable AI machines humans. Although primary purpose Prose Storyboarding Language serve high-level user interface artificially intelligent filmmaking technologies, also applications close analysis motion pictures film scholars can used describe scenes films, communicating stylistic formal features concisely effectively matter complex shot sequence analysis.application data visualization techniques, CFA frees us idea analysis film studies literary activity ‚Äì ‚Äòform writing addresses films potential achievements wishes convey distinctiveness quality (lack )‚Äô (Clayton & Klevan, 2011: 1). use graphical methods analysis makes possible place various elements film style relationship one another simultaneously grasp coherence film‚Äôs system style rather dealing singly, one one. Andrew Klevan writes ‚Äòfilm ‚Äì visual, aural, moving ‚Äì particularly slippery art form‚Äô ‚Äòsets peculiar problems analysis description tantalizingly present yet always escaping‚Äô (2011: 71, original emphasis). Consequently, Film Studies tends treat style static phenomenon, either abstracting individual scenes flow larger work ignoring temporal dimension altogether. Applying data visualisation form cinema enables researchers analyse film style dynamic rather static, tracking evolution style across single film describing trends uses formal devices across decades. CFA thus enables film scholars approach film dynamic, complex work art entirety cinema mass art, rather focussing individual devices isolation restricted small groups films.","code":""},{"path":"CFA.html","id":"film","chapter":"2 Computational film analysis","heading":"2.4.2 Film","text":"Alan Marsden (2016) writes computational approaches aesthetic analysis impose limits can think text, whether novel, film, piece music, artwork must definite input analysis. term film can used materially (albeit skeuomorphically) refer recording events stored celluloid, video tape, DVD/Blu-Ray, DCP, hard drive, etc.; can used experientially, refer presentation events screen witnessed viewer (Redfern, 2007). computational film analysis, film input can ever material form. analyst must therefore accept certain ‚Äòontological rigidity‚Äô film text must take form binary code every bit determined prior analysis. Marsden (2016: 20) argues provided data recognised standing text stage, reason locate analysis anywhere film analysis depend inputs extrinsic film.However, mean results analysis inevitable , Marsden argues, single definitive representation artwork can generated input; artwork never fully captured computational methods , accordance principle incomplete knowledge (Heylighen, 1993), model film generated analytical process must simpler object explains. analytical outputs computational approaches fluid inputs researcher must make decisions aspects film selected answer research question projections adequately represent aspects. Consequently, necessarily reason ‚Äòbelieve particular structure output analytical process privileged status among myriad possible ways input data structured‚Äô (Marsden, 2016: 20).example, analysis sound design short horror film Behold Noose (Redfern, 2015b) extracted soundtrack film. selection therefore made aspect film analysed (complete soundtrack film); input analytical software necessarily binary code (.wav file), ontological equivalence soundtrack data comprising wav file processed. stage, text equals data: data file containing amplitude values comprising waveform film‚Äôs soundtrack converted audio file, thereby reproducing soundtrack original form. soundtrack processed produce set projections (spectrogram normalised aggregated power envelope) represented soundtrack analytical power allow identify key features explained. producing projections, information lost cost revealing latent structure data: spectrogram power envelope contain less information soundtrack neither exactly describes film‚Äôs soundtrack, though illuminating various features soundtrack. Alfred Korzybski stated:map territory represents, , correct, similar structure territory, accounts usefulness (Korzybski, 1994: 58, original emphasis).spectrogram power envelope data structures; stage, data text longer ontologically equivalent ‚Äì one substituted . data-based projections represent film‚Äôs soundtrack. Using projections, presented critical narrative explained structure functions different parts soundtrack, use non-linear sound mixing, role affective events place within narrative-emotional economy film. chosen alternative set projections (wavelet analysis spectral flux) represent soundtrack, perhaps leading different critical narrative. Computational film analysis thus involves process transformation film data representation critical narrative researcher moves ontological rigidity text analytical fluidity account film present. Figure 2.3 illustrates process.\nFigure 2.3: transformational process computationally analysing short horror film (Redfern, 2015b). film processed extract selected data required analysis, film data equivalent. turn, data processed produce set projections (visualisations, numerical summaries, etc.) represent structure film. representations basis critical narrative explains range features film. ‚òùÔ∏è\n","code":""},{"path":"CFA.html","id":"computational","chapter":"2 Computational film analysis","heading":"2.4.3 Computational","text":"Digital Humanities broad term includes works humanists use digital methods materials (Ramsay, 2012b). digital humanists employ existing available technologies Voyant Tools, VIAN (Flueckiger & Halter, 2020), ImageJ (Schneider et al., 2012) process texts; others create tools, developing new software writing code analysis. computational humanities thus part co-extensive digital humanities; able code digital humanist. Birnbaum & Langmead (2017: 64) point , ‚ÄòDigital Humanities way humanities, way computing.‚Äôevery digital humanist needs able code, advantages acquiring knowledge develop tools process data digital humanities. Understanding software digital humanities developed operates allows researchers use existing tools effectively find ways ‚Äòhack‚Äô tools use new ways (Birnbaum & Langmead, 2017). Going , learning code means digital humanists restricted procedural logic designed tools creators can create tools according workflows required research projects. Learning code can therefore source independence researchers. time, digital humanities projects often bring together groups researchers different types knowledge, understanding tools logic underpins research project required collaboration. Learning code creates opportunities discovery, enabling researchers play texts test refine ways producing knowledge . ‚Äòexploratory programming‚Äô (Montfort, 2021) can lead new ways thinking problems conceive new methods analysis. Developing tools form humanities research, different set outputs (data sets, data papers, software, databases, etc.) knowledge shared beyond traditional six--eight-thousand-word article (though digital humanities remain devoted communicating knowledge form). Finally, much satisfaction gained writing code reveals new aspect text lets us see topic new way, developing tool enable others . act creative problem solving, coding source pleasure research.","code":""},{"path":"CFA.html","id":"summary","chapter":"2 Computational film analysis","heading":"2.5 Summary","text":"Computational film analysis digital humanities approach understanding style cinema. one approach among many (Burghardt et al., 2020); name indicates, one based proficiency creating applying tools coding. Referring model CFA project described Figure 2.1, necessary scholars understand quantitative methods employed methods operationalized software. topics Film Studies scholars traditionally shied away bar entry low, requiring (1) recognition computational tools can useful helping us understand form style cinema (2) time acquire requisite knowledge. subsequent chapters book present methods using R statistical programming language within CFA framework described analysis film form style, demonstrating application making explicit concepts logic underpin . reader/user/explorer chooses methods create new knowledge tools creating knowledge cinema.","code":""},{"path":"tools.html","id":"tools","chapter":"3 Tools","heading":"3 Tools","text":"\nchapter presents brief overview three tools use analyse films computationally ‚Äì R, RStudio, FFmpeg. chapter intended provide enough knowledge tools come apply subsequent chapters can focus analysing films learning key methods applicable framework rather overloading reader information without context.numerous excellent guides using technologies provide comprehensive tuition reader wants delve deeper. website R Tutorial good place start never used R , R-Bloggers excellent bite size tutorials. Hadley Wickham Garrett Grolemund‚Äôs R Data Science (Wickham & Grolemund, 2017) provides comprehensive overview tidyverse suite packages (make extensive use book) good practice using R; can accessed online https://r4ds..co.nz.","code":""},{"path":"tools.html","id":"r","chapter":"3 Tools","heading":"3.1 R","text":"R free open-source environment statistical computing graphics programming language available Windows, Linux, MacOS.R created statistical computing data analysis statisticians data analysts principal software used many areas data science academia. designed software development. Consequently, R‚Äôs underlying logic statistician data analyst rather software developer. Using R, find spend less time writing code analyse data time thinking question trying answer better fits analytical workflow compared using tools designed programming language (Python Julia) different underlying logic. (‚Äôm sure whose background programming rather data science prefer use Python rather R reasons). mean R tool suitable computational film analysis, areas tools optimal due availability broad support base. example, Python preferred language computer vision machine learning tasks, relevant computational film analysis. Part designing computational film analysis project choose methods employed computationally operationalised. choice analytical tool depends researcher, project goals, like work.several reasons R desirable choice. R freely available across range platforms, making easy share code replicate research without worrying licenses replicating workflows based point--click selections menus. R highly extensible thousands packages. R flexible, can used data visualisation, exploratory data analysis, statistical modelling; data mining machine learning; web scraping; create publish documents (handouts, articles, books ‚Äì including one), interactive notebooks, websites, applications. extensibility flexibility R mean often tool required stage workflow, including data cleaning analysis, producing publication-quality visualisations, creating research outputs, thereby greatly simplifying management research project. Learning analyse data writing code forces researcher think deeply want achieve goals software requires point--click item menu. Consequently, learning use R results greater knowledge gain quantitative methods using data analysis tools. also enable learn data can tell world. Finally, ability share R code used research project promotes reproducibility accountability research, allowing researchers understand analyst arrived conclusions CFA project reproduce analysis.","code":""},{"path":"tools.html","id":"installing-r","chapter":"3 Tools","heading":"3.1.1 Installing R","text":"R can downloaded Comprehensive R Archive Network (CRAN) https://cran.r‚Äëproject.org. installation process varies platform platform (Windows, Linux, MacOS) case difficult CRAN website clear instructions operating systems. installation process completed R scripting language, R environment console (Figure 3.1), base set data analysis packages ready use.\nFigure 3.1: R environment console. Code entered command prompt >. ‚òùÔ∏è\n","code":""},{"path":"tools.html","id":"using-r","chapter":"3 Tools","heading":"3.1.2 Using R","text":"section, cover basic concepts required make sense code written R. key concepts (control flow, vectorisation, accessing elements) introduced contexts applied make meaning use clear.R can used simply calculator perform mathematical operations, entering values command prompt (>) console, pressing Enter.case, R prints answer store result later use. store answer, need assign output command object using assignment operator <-, combines less sign (<) dash (-) used name objects, assign values variables, evaluate expressions, apply functions pass value object.names objects case‚Äësensitive: example, objects answer ANSWER different objects. Names can contain letters, numbers, underscores, periods, start letter. (Names can start period followed letter, avoided general). Names start number underscore. Names assigned objects can formed several ways:one word lowercase: objectnameseparating words period: object.nameseparating words underscore: object_namecamel case beginning lower case: objectNamecamel case beginning upper case: ObjectNameThere right wrong choice, consistency source sanity coding. prefer using underscore method names written using format easier read. Finally, names use reserved words used R (e.g., TRUE, FALSE, NULL, etc.).Objects R include data structures (lists, vectors, matrices, data frames, etc.), variables, functions, packages. workspace comprises collection objects currently active R session. workspace objects contains permanently lost session closed unless saved .RData file. can save either whole workspace individual objects workspace.restore workspace /objects active R session, use load() function.","code":"\n2 + 3## [1] 5\n# Store the result of a sum in the variable answer\nanswer <- 2 + 5\n\n# Call the variable by typing the name of the object\nanswer## [1] 7\n# Use the object answer for another sum\nanswer + 5## [1] 12\n# Save the workspace and the objects it contains\nsave.image(\"TodaysWork.RData\")\n\n# Save the object answer in a .RData file\nsave(answer, file = \"answer.RData\") \n# Load saved objects into an R session\nload(\"answer.RData\")"},{"path":"tools.html","id":"comments","chapter":"3 Tools","heading":"3.1.3 Comments","text":"R, comments human readable descriptions purpose piece code begin hashmark (#). Anything # line code run.Comments excuse poorly written code meaningful names used whenever possible; essential producing good code:Comments make code navigable: long script easy use can find way around comments make valuable signposts.Comments make code shareable: someone else able understand code written without decipher code .Comments make code maintainable: ability re-use code time depends ability understand thinking originally wrote code forget.Header comments top script describe wrote code, written, . Functions commented describe purpose parameters. Inline comments used make piece code obvious already ‚Äì comments need duplicate code.","code":"\n# Comments can be on a single line\n# or multiple lines\nanswer <- 2 + 5  # Inline comments can also be added to a line of code"},{"path":"tools.html","id":"functions","chapter":"3 Tools","heading":"3.1.4 Functions","text":"function self-contained piece code performs defined task, taking sort data input, processing , returning result (model, plot, data structure).Functions take form function name followed parentheses: function_name(). input(s) function arguments, includesthe data wish apply function ; andany control parameters determine function behaves (e.g., function return plot ).Arguments optional functions may arguments: example, get current working directory need add arguments call getwd(), need parentheses. Arguments may default values used explicitly specify argument: example, function returns plot may default argument plot = TRUE automatically return plot default unless set plot = FALSE. get list arguments function can use args(function_name).Part flexibility R allows users create functions objects workspace using function() function. functions basic form, comprising function name, set arguments, function body.object workspace function name short informative ‚Äì sense function name. noted , arguments inputs function want apply operations. operations implemented function statements wrapped curly braces. example, create function add two numbers together return sum, create function named addition() takes two arguments (b) adds together.Functions can take functions arguments can nested: example, addition(addition(2, 3), addition(4,2)) perfectly valid (ugly) application addition() function return correct result 11.find repeatedly using piece code project across set projects makes sense put function call function needed rather clutter project repetition. Functions can stored files extension .R: example, save addition() function file addition.R. creates script file can loaded whenever wish apply function, making code reusable across projects, shareable users, easier maintain need manage one chunk code rather several instances code project. source() function loads functions stored .R script workspace.script may contain multiple functions, called workspace file sourced. However, scripts containing multiple functions become larger complicated preferable single script function store use functions package.","code":"function_name <- function(arg_1, arg_2, ‚Ä¶){\n    function_statements\n}\n# A simple function to add two numbers together\naddition <- function(a, b){\n    a + b\n}\n\n# Use addition() to add 2 and 3\naddition(2, 3)## [1] 5\nsource(\"addition.R\")"},{"path":"tools.html","id":"packages","chapter":"3 Tools","heading":"3.1.5 Packages","text":"Packages comprise sets functions, data, documentation extend functionality R. R enormous collection libraries (18000+) covering enormous range functions. analytical method data exists, least one R package applying method. Packages stored online repositories can downloaded installed directly within R. Comprehensive R Archive Network (CRAN) official repository R packages publishes packages meet policies package development, packages (individual functions) can also accessed GitHub (NB: requires devtools pacman packages).package‚Äôs functions available use required package installed computer package loaded current workspace. can check packages currently loaded R session using sessionInfo(), print information R, operating system, packages loaded workspace. Loading package means every function package available namespace. want use individual function without loading whole package, can call function package_name::function_name() package installed computer.Packages functions objects workspace names case‚Äësensitive: example, tuner::readwave() load wave file R names package function tuneR::readWave().","code":""},{"path":"tools.html","id":"pacman","chapter":"3 Tools","heading":"3.1.5.1 pacman","text":"pacman package R package management tool comprising useful set functions streamline process install packages loading workspace. install pacman use base install.packages() command.Using pacman::p_load() function, now possible load multiple packages single command, separating package names commas. install load package, use pacman::p_install(). example, load tuneR seewave packages use Chapter 4 use following code:pacman::p_load() automatically install packages already installed computer (NB: internet connection required). Packages can unloaded longer needed using pacman::p_unload() can updated using pacman::p_update().","code":"\n# Note the quote marks around the package name\ninstall.packages(\"pacman\") \n# Quote marks are not needed using p_load()\npacman::p_load(tuner, seewave)"},{"path":"tools.html","id":"tidyverse","chapter":"3 Tools","heading":"3.1.5.2 tidyverse","text":"common call make using pacman load tidyverse package (Wickham et al. (2019)).tidyverse package really package; suite packages data science share common approach processing visualising data, designed single workflow streamlines analytical process. Almost general tasks data analysis ‚Äì data inspection summarisation, data wrangling, data cleaning, data visualisation ‚Äì can done using tidyverse suite.Installing tidyverse package install following individual packages:readr: read parse data files (.csv, .tsv) R‚Äôs workspace. Loading file using readr::read_csv() automatically load data data frame (see ). read Excel files use readxl package. work googlesheets use googlesheets4.dplyr: manipulate data tabular format. functions select(), filter(), arrange(), mutate(), summarise(), group() ones use analysing data.tidyr: reshape tidy data frames based concept tidy data.stringr: work strings (text data).forcats: work categorical data factors.tibble: alternative data frame data structure R.purrr: apply functions data frames.ggplot2: plot data customize just aspect plot can think . numerous extensions add functionality ggplot2 maintaining consistent aesthetic across visualisations.tidyverse selection cheat sheets help use packages efficiently.Loading tidyverse package load packages suite, though package suite can loaded individually desired. example, already prepared data want create plot can load just ggplot2 package calling package .key concept underpinning tidyverse tidy data (Wickham, 2014). tidy dataevery column variable.every row observation.every cell single value.example, come analyse colour Chapter 5 calculate average colour frame short film. data stored data frame store following information tidy data format:column variable data set identifies frame (frame number, frame_id), time frame occurs film (time), colour parameters frame colour space (L, , B, C, H, S, r, g, b).observational unit data frame row represents frame film.cell value variable frame.Table 3.1 illustrates arrangement data tidy data format.\nTable 3.1: Colour data frames fom film arranged tidy format. ‚òù\ntidy data approach provides standard way think data arranged makes quicker easier process. need rearrange data messy formats reduced (eliminated). different packages tidyverse expect data arranged way easier move different stages data analysis tidy data. find loaded tidyverse package use functions several packages tidyverse suite together seamlessly, rarely thinking specific packages using. reduces amount time need spend wrangling data can spend time thinking questions film form film style trying answer.","code":"\n# Load the full suite of packages in the tidyverse\npacman::p_load(tidyverse)\n\n# Load ggplot2 only\npacman::p_load(ggplot2)"},{"path":"tools.html","id":"the-pipe-operator","chapter":"3 Tools","heading":"3.1.6 The pipe operator","text":"pipe operator %>% magrittr dplyr packages makes easy chain together commands.Use pipe operator makes code easier read edit need nested function calls. Commands flow left--right top--bottom rather inside-, need intermediary variables cluttering workspace using memory reduced (though eliminated ‚Äì long chains piped commands can become unwieldy troubleshoot). example, three code chunks perform operations (1) taking exponent value vector num, (2) summing exponents, (3) taking logarithm sum, (4) rounding result one decimal place (5) storing result object num_output; one logically simpler efficient others.R pipe operators perform task, use %>% book operator used widely.","code":"\n# Set the seed of R's random number generator for reproducibility\nset.seed(1) \n# Create a vector of 10 random numbers from a uniform distribution\nnum <- runif(10)\n\n# Example 1: using nested functions\nnum_output_ex1 <- round(log(sum(exp(num))), 1)\n\nnum_output_ex1## [1] 2.9\n# Example 2: using intermediary variables\nnum_exponents <- exp(num)\nnum_exponents_sum <- sum(num_exponents)\nnum_exponents_sum_log <- log(num_exponents_sum)\nnum_output_ex2 <- round(num_exponents_sum_log, 1)\n\nnum_output_ex2## [1] 2.9\n# Example 3: using the pipe operator\npacman::p_load(dplyr) # call the dplyr package to use the pipe operator\nnum_output_ex3 <- num %>% \n  exp() %>%\n  sum() %>%\n  log() %>% \n  round(1)\n\nnum_output_ex3## [1] 2.9"},{"path":"tools.html","id":"df","chapter":"3 Tools","heading":"3.1.7 Data frames","text":"data structure way storing information computer. R many different types data structures, data structure use extensively throughout book data frame.data frame two-dimensional data structure ‚Äì table ‚Äì can store different types data, including numerical data, logical values (.e., TRUE FALSE), text. R data structures, vector matrix, can store single type data. makes data frames highly flexible data structure.get structure data frame can use dplyr‚Äôs glimpse() function. example, see structure Early Hitchcock editing data set use chapters 6 7, load .csv file containing shot length data using readr::read_csv() create data frame df_hitchcock call dplyr::glimpse(df_hitchcock).output tells us object df_hitchcock 1056 rows 9 columns. get list columns data set listing names, indicating type <dbl> (short double-precision floating-point format), showing first values column data frame. head() function also let‚Äôs us see top rows df_hitchcock data frame, makes easy grasp structure data frame tidy format.can access individual columns data frame using $ operator. see first rows Blackmail (1929) column df_hitchcock data frame, use head(df_hitchcock$`Blackmail (1929)`). Note need backticks (`) around column name includes space.columns data frame must equal numbers rows R populate empty cells NA ensure columns length. can get complete summary data frame using summary() function. columns storing numeric data, return summary column data frame including five-number summary, mean value, number NA values column.NA values need removed using data column using na.omit() function. Applying process shot length data Blackmail (1929), see Blackmail (1929) column df_hitchcock data frame 1056 values. However, see summary 618 NA values. omitting NA values see actually 438 shots film data.designate data frames workspace prefix df_ always aware type object working . occasionally use data structures, matrix, whenever possible ensure data structured data frame tidy possible.","code":"\ndf_hitchcock <- readr::read_csv(here::here(\"Data\", \"early_Hitchcock.csv\"), show_col_types = FALSE)\n\n# Inspect the data set\ndplyr::glimpse(df_hitchcock)## Rows: 1,056\n## Columns: 9\n## $ `The Ring (1927)`         <dbl> 4.04, 8.46, 5.58, 6.00, 1.71, 5.04, 5.13, 1.‚Ä¶\n## $ `The Farmers Wife (1928)` <dbl> 5.79, 17.13, 8.67, 5.08, 4.50, 11.08, 2.75, ‚Ä¶\n## $ `Champagne (1928)`        <dbl> 19.21, 16.96, 3.42, 14.71, 2.67, 2.79, 1.58,‚Ä¶\n## $ `The Manxman (1929)`      <dbl> 7.8, 25.1, 7.5, 3.1, 3.2, 2.2, 2.0, 8.4, 7.6‚Ä¶\n## $ `Blackmail (1929)`        <dbl> 6.88, 4.67, 2.96, 1.75, 2.67, 6.21, 7.25, 5.‚Ä¶\n## $ `Murder (1930)`           <dbl> 6.0, 2.9, 2.3, 1.4, 36.0, 4.5, 6.4, 3.1, 19.‚Ä¶\n## $ `The Skin Game (1931)`    <dbl> 19.38, 5.71, 105.17, 1.40, 3.00, 9.30, 2.80,‚Ä¶\n## $ `Rich and Strange (1931)` <dbl> 36.9, 9.3, 3.8, 4.4, 2.5, 2.3, 2.0, 3.4, 7.0‚Ä¶\n## $ `Number Seventeen (1932)` <dbl> 43.8, 3.3, 77.7, 7.5, 4.5, 3.4, 4.0, 2.4, 8.‚Ä¶\nhead(df_hitchcock)## # A tibble: 6 √ó 9\n##   `The Ring (1927)` `The Farmers Wife (1928)` `Champagne (1928)`\n##               <dbl>                     <dbl>              <dbl>\n## 1              4.04                      5.79              19.2 \n## 2              8.46                     17.1               17.0 \n## 3              5.58                      8.67               3.42\n## 4              6                         5.08              14.7 \n## 5              1.71                      4.5                2.67\n## 6              5.04                     11.1                2.79\n## # ‚Ñπ 6 more variables: `The Manxman (1929)` <dbl>, `Blackmail (1929)` <dbl>,\n## #   `Murder (1930)` <dbl>, `The Skin Game (1931)` <dbl>,\n## #   `Rich and Strange (1931)` <dbl>, `Number Seventeen (1932)` <dbl>\nhead(df_hitchcock$`Blackmail (1929)`)## [1] 6.88 4.67 2.96 1.75 2.67 6.21\nsummary(df_hitchcock)##  The Ring (1927)  The Farmers Wife (1928) Champagne (1928) The Manxman (1929)\n##  Min.   : 0.040   Min.   : 0.250          Min.   : 0.040   Min.   : 0.600    \n##  1st Qu.: 2.170   1st Qu.: 2.290          1st Qu.: 2.290   1st Qu.: 2.600    \n##  Median : 3.520   Median : 4.000          Median : 4.040   Median : 4.400    \n##  Mean   : 5.006   Mean   : 5.746          Mean   : 5.883   Mean   : 6.074    \n##  3rd Qu.: 6.140   3rd Qu.: 6.900          3rd Qu.: 7.330   3rd Qu.: 7.525    \n##  Max.   :57.630   Max.   :78.830          Max.   :63.710   Max.   :52.500    \n##                   NA's   :49              NA's   :163      NA's   :248       \n##  Blackmail (1929) Murder (1930)    The Skin Game (1931) Rich and Strange (1931)\n##  Min.   :  0.92   Min.   :  0.50   Min.   :  0.80       Min.   : 0.200         \n##  1st Qu.:  2.88   1st Qu.:  2.10   1st Qu.:  2.20       1st Qu.: 2.000         \n##  Median :  5.42   Median :  4.80   Median :  5.10       Median : 3.700         \n##  Mean   : 11.47   Mean   : 13.73   Mean   : 18.05       Mean   : 7.111         \n##  3rd Qu.: 12.75   3rd Qu.: 15.32   3rd Qu.: 14.20       3rd Qu.: 7.100         \n##  Max.   :148.17   Max.   :223.90   Max.   :281.00       Max.   :79.100         \n##  NA's   :618      NA's   :618      NA's   :787          NA's   :369            \n##  Number Seventeen (1932)\n##  Min.   : 0.100         \n##  1st Qu.: 1.300         \n##  Median : 2.400         \n##  Mean   : 5.658         \n##  3rd Qu.: 5.350         \n##  Max.   :77.700         \n##  NA's   :401\n# Check the length of the column storing the Blackmail \n# shot length data in the df_hitchcock data frame\nlength(df_hitchcock$`Blackmail (1929)`)## [1] 1056\n# Create a new object storing the Blackmail shot length \n# data having removed NA values\nblackmail <- na.omit(df_hitchcock$`Blackmail (1929)`)\n\n# Check the length of the new object blackmail\nlength(blackmail)## [1] 438"},{"path":"tools.html","id":"rstudio","chapter":"3 Tools","heading":"3.2 RStudio","text":"RStudio cross-platform integrated development environment (IDE) user‚Äëfriendly base R console, though necessary install IDE use R. installer RStudio can downloaded https://www.rstudio.com/products/rstudio/download/. RStudio Cloud makes browser-based version IDE available though book assume using desktop version.Using RStudio means many tasks encounter using R can automated save time. example, rather type assignment operator <- every time can use keyboard shortcut ALT + -. can set RStudio save .RData file workspace exiting IDE load file start-wish . RStudio also features code completion packages, functions arguments, objects workspace speed process writing code reduce potential errors arising typos; syntax highlighting easily distinguish functions, strings, values, objects; bracket matching save time writing troubleshooting code.","code":""},{"path":"tools.html","id":"the-rstudio-interface","chapter":"3 Tools","heading":"3.2.1 The RStudio interface","text":"Opening RStudio interface presents us set panes, contain set tabs access specific information R session.Console pane contains standard R console, code entered run, results returned.Files pane contains tabs let explore files current working directory navigate directories computer (Files); display export plots (Plots); see packages available currently loaded workspace (Packages); access help R general, specific packages, functions (Help).Environment pane contains tabs let see objects workspace current R session (Environment); see history commands sent R (History).Source pane lets inspect objects workspace (data frames, lists, etc.) write code scripts.console, line code evaluated interactively presses Enter. fine single lines code executed immediately cumbersome longer piece code requires several steps executed sequence. therefore preferable write code script Source pane, single command can run across multiple lines /series commands can written run together. new script file can created using File > New File > R Script using shortcut Ctrl + Shift + N. Unlike console, pressing Enter cause line code evaluated; adds new line script. run chunk code script, highlight desired lines (single lines code placing cursor beginning end line sufficient) press Ctrl + Enter. run code script, press Ctrl + Shift + Enter. Running code script sends console evaluated, results also returned. script can saved using File > Save ‚Ä¶ naming script .R extension.Opening Global Options menu (Tools > Global Options‚Ä¶) lets us customize appearance RStudio suit preferences. See Figure 3.2 customised layout RStudio.Pane Layout page, can change location panes interface suit preference assign tabs panes. example, moved Files tab pane Environment tab customisation RStudio.Appearance page lets us select RStudio theme, editor font font size, editor theme (background colour syntax highlighting) suit needs preferences.particularly useful feature available RStudio use Rainbow parentheses make easier match parentheses, brackets, braces writing code. Mismatching missing brackets common source errors coding turning Rainbow parentheses colour coding items script makes easier spot errors occur. turn Rainbow parentheses open Global options dialogue box Tools menu select Code followed Display. Check Rainbow parentheses box. Note work coding script source window. apply coding console, text always monochrome. another reason write R scripts Source pane rather console.\nFigure 3.2: customised layout RStudio interface. Text Source pane (top left) highlighted using customised version Monokai editor theme uses rainbow parentheses. Text Console (bottom left) monochrome. Environment pane (top right) lists objects currently workspace. Plots pane (bottom right) displays visualisations creating book. ‚òùÔ∏è\n","code":""},{"path":"tools.html","id":"projects","chapter":"3 Tools","heading":"3.2.2 Projects","text":"key advantage using RStudio simplifies process managing projects.working directory folder R looks data import scripts source, destination outputs created. widespread practice set working directory absolute path explicitly head script using setwd() function.advisable. Setting working directory way makes code easy break hard share. absolute path working directory works long directory structure computer running script change. script file containing code moved, directory structure changes time, code shared run different machine, path working directory longer valid. Every setwd() command every script project must changed make new valid paths working directory, tedious unnecessary.better approach employ robust workflow using projects RStudio package.starting new project RStudio (File > New Project‚Ä¶) working directory project set project folder, can created starting new project RStudio associated existing folder. .Rproj file, stores settings project, linked project folder. Opening existing project RStudio (File > Open Project) starts new R session automatically sets working directory session project folder loads .RData .RHistory files, plus project settings.project folder created associated .Rproj file, time create folders organise source data analysis, scripts contain code create, outputs produced. organisation R project depends nature research project, Figure 3.3 illustrates basic folder structure R project starting point thinking projects. .Rproj file top level project working directory set project folder paths folders files within project relative project folder. Using paths relative project folder means code work even location project folder changes.\nFigure 3.3: basic structure R project.\n","code":"\nsetwd(\"C:/Users/Nick/Documents\")"},{"path":"tools.html","id":"here","chapter":"3 Tools","heading":"3.2.2.1 here","text":"package makes working relative paths easy manage. ::() gets file path project folder current project (determines presence .Rproj file) lets us create paths relative folder. example, read .csv file Data folder project folder wrap file information () command within call read file (readr::read_csv()) list folders within project folder (‚ÄúData‚Äù) files (‚Äúdata.csv‚Äù) order.Similarly, write data frame .csv file Outputs > Data folder build path listing folders file name order.advantage using package longer need worry enter paths different platforms. MacOs Linux, paths written using single forward slashes separate folders (/), Windows machines paths files use single backlashes (e.g., C:\\Users\\Nick\\Documents). However, backslashes used escape characters strings, R recognise path Windows machine path necessary edit location folder use either double backslashes (C:\\\\Users\\\\Nick\\\\Documents) single forward slashes (C:/Users/Nick/Documents). tedious another way code can broken. problem easily eliminated using package, require us enter back- forward slashes.","code":"\n# Install the tidyverse (which includes the readr package) and \n# here packages if you have not already done so\npacman::p_load(here, tidyverse)\n\ndf <- readr::read_csv(here::here(\"Data\", \"data.csv\"))\nreadr::write_csv(df, here::here(\"Outputs\", \"Data\", \"data_output.csv\"))"},{"path":"tools.html","id":"a-script-for-the-folder-structure-of-a-project","chapter":"3 Tools","heading":"3.2.2.2 A script for the folder structure of a project","text":"created project folder associated .Rproj file can create function called project_folders() using package dir.create() function create basic folder structure need project.loop repeats specific piece code set number times, iterating values sequence. example, code function dir.create() applied every item (f) list folders. Using package can specify folder sub-folders created run another loop create sub-folders within Outputs directory. loops save time removing need repeatedly execute piece code often one slowest ways executing code R. However, book use loops can see code using step ‚Äì aim understanding rather speed. feel understand code, can experiment putting content loop function using R‚Äôs family apply functions, lapply() sapply(), run code efficiently.can save project_folders() function script ‚Äì let‚Äôs call project_folders.R ‚Äì can source run beginning new project. Make sure make note save script, creating folder structure one first tasks every chapter book: simply copy script project folder, source function using source(\"project_folders.R\") run function.","code":"\nproject_folders <- function(){  # no arguments are required\n  \n  # Load the here package\n  pacman::p_load(here)\n  \n  # Name the folders and sub-folders for the project\n  folders <- c(\"Data\", \"Scripts\", \"Outputs\")\n  Outputs_sub_folders <- c(\"Data\", \"Plots\")\n  \n  # Loop over the folders to create them in the project directory\n  for (f in folders){dir.create(f)}\n  \n  # Loop over the sub-folders to create them in the Outputs directory\n  for (s in Outputs_sub_folders){dir.create(here(\"Outputs\", s))}\n  \n}\n\n# To run the function\nproject_folders()"},{"path":"tools.html","id":"accessing-help","chapter":"3 Tools","heading":"3.2.3 Accessing help","text":"Help packages functions can accessed directly within RStudio. Vignettes describing packages illustrating use functions can accessed using browseVignettes(). example, see vignettes seewave package useThis open list vignettes web browser (NB: internet connection required). R packages vignettes, though many . get help function package currently loaded workspace, type help followed name function parentheses. example, get help spectro() function seewave package use . open documentation function help tab RStudio.RStudio also Tutorial tab learning use R requires learnr package.","code":"\nbrowseVignettes(package = \"seewave\")"},{"path":"tools.html","id":"ffmpeg","chapter":"3 Tools","heading":"3.3 FFmpeg","text":"process audio video need FFmpeg, free command-line based utility decoding, encoding, converting media files workhorse several packages across different languages media processing. FFmpeg can used encode/decode/transcode mux/demux multimedia files. can used blurring denoising, colour conversion, letterboxing, extracting individual frames, adding removing subtitles video files.MacOS, FFmpeg easily installed package Homebrew. Linux can installed via package manager. FFmpeg require installation Windows, necessary set environment path FFmpeg commands addressed executed.\nfollowing steps allow download FFmpeg set \nenvironment variable operating system.\n\nGo https://ffmpeg.org/download.html\nselect Windows logo get list download options \nWindows. Click Windows¬†builds¬†¬†gyan.dev go \nCodex FFmpeg repository.\n\nSelect latest full build:\nffmpeg-git-full.7z. begin downloading FFmpeg \ncompressed zip file default Downloads\nfolder.\n\nOpen default Downloads folder, right-click \nfile just downloaded select Extract\n. unpack compressed file new folder.\nRename folder FFmpeg.\n\nCut--paste folder Downloads\nfolder root directory PC (C:\\).\n\nOpen Windows search box (Windows + s) \ntype Environment variables search box.\nSelectEdit system environment variables open \nSystem Properties dialogue box.\n\nSystem Properties dialogue box click \nEnvironment variables‚Ä¶ button bottom right-hand\ncorner open Environment variables dialogue\nbox.\n\nUser variables <username> panel,\nselect Path click Edit.\n\nClick New type C:\\FFmpeg\\bin \nprompt. Click OK set path.\n\nClick OK Environment\nvariables dialogue box save changes. Click\nOK exit System Properties\ndialogue box.\n\nFFmpeg now set environment variable Windows. \ncheck , open Windows search box (Windows +\ns), type PowerShell, select\nWindows PowerShell results. window\nopened, enter ffmpeg prompt press\nEnter. FFmpeg set environment variable\ncorrectly PowerShell display summary version FFmpeg \nnow set PC.\nFFmpeg can used via command line can called passing system commands R operating system. streamline workflow, work within single environment enter FFmpeg commands via R using av package simplify code possible. example, convert .webm file .mp4 file without re-encoding data using FFmpeg within R pass following command system:Breaking :system() function passes command OS string ‚Äì note use quote marks around command.ffmpeg: tells OS entering command FFmpeg-input_video.webm: -indicates input command followed exact name video file convertedoutput_video.mp4: sets name output file. name output can prefaced -vcodec libx264 control encoding output file.process can simplified using av package R, provides R wrapper common FFmpeg operations. convert .webm file .mp4 using av::av_video_convert(), enterThis command performs exact conversion FFmpeg code command line readable.\nProgramming Historian comprehensive guide (Rodriguez, 2018) using FFmpeg \nlike learn .\n","code":"\nsystem(\"ffmpeg -i input_video.webm output_video.mp4\")\n# Load the av package\npacman::p_load(av)\n\nav::av_video_convert(\"input_video.webm\", output = \"output_video.mp4\")"},{"path":"tools.html","id":"summary-1","chapter":"3 Tools","heading":"3.4 Summary","text":"analytical presentational tools required computational film analysis project vary according demands project. R excellent choice can used many different types data subsequent chapters use R analyse sound, colour, editing, shot data, textual data, working directly audio files, images, subtitiles, .csv files containing numerical data derived films.","code":""},{"path":"audio.html","id":"audio","chapter":"4 Analysing film audio","heading":"4 Analysing film audio","text":"cinema, different elements film audio ‚Äì dialogue, music, sound effects, ambient noises, silence ‚Äì organised soundtrack contributes viewers‚Äô experiences motion pictures multiple ways. Soundtracks convey narrative information emotions; define mood film; shape pace action tempo film; contribute realism (hyperrealism) movie; establish location historical period; carrying thematic content across film; organise viewers‚Äô direct experience film smoothing cuts shots enabling transitions scenes, locations, different times. Sound adds visual content scene, often drawing attention important screen can modify perceive witnessing screen. Audio content therefore rich source information narrative structure, action, emotion motion pictures.Timmy Chih-Ting Chen describes sound design analytic concept thathelps us describe aesthetic stylistic construction elements soundtrack, attending sound mix comprising music, voice, sound effects, ambience, silence relation visual track. sonic counterpart visual design, sound design draws audience‚Äôs attention neglected aural dimension cinematic style (T. C.-T. Chen, 2016: 34).Computational analyses film style largely ignored role sound wide range audio analysis tools freely available applied understanding sound design cinema. chapter cover methods analysing sound cinema R.","code":""},{"path":"audio.html","id":"a-brief-digital-audio-primer","chapter":"4 Analysing film audio","heading":"4.1 A (brief) digital audio primer","text":"vibrations produced violin string, vocal cords, speaker cone move air molecules surrounding causing air pressure rise slightly. turn, molecules move air molecules next , move molecules next , , propagating pressure wave air. vibrations arrive aural receptors experience pressure wave sound.waveform shape graph pressure wave comprising audio signal. amplitude waveform change pressure peak wave trough. cycle wave amount time taken go one amplitude, way wave‚Äôs amplitude changes, reach amplitude frequency wave number cycles wave per second given Hertz (Hz). Figure 4.1, waveform goes four complete cycles second frequency 4 Hz.\nFigure 4.1: waveform audio signal frequency 4 Hertz.\nsound wave can captured, stored medium, reproduced. Analog media represent waveform sound directly continuous voltage analogous sound wave, whereas digital audio converted analog--digital converter represented discrete numbers (digits) computer readable form.digital audio, amplitude values sound‚Äôs waveform captured evenly spaced time points (Figure 4.2). sampling rate number samples per second given Hertz (Hz) kilohertz (kHz). film audio, standard sampling rate 48000 Hz 48 kHz. amplitude waveform single sample represented number, amplitude rounded nearest integer. process called quantisation. range numbers used represent sound determined bit depth. example, 4-bit quantisation audio file sixteen possible values can used represent amplitude sample \\(2^{4}=16\\). 16-bit quantisation, \\(2^{16} = 65536\\) possible values. Conventionally, possible amplitude values arranged symmetrically around zero. bit rate number binary digits per second used represent audio signal measured kilobits per second (kbps) equal product sampling rate, exponent bit depth, number channels. example, stereo signal sampling rate 48000 Hz bit-depth \\(2^{16}\\) bit rate 48000 √ó 16 √ó 2 = 1536000 1536 kpbs.\nFigure 4.2: analogue audio waveform, digitally sampled 32 kHz 4-bit quantisation.\nTogether sampling rate, bit depth, bit rate determine quality digital audio recording: increasing either sample rate bit depth improve quality sound, also increase size audio file. implications processing power required analysis film audio discuss practical considerations involved selection sample rates . Throughout chapter use audio bit depth 16-bits, provides good level dynamic range (96 dB) manageable file sizes.","code":""},{"path":"audio.html","id":"setting-up-the-project","chapter":"4 Analysing film audio","heading":"4.2 Setting up the project","text":"","code":""},{"path":"audio.html","id":"create-the-project","chapter":"4 Analysing film audio","heading":"4.2.1 Create the project","text":"created project RStudio new directory using New project... command File menu, can run script projects_folders.R created Chapter 3 create folder structure required project.","code":""},{"path":"audio.html","id":"packages-1","chapter":"4 Analysing film audio","heading":"4.2.2 Packages","text":"project use packages listed Table 4.1.\nTable 4.1: Packages analysing audio data.\npackages use analyse audio files R, including soundgen (Anikin, 2019) warbleR (Araya‚ÄêSalas & Smith‚ÄêVidaurre, 2017). functions packages make use seewave package, adding new methods acoustic analysis. warbleR designed working multiple files useful working data sets comprising large number audio files.","code":""},{"path":"audio.html","id":"data","chapter":"4 Analysing film audio","heading":"4.2.3 Data","text":"Insidious franchise comprises four supernatural horror films released 2011 2018. chapter, look trailers Insidious (2011), Insidious: Chapter 2 (2013), Insidious: Chapter 3 (2015). fourth film franchise, Insidious: Last Key, released 2018. leave exercise reader analyse soundtrack trailer see compares results presented .first two films franchise focus Lamberts, family haunted malevolent spirits astral plane called ‚Äú‚Äù. first film, Insidious (Video 4.1), son, Dalton, falls coma begins travelling ‚Äú‚Äù becomes possessed tortured soul possessed father, Josh, young boy. help demonologist Elise Rainier, Josh travels ‚Äú‚Äù rescue son spirit evil woman possessed child returns drives kill Elise.\nVideo 4.1: trailer Insidious (2011) ‚òùÔ∏è\nsecond film franchise, Insidious: Chapter 2 (Video 4.2), takes place events first film, police investigating death Elise Lambert family trying defeat ghosts seeking harm . time, Dalton enters ‚Äú‚Äù save possessed father spirit Elise, destroying ‚ÄúWoman White‚Äù haunted Josh throughout life.\nVideo 4.2: trailer Insidious: Chapter 2 (2013) ‚òùÔ∏è\nthird film franchise, Insidious: Chapter 3 (Video 4.3), prequel set seven years events first two films centers teenager Quinn Brenner, becomes possessed ‚ÄúMan Can‚Äôt Breathe‚Äù trying contact dead mother seance Elise.\nVideo 4.3: trailer Insidious: Chapter 3 (2015) ‚òùÔ∏è\nRather repeat process trailer, illustrate processing audio trailer Insidious: Chapter 3 include ouputs trailers come analysing sound design.","code":""},{"path":"audio.html","id":"extracting-the-audio","chapter":"4 Analysing film audio","heading":"4.2.3.1 Extracting the audio","text":"downloaded trailers .mp4 files YouTube placed Data folder, need process remove MPAA rating tag screens extract soundtracks audio file. can done variety ways, including using editing software trim video file export audio required format. use av package, functions wrapper FFmpeg. can get summary .mp4 file downloaded YouTube using av::av_media_info().file info see video file soundtrack sampling rate 44.1 kHz, standard sampling rate film audio typical YouTube videos. version soundtrack sampling rate 48 kHz can downloaded YouTube using youtube-dl .webm file, proceed version soundtrack hand. impact sampling rates analysis discussed .extract audio .mp4 file use av::av_audio_convert() create .wav file write Data folder. time trim first seven seconds trailer setting start_time = 7. section .mp4 file comprises MPAA green band trailer tag screen followed black screen accompanying audio. trailers tag screen, duration screen vary trailer trailer. common practice put announcement title beginning trailer, essentially trailing trailer video (see example), YouTube channels add additional promotional material trailer. important therefore check much material trimmed top tail trailer analysis.One consequence extracting audio way timings different original trailer YouTube. needs taken account associating events trailer features identified analysis.","code":"\npacman::p_load(here, av)\n\nav_media_info(here(\"Data\", \"insidious_chapter_3.mp4\"))\n## $duration\n## [1] 129.7763\n## \n## $video\n##   width height codec frames framerate  format\n## 1  1280    720  h264   3111  23.97602 yuv420p\n## \n## $audio\n##   channels sample_rate codec frames bitrate layout\n## 1        2       44100   aac   5589  125588 stereo\nav_audio_convert(here(\"Data\", \"insidious_chapter_3.mp4\"), \n                 output = here(\"Data\", \"insidious_chapter_3.wav\"), \n                 start_time = 7)"},{"path":"audio.html","id":"loading-and-pre-processing-an-audio-file","chapter":"4 Analysing film audio","heading":"4.2.3.2 Loading and pre-processing an audio file","text":"load audio file R use tuneR package, can load .wav .mp3 files using tuneR::readWave() tuneR::readMP3() functions, respectively. Audio files loaded using tuneR way Wave-class objects attributes can accessed using @ operator. can get summary audio file calling object audio created loaded file. confirms duration soundtrack now seven seconds shorter original .mp4 file.stage, audio working stereo. Moving forward, easier work mono audio object single audio object work (see discussion using mono versus stereo). convert soundtrack stereo mono, use tuneR::mono() function, lets us extract either left right channel average two channels. choose latter option set = \"\". check audio now mono Wave object can access stereo attribute, now returns value FALSE.can plot audio file simply calling R‚Äôs base::plot() function directly wave object audio. can also pass arguments change plot‚Äôs appearance consistent style plots produced ggplot2 package.\nFigure 4.3: mono waveform soundtrack Insidious: Chapter 3 trailer trimming.\n","code":"\npacman::p_load(tuneR)\naudio <- readWave(here(\"Data\", \"insidious_chapter_3.wav\"))\n\n# Get a summary of the audio object\naudio\n## Wave Object\n##  Number of Samples:      5415936\n##  Duration (seconds):     122.81\n##  Samplingrate (Hertz):   44100\n##  Channels (Mono/Stereo): Stereo\n##  PCM (integer format):   TRUE\n##  Bit (8/16/24/32/64):    16 \naudio <- mono(audio, which = \"both\")\n\n# Check that audio is no longer a stereo object\naudio@stereo\n## [1] FALSE\n# Set the parameters of the plot\npar(mgp = c(1.5, 0.4, 0),  # Set the position of the axis labels\n    tcl = -0.25,           # Set the length of the tick marks\n    cex.axis = 0.85,       # Set the size of the axis labels\n    col.axis = \"#444444\",  # Set the colour of the axis\n    mar = c(3, 4, 1, 1))   # Set the margins of the plot space: bottom, left, top, right\n\nplot(audio, bty = \"n\",  xaxt = \"n\", yaxt = \"n\", \n     col = \"#277F8E\", xlab = \"Time (s)\")\nbox(\"plot\", bty = \"l\", lwd = 2, col = \"#444444\")\naxis(side = 1, lwd = 0, lwd.ticks = 2, mgp = c(1.5, 0.2, 0))\naxis(side = 2, lwd = 0, lwd.ticks = 2, las = 2)\nmtext(\"Amplitude\", side = 2, line = 3)"},{"path":"audio.html","id":"visualising-the-soundtrack","chapter":"4 Analysing film audio","heading":"4.3 Visualising the soundtrack","text":"many structural features soundtrack readily apparent plot waveform, presence quiet loud sections abrupt onset audio events much harder describe features sections events different scales. section create visualisations trailer soundtracks show dynamic structure clearly.","code":""},{"path":"audio.html","id":"the-spectrogram","chapter":"4 Analysing film audio","heading":"4.3.1 The spectrogram","text":"next stage applies short-time Fourier transform (STFT) mono wave file produce 2D time-frequency representation signal called spectrogram. See Goodwin (2008) M. M√ºller (2015: 40-57) detailed discussions methods mathematical underpinnings.audio signal can represented sum series sine waves. Fourier transform mathematical function decomposes signal component sine waves different frequencies, amplitude sine wave representing amount power energy present signal frequency. analogy, Fourier transform sound prism light, splitting signal reveal spectrum frequencies make sound wave. can done matter complex audio signal interested. limitation Fourier transform assumes signal stationary ‚Äì- , assumes statistical properties signal alter time. Applying Fourier transform soundtrack trailer give us spectrum representing overall frequency content soundtrack averaged duration entire signal time tell us anything soundtrack evolves time. STFT overcomes limitation dividing signal series windows calculating Fourier transform window. result Fourier transform signal localised time depends shape (rectangle, Hanning, etc.), size (number samples within window, power 2), overlap windows successive local Fourier transforms. example, successive Fourier transforms overlapped 50% produce smoother spectrogram accurately reflects structure continuous audio signal.Calculating STFT signal give us matrix, columns time windows, rows frequency bands, cell values amplitude signal time window band frequencies. STFT visualised spectrogram. spectrogram describes magnitudes individual frequencies (shown y-axis) comprising signal vary time (represented x-axis), amplitude signal time-frequency window indicated colour.Figure 4.4 displays segments two signals one-second duration frequency components 329.63 Hz 493.88 Hz corresponding notes E4 B4, respectively. waveform Figure 4.4.superposition frequencies, notes present throughout duration signal. waveform Figure 4.4.B frequency components, present time: waveform shifts frequency E4 B4 0.5 seconds. Figures 4.4.C D show spectra waves amplitude peaks component frequencies. example evident Fourier transform distinguish stationary non-stationary signals; , see difference stationary signal statistical properties signal consistent time (Figure 4.4.) non-stationary signal components frequencies whose statistical properties vary time (Figure 4.4.B).spectrogram describes magnitudes individual frequencies comprising signal vary time. Time displayed x-axis, frequency y-axis, amplitude given frequency band within time window represented colour. Figures 4.4.E F present spectrograms shows two signals different temporal structures. size window determines temporal frequency resolution spectrogram. temporal resolution equal size window divided sampling rate frequency resolution sampling rate divided window size. Overlapping windows produces smoother spectrogram accurate representation continuous evolution frequencies signal.\nFigure 4.4: Segments two sounds notes E4 (329.63 Hz) B4 (493.88 Hz) arranged composite wave (), notes played continuously one second, sequential wave (B), changing pitch E4 B4 0.5 seconds. Despite different waveforms sounds periodograms composite wave (C) sequential wave (D) power spectral density. differences sounds clear see spectrograms: notes present duration composite wave (E), transition E4 note B4 sequential wave halfway time window clear see (F).\ntemporal resolution STFT equal window length (example, 2048) divided sampling rate: 2048/48000 = 0.0426 seconds per window. frequency resolution equal sampling rate divided window length: 48000/2048 = 23.44 Hz per band. clear reducing window size improve temporal resolution expense poorer frequency resolution; increasing window length improve frequency resolution expense temporal resolution. time frequency conjugate variables subject uncertainty principle: possible localize time frequency absolute precision. exists trade-time frequency resolution needs addressed design analysis order ensure resulting spectrogram meaningful representation signal relative want discover film‚Äôs sound design.two ways producing spectrogram using seewave package. spectro() function uses R‚Äôs base plotting functions, whereas ggspectro() function uses ggplot2. advantages. Using spectro() easier plot frequency axis spectrogram log scale order see easily happening lower frequencies. limitation approach calculating STFT way introduces matrix entries value -Inf whenever soundtrack encounters silence values transparent spectrogram. can avoided exporting audio directly editing software processing audio file using audio editor (e.g., Audacity Wavosaur) loading file plotting spectrogram using code demonstrated .\nFigure 4.5: spectrogram soundtrack trailer Insidious: Chapter 3 using seewave::spectro(). frequency axis plotted log-scale.\nUsing ggspectro() can take advantage ggplot2‚Äôs wide range commands customise appearance plot , ggplot() object, can combined plots create single figures. ggspectro() function replaces ggplot() function creating plot ggplot2, ggplot2 commands remain . seewave documentation ggspectro() includes examples rely geom_tile() function draw spectrogram, geom_raster() quicker uses less memory produce better quality image.\nFigure 4.6: spectrogram soundtrack trailer Insidious: Chapter 3 using seewave::ggspectro().\nSTFT values still take value -Inf (STFT matrix produced using spectro() using parameters), setting na.value = \"#440154\" values take minimum value amplitude scale using viridis palette appear transparent.\nggplot2 package maps aesthetic attributes data set \nproduce highly-customisable publication-ready visualisations based \nLeland Wilkinson‚Äôs Grammar Graphics (Wilkinson, 2005). Hadley Wickham‚Äôs\nggplot2: Elegant Graphics Data Analysis (2016) presents detailed\nintroduction ggplot2 package. online version available .\n\nggplot2 accepts data frames . data stored\ndata frame need converted data frame prior \nplotting.\n\ngeom determines geometry data visualised\nline (geom_line), cloud points\n(geom_point), bar (geom_bar \ngeom_col), text (geom_text), \nmany different ways visualising data. Plots built \nlayers, geom listed first plot created \nbottom layer. Subsequent geoms overlay layer \norder called. Note means higher layers may\nobscure features lower layers necessary consider \norder geoms plotted ensure data visualised\neffectively.\n\naesthetics geom set mapping variables data\nframe listed aes() command, includes \nvariables data frame set x y axes, \ngrouping data, variables associated scale \ncolour, fill, shape. (Note ggplot2 works English \nAmerican spellings ‚Äòcolour‚Äô).\n\nappearance plot can controlled formatting axes,\napplying colour schemes variables, applying coordinate system (\nCartesian polar co-ordinates), splitting data set \nsubsets based variable (called facets), applying theme \nmodify non-data elements plot (titles, axis labels,\nplot margins, legends, etc.).\n\nbook always construct plots using ggplot2 \nway.\n\ncreate plot using ggplot() function\ndata plot typically passed function \nstage.\n\nGeoms added plot order need \nlayered. different geoms require different data, passed\nrespective geoms stage. Aesthetics typically \nmapped geoms created.\n\nFeatures plot set, including formatting axes \nguides, coordinates plot place graphic window,\nuse facets visualise subsets small multiples.\n\nFinally, select theme format appearance \nplot setting parameters theme. See \ngallery themes ggplot2 package.\n\nnecessary plot features order. \ncreate R object containing plot (p) using ggplot2 \nadditional geoms add modify themes later: example,\np + theme_dark() apply dark theme object\np.\n\nmany R packages extending functionality ggplot2,\nadding new geoms expand range plots can produce, adding new\nfeatures control data displayed (e.g., controlling overlap\ndata labels, adding marginal plots, interactivity, new themes, etc.),\ncombining multiple plots together make single figure.\n","code":"\npacman::p_load(seewave, viridis)\n\nspectro(audio,\n        f = audio@samp.rate,           # the sampling rate\n        wl = 2048,                     # window length in samples\n        wn = \"hanning\",                # the shape of the window\n        ovlp = 50,                     # overlap of the windows\n        norm = TRUE,                   # amplitude is normalised to 0.0 dB\n        fastdisp = TRUE,               # draws the plot quicker\n        flog = TRUE,                   # plot frequencies on a log scale\n        collevels = seq(-200, 0, 10),  # the range of the amplitude scale\n        palette = viridis,             # colour palette selection\n        axisX = FALSE)                 # suppress the x-axis\n\n# Label the x-axis every 20 seconds from 0 to 120 seconds\naxis(1, at = seq(0, 120, 20), labels = seq(0, 120, 20))\npacman::p_load(tidyverse)\n\nggspectro(audio, wl = 2048, wn = \"hanning\", ovlp = 50) + \n  geom_raster(aes(fill = amplitude), interpolate = TRUE) +\n  scale_x_continuous(expand = c(0, 0), breaks = seq(0, 120, 20)) +\n  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 20, 4)) +\n  scale_fill_viridis_c(name=\"Amplitude (dB)\", na.value = \"#440154\") + \n  guides(fill = guide_colourbar(barwidth = 20, \n                                barheight = 1, \n                                title.position = \"top\")) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")"},{"path":"audio.html","id":"the-normalised-aggregated-power-envelope","chapter":"4 Analysing film audio","heading":"4.3.2 The normalised aggregated power envelope","text":"limitation visualising soundtrack using spectrogram can difficult characterise temporal evolution specific features. Additionally, possible see relationships different elements soundtrack different scales.normalised aggregate power envelope (nape) plots evolution power soundtrack, making easier see structure soundtrack, relationships various parts soundtrack different scales, shape local features. also easier compare soundtracks using visualisation waveform spectrogram.calculate normalised aggregated power envelope Insidious: Chapter 3 trailer use acoutsat() function seewave package, keeping settings used produce spectrogram. envelope simply sum columns matrix comprising STFT normalised unit area (Cortopassi, 2006).obtain time contour produced acoustat(), use $ operator access element list results returned function. return two column matrix containing times individual spectra comprising spectrogram aggregate amplitude values spectra. set plot = FALSE can draw plot data using ggplot2::ggplot().ggplot2::ggplot() expects data data.frame need convert object insidious_chapter_3_nape, currently class ‚Äòmatrix‚Äô data frame using .data.frame(). name object df_insidious_chapter_3_nape make clear data frame.\nR everything object object belongs \nclass serves blueprint object. \nclass object determines functions interact \nobject, functions require objects belong \nparticular class.\n\ncheck class object R use class()\nfunction.\n\nsee insidious_chapter_3_nape class\nmatrix class array. R,\nmatrix class subset class\narray; specifically, matrix class object \narray two dimensions.\n\nchange class object prefix target class \nobject coerced .. example, \nconvert data frame matrix use .matrix(x),\nx name data frame converted.\n\ntypeof() function can used obtain additional\ninformation object.\ncan save data frame containing normalised aggregated power envelope trailer data folder .csv file using write_csv() function readr package, part tidyverse suite packages loaded earlier. can read file using readr::read_csv() rather repeat whole analytical process scratch.Now lets plot normalised aggregated power envelope soundtrack trailer Insidious: Chapter 3 (Figure 4.7).\nFigure 4.7: normalised aggregated power envelope soundtrack trailer Insidious: Chapter 3.\n","code":"\ninsidious_chapter_3_nape <- acoustat(audio, \n                                     wl = 2048, \n                                     wn = \"hanning\", \n                                     ovlp = 50, plot = FALSE)$time.contour\n\nhead(insidious_chapter_3_nape)\n##            time      contour\n## [1,] 0.00000000 0.000000e+00\n## [2,] 0.02322874 0.000000e+00\n## [3,] 0.04645748 0.000000e+00\n## [4,] 0.06968622 0.000000e+00\n## [5,] 0.09291495 0.000000e+00\n## [6,] 0.11614369 4.582695e-09\n# Convert the matrix to a data frame\ndf_insidious_chapter_3_nape <- as.data.frame(insidious_chapter_3_nape)\n\n# Save as a csv file using readr::write_csv()\nwrite_csv(df_insidious_chapter_3_nape, here(\"Data\", \"insidious_chapter_3_nape.csv\"))\n\n# To read the csv file using readr::read_csv()\ndf_insidious_chapter_3_nape <- read_csv(here(\"Data\", \"insidious_chapter_3_nape.csv\"))\n# Plot the normalised aggregated power envelope\nggplot(data = df_insidious_chapter_3_nape) +\n  geom_line(aes(x = time, y = contour), colour = \"#277F8E\") +\n  scale_x_continuous(name = \"Time (s)\", breaks = seq(0, 120, 20)) +\n  # Add an extra line after the axis title to move it away from the tick labels with \\n\n  scale_y_continuous(name = \"Normalised aggregated power\\n\") +\n  theme_classic()"},{"path":"audio.html","id":"sound-design-in-trailers-for-the-insidious-franchise","chapter":"4 Analysing film audio","heading":"4.4 Sound design in trailers for the Insidious franchise","text":"","code":""},{"path":"audio.html","id":"structure","chapter":"4 Analysing film audio","heading":"4.4.1 Structure","text":"Redfern (2020c) showed trailers US horror films can typically divided three sections:narrative: establishing character, locations, time-frames, plotemotional: creating monomaniac version film characterised heightened emotional intensitymarketing: key promotional information trailer (title, release date, social media information, etc.) presented audience.pattern clear plots normalised aggregated power envelopes three trailers Figure 4.8. Although trailers different running times (Insidious: 102.21s; Insidious: Chapter 2: 140.18s; Insidious: Chapter 3: 122.81s), structurally similar normalising running time range [0-100%] brings commonalities.\nFigure 4.8: normalised aggregated power envelope soundtracks trailers Insidious franchise\nthirty-eight second difference running time trailers Insidious Insidious: Chapter 2 increase power soundtracks trailers occurs approximately ~38% way trailer trailer shifts narrative phase emotional phase. first part emotional phase power soundtracks shows trend either trailer, show linear increase power ~60% ~78%, point trailers move emotional phase marketing phase.trailers Insidious Insidious: Chapter 2 relate haunting Lambert family, third film franchise prequel set several years events first two movies must establish place within franchise. Consequently, trailer Insidious: Chapter 3 devotes greater proportion running time narrative, blending narrative function increasing level anxiety fully giving emotional intensity trailer. emotional phase trailer power soundtrack increases steadily time looks corresponding section trailers albeit compressed much shorter section just like trailers, point soundtrack‚Äôs power increases linearly occurs 60% way trailer.","code":""},{"path":"audio.html","id":"acoustic-startle-events","chapter":"4 Analysing film audio","heading":"4.4.2 Acoustic startle events","text":"Acoustic startle events generate startle response audience abrupt onset loud sound (Sbravatti, 2019). Different types affective events different audio envelopes (Figure 4.9): Type-1 events characterised step-edge attack, sustain, step-edge release creating jump scare audience; Type-3 events step-edge attack, long sustain, unspecified release stage, also jolting audience maintaining heightened emotional intensity period time; Type-5 events step-edge attack, sustain, long release power startle event falls away characters react shock (audience) just experienced. (Type 2 Type 4 affective events slope attack generate startle repsonse; see Redfern (2020c) discussion different types affective events horror film trailers).\nFigure 4.9: Schematic representations acoustic startle events.\nPlotting timing type acoustic startle events trailers (Figure 4.10) see clear pattern timing functions. First, acoustic startle events occur narrative phase trailer. Second, Type-1 events occur emotional phase classic examples basic jump scare horror cinema (Baird, 2000). two Type-1 events trailer Insidious: Chapter 2 make audience jump, marking turning points trailer, first shift narrative stage horror stage, second soundtrack begins increase power emotional content trailer intensifies. Interestingly, Type-1 events last 20% trailers. Third, Type-3 Type-5 events occur later trailers associated marketing functions trailers, occurring final fifth trailer. use acoustic startles marketing phase attentional rather emotional, sustain slope decay envelope events exploits fact human beings evolved enter state hypervigilance three ten seconds following sudden redirect attention following loud, abrupt, intrusive sound environment sound (Dreissen et al., 2012). attack startle event draws attention screen attention sustained told information film‚Äôs release need remember watch film released.\nFigure 4.10: onsets acoustic startle events trailers films Insidious franchise..\nmultiple functions acoustic startle events illustrated second acoustic startle event trailer Insidious: Chapter 3. Type-5 startle event occurring 110.7 seconds presents audience abrupt transition silence loud noise abrupt increase power across full range frequencies soundtrack, slope decay envelope means power soundtrack drops quickly first slows soundtrack transitions intense, abrasive rapid onset acoustic startle tune ‚ÄòTiptoe Tulips‚Äô plays title cards announcing title film upcoming release. Mixing different sounds single event allows trailer seamlessly bridge non-sequitur cut away action title card also hold audience‚Äôs attention time looking screen marketing information presented. startle frightening ‚Äì makes jump ‚Äì primary function attentional serves convey narrative information film create emotional experience viewer isolation presenting marketing information viewer. employs sound design fulfil trailer‚Äôs primary function selling movie.","code":""},{"path":"audio.html","id":"non-linear-mixing","chapter":"4 Analysing film audio","heading":"4.4.3 Non-linear mixing","text":"earlier computational analyses film audio, showed exponential growth decay (see Computational film analysis can save life ) key part sound design horror films trailers (Redfern, 2020b, 2021c), ability recognise interpret features key part applying computational analysis film audio. Figure 4.11 plots envelope final affective event Insidious: Chapter 3 trailer linear (Figure 4.11.) logarithmic (Figure 4.11.B) axes. log-scaled version shows us really happening power soundtrack decays startling audience reveals exponential decay part sound design trailer.\nFigure 4.11: evolution affective event trailer Insidious: Chapter 3 normalised aggregated power plotted () linear (B) logarithmic axes.\nshape decay Figure 4.11.B raises interesting question: sound designer editor put part soundtrack together? base level log-scaled version event appears almost perfectly straight interesting discover extent type non-linear sound mixing intentional part filmmakers. filmmakers consciously designing features exponential rates growth decay soundtracks? , producing features controlling parameters audio envelope ‚Äòhand‚Äô? features result software components (e.g., spline curves, plug-ins, etc.) digital audio workstations used filmmakers necessarily aware exponential evolution affective event? Adobe Audition, example, allows users select several types fades, including linear, logarithmic, cosine fades (Adobe, 2021). raises questions features first appeared horror film audio. non-linear sound mixing always part horror film soundtracks? features appear introduction digital audio workstations practice film sound? normalised aggregated power envelope can thought trace set aesthetic practices provides basis exploring ways filmmakers produce soundtracks.\n2013 article, ‚ÄòFilm studies statistical literacy‚Äô, wrote\n\n\n‚Ä¶ statistics found every area research cinema\nstatistical literacy comprises set skills attitudes\nfilm scholars possess order comprehend evaluate\nresearch;nothing Film Studies fear looking\nbeyond humanities; introducing statistical literacy \ncurriculum better prepare students (lecturers) work \nlife (Redfern, 2013a: 67-68).\n\nneed ensure students acquire statistical literacy never\napparent coronavirus pandemic began \n2019.\n\npandemic, phrase exponential\ngrowth became part everyday vocabularies. However, \nlong known , general, people difficulty understanding\nnature implications exponential growth, exhibiting strong\nsystematic bias towards linear growth leads \nunderestimate processes (interest rates, memes, viruses,\netc.) evolve time (Levy & Tasoff, 2016).\n\nstatistically illiterate perceive phenomenon \nexponential growth correctly, failing grasp exponential growth\nproceeds slowly first growing rapidly, statistically\nliterate interpret chart correctly showing exponential\ngrowth throughout.\n\nmuch easier identify phenomena characterised exponential\ngrowth plot y-axis logarithmic scale, causes \nnon-linear growth curve appear straight line. clear \nchart curve describes phenomenon grows\nexponentially distinct stages part single\nphenomenon single interpretation.\n\nLammers et al. (2020) found \ncoronavirus pandemic people exhibited exponential-growth\nbias, tending view growth virus linear terms \nthereby underestimate scale evolution pandemic. also\nfound correcting misconceptions exponential growth lead \nreduction exponential-growth bias resulting increased levels \nsupport virus-mitigation strategies social distancing.\n\nFaced highly-transmissible virus, comprehending concept \nexponential growth becomes matter life death.\n\nseen, exponential growth decay part sound\ndesign horror film trailers able recognise interpret\nfeatures key part applying computational film analysis \naudio texts. Teaching methods used chapter demonstrated\nresearch sound design horror cinema (Redfern, 2020c, 2020b, 2021c) introduced students concept\nexponential growth, allowing obtain level statistical\nliteracy urgent real-world applications.\n\n2012, British Academy reported \n\nhigher education, almost disciplines require quantitative\ncapacity, students often ill-equipped cope \ndemands. leave university skills inadequate needs\nworkplace ‚Äì- business, public sector academia (British Academy, 2012: 2).\n\nFilm educators must address failure teach quantitative skills\npart study film, ensuring students graduate \nconfidence ability work quantitative data methods, \nrecognise consequences failing embed statistical literacy\nwithin film education trivial. may matter life \ndeath.\n","code":""},{"path":"audio.html","id":"practical-considerations","chapter":"4 Analysing film audio","heading":"4.5 Practical considerations","text":"section, address practical considerations audio analysis relating use mono stereo tracks, sampling rate, normalisation audio files.","code":""},{"path":"audio.html","id":"monovstereo","chapter":"4 Analysing film audio","heading":"4.5.1 Mono versus stereo","text":"calculate spectrogram normalised aggregated power envelope converted stereo soundtrack trailers mono. inevitably results loss information, easier work single channel audio little purpose analysing left right channels stereo signal additional information gained. analyse spectrograms normalised aggregated power envelopes channels original stereo soundtrack, increase amount work involved analysing audio without necessarily adding understanding film‚Äôs sound design.times desirable work stereo audio files, left right channels contain directional information lost mixing rendering mono track, resulting analysis failed address key component sound design.decision use mono stereo soundtracks depend design analysis.","code":""},{"path":"audio.html","id":"samprate","chapter":"4 Analysing film audio","heading":"4.5.2 Sampling rate","text":"assess impact sampling rates outcomes analysis, can compare mono soundtrack trailer Insidious: Chapter 3 sampling rates 44.1 kHz, 32 kHz, 22.05 kHz.can downsample audio object using tuneR::downsample() function. downsampled audio files, can produce spectrograms normalised aggregated power envelopes using code employed earlier using seewave::ggspectro().Figure 4.12 plots spectograms three versions soundtrack, parameters used case (.e., hanning windows 2048 samples overlapped 50%). range frequencies can plotted different spectogram range maximum frequency can represented equal half sampling rate accordance Nyquist-Shannon sampling theorem. Thus audio_44100, sampling rate 44.1 kHz, Nyquist frequency equal 22.05 kHz; whereas audio_22050, sampling rate 22.05 kHz, Nyquist frequency 11.025 kHz.\nFigure 4.12: spectograms sound track trailer Insidious: Chapter 3 three different sampling rates\nDownsampling audio files inevitably results loss information. Downsampling 44.1 kHz 32 kHz, lose information frequencies 16 kHz, appear concern . see Figure 4.12.relatively little information 16 kHz audio extracted original .mp4 file downsampling rate 32 kHz (Figure 4.12.B) means lose much data. despite normal range human hearing 20 Hz 20 kHz, people hear sounds 16 kHz shelving sounds frequency reduces file sizes without affecting perception soundtrack.Downsampling 22.05 kHz (Figure 4.12.C) means lose even information, still able adequately represent information soundtrack. Figure 4.13 shows normalised aggregated power envelopes three different sampling rates similar. Using sampling rate lower 48 kHz 44.1 kHz lead us overlook mis-characterise key features soundtrack trailer. Halving sampling rate 44.1 kHz 22.05 kHz reduces number short-time spectra spectrogram half envelope plotted y-axis re-scaled resolution time axis remains sufficiently high even though loss temporal precision relative original version.\nFigure 4.13: normalised aggregated power envelope sound track trailer Insidious: Chapter 3 three different sampling rates\ndecision sampling rate use involves trade-amount information lost relative increase processing speed gained. Working trailer soundtracks couple minutes duration reason reduce sampling rate soundtrack computational gains achieved significant. Analysing soundtrack 120-minute feature require substantially computational resources (e.g., memory, processing time, etc.) downsampling lower sampling rate one strategy reducing demands still allowing us represent soundtrack sufficient level quality analysis.","code":"\n# Load the original audio file and convert to mono\naudio <- readWave(here(\"Data\", \"insidious_chapter_3.wav\"))\naudio <- mono(audio, which = \"both\")\n\n# Create an object from the original audio to identify\n# that it has a sampling rate of 44100 Hz\naudio_44100 <- audio\n\n# Downsample to 32000 Hz\n# Note that tuneR::downsample() expects the sampling rate in Hertz\naudio_32000 <- downsample(audio, 32000)\n\n# Downsample to 22050 Hz\naudio_22050 <- downsample(audio, 22050)"},{"path":"audio.html","id":"normalisation","chapter":"4 Analysing film audio","heading":"4.5.3 Normalisation","text":"R functions employed default settings apply peak normalisation audio file. Peak normalisation applies constant amount gain signal bring peak amplitude target level, examples 0.0 dB. time contour plot soundtrack calculated sum energy spectra spectrogram identical peak normalisation used normalisation rescale amplitude without altering dynamic range soundtrack amount gain applied entire signal. Normalising soundtrack therefore impact outcome analysis.","code":""},{"path":"audio.html","id":"summary-2","chapter":"4 Analysing film audio","heading":"4.6 Summary","text":"Mark Kerins argued studies sound cinema must find ways go beyond reliance methods vocabulary musicology establish ‚Äòcollective vocabulary moves beyond classical terms music encompass rich variety texture sounds used cinema audio-visual media‚Äô (Kerins, 2008: 11).methods demonstrated chapter go way addressing Kerins‚Äôs call vocabulary allows us speak soundtracks integrated way, using graphical approach adopts methods bioacoustics. vocabulary film analysis, graphical methods demonstrated descriptive, analytical, communicative power. normalised aggregated power envelope makes easy visualise soundtrack film, pick features interest, share information. allows us achieve integrated understanding sound design cinema letting us look evolution soundtrack time see relationships audio features different scales, enabling us see simultaneously global structure soundtrack focus individual features local level. Furthermore, graphical language shared sound engineers, mixers, designers create film soundtracks: waveform, Fourier transform, frequency analysers, spectrogram features digital audio workstations used film production. much audio production now takes place computer, , significant degree, visual practice makes sense analysis film sound use visual vocabulary produce sounds analyse.","code":""},{"path":"colour.html","id":"colour","chapter":"5 Analysing film colour","heading":"5 Analysing film colour","text":"Like sound design, colour shapes experience cinema many different ways. Different genres associated different colour schemes different levels brightness saturation (.-P. Chen et al., 2012; Redfern, 2021a) can identify genre use warm cool colours, level lightness image, level saturation (Bellantoni, 2005). Colours within film carry meanings determined local economy individual text, associated narratively significant objects, characters, locations, themes, nonetheless exist within culturally codified meanings colour shared audiences belonging particular culture. Colour acts unifying factor, fusing together different aspects film style including production design, costume design, lighting, cinematography, post-production colour processing create coherent world viewer (Rotem, 2003). Changes dominant colour film often mark changes mood, location, evolution characters. Colour organises attention viewer, separating foreground background drawing attention important within frame. Colour sets mood film, conveying emotional information internal lives characters state -screen world also eliciting emotional repsonses viewer, influencing feel intensity emotions (Detenber et al., 2021).chapter cover use R analyse colour motion pictures. learn sample frames film using av package pass commands FFmpeg; calculate average colour colourfulness frame; create colour palette film using cluster analysis; analyse temporal structure colours film using movie barcodes.","code":""},{"path":"colour.html","id":"colour-models-and-spaces","chapter":"5 Analysing film colour","heading":"5.1 Colour models and spaces","text":"colour model mathematical model describing ways colour can represented ordered, immutable list numbers tuple.colour space geometrical model maps colour model onto actual colours define range colours capable represented determined gamut (range colour space‚Äôs chromaticity), gamma (non-linear distribution luminance values), whitepoint (standard illuminant defines colour temperature white within colour space).chapter work three colour models colour spaces: RGB, L**b*, LCH(ab).","code":""},{"path":"colour.html","id":"rgb","chapter":"5 Analysing film colour","heading":"5.1.1 RGB","text":"RGB colour model (Figure 5.1) additive colour model colours defined chromaticity red, green, blue primaries. Colours specified tuple RGB values one number colour channel, channel represented scale \\([0, 1]\\) \\([0, 255]\\). example, red maximum amount red light green blue light given tuple \\((1, 0, 0)\\) \\((255, 0, 0)\\), whereas magenta maximum amount red blue light green specified \\((1, 0, 1)\\) \\((255, 0, 255)\\). values RGB tuple equal specified colour achromatic, ranging black \\((0, 0, 0)\\) white (\\((1, 1, 1)\\) \\((255, 255, 255)\\)), greys lying extremes. example, middle grey \\((0.5, 0.5, 0.5)\\) \\((128, 128, 128)\\).different scales used represent RGB colours equivalent easy convert multiplying dividing 255. standard way represent RGB colours R. packages use \\([0, 1]\\) scale others us \\([0, 255]\\). consistency, use latter RGB colours expressed image video processing software.RGB colours can also specified using hex codes converting tuple decimal numbers hexadecimal format. typically numbers represented R plotting. channel described two character hex code range \\([00, FF]\\) respects ordering RGB tuple form hex triplet: \\(\\#RRGGBB\\). Thus, hex code red \\(\\#FF0000\\) hex code mid-gray \\(\\#808080\\).\nFigure 5.1: RGB colour space (18-bit) (CC--SA 4.0: Kjerish)\nRGB common colour model used devices project light, default colour space televisions computer monitors. Colour spaces using RGB colour model include sRGB, colour space world wide web; Rec.709, colour space used high definition television; Rec.2020, colour space used ultra-high definition television.RGB perceptually uniform colour model ‚Äì , distances two colours proportional Euclidean distance differences RGB values reflect differences perceive colours. use RGB colour values project display colours screen, performing calculations plotting distribution colours use perceptually uniform colour spaces.","code":""},{"path":"colour.html","id":"cielab","chapter":"5 Analysing film colour","heading":"5.1.2 CIELAB","text":"CIELAB colour model L**b* perceptually uniform colour space defined three attributes:Lightness (L*): perceptual lightness colour range \\([0\\%, 100\\%]\\)*: chromaticity colour green (-*) ‚Äì red(+*) axisb*: chromaticity colour blue (-b*) ‚Äì yellow (+b*) axisAchromaticity occurs * = b* = 0, ranging black (L* = 0%) white (L* = 100%), greys lying limits. Figure 5.2 plots swatches CIELAB colour space range lightness values.\nFigure 5.2: Swatches CIELAB colour space range lightness values.\nrange * b* theoretically unlimited, though typically clamped range \\([-128, 127]\\). Note true colorspace package, clamps range \\([-100, 100]\\). Converting colour spaces done package project avoid errors resulting different ways representing colours. example, converting RGB colours CIELAB using colorspace package converting back RGB farver package reproduce original RGB colours packages express ranges * b* differently. Either package can used purposes analysis, best use one package colour conversion project. chapter use farver package.","code":""},{"path":"colour.html","id":"lchab","chapter":"5 Analysing film colour","heading":"5.1.3 LCH(ab)","text":"L**b* colour space perceptually uniform intuitive. LCH(ab) polar representation L**b* colour space much easier understand. LCH(ab) three colour attributes:Lightness (L): identical L* value defined L**b* colour space aboveChroma (C): saturation colour measured distance achromatic axis range \\([0\\%, 100\\%]\\)Hue (H): basic colour represented angle colour wheel, red = 0¬∞, yellow = 90¬∞, green = 180¬∞, blue = 270¬∞.Achromatic colours occur C = 0%, vary lightness, black (L = 0%) white (L = 100%), arbitrary values hues. Figure 5.3 plots slices LCH(ab) colour space range lightness values.\nFigure 5.3: LCH(ab) colour space sliced range lightness values\ncan calculate saturation colour lightness chroma:\\[\nS_{ab} = 100 \\times \\frac{C_{ab}}{\\sqrt{C_{ab}^{2} + L_{ab}^{2}}} ,\n\\]saturation represents proportion pure chromatic colour total colour experience.","code":""},{"path":"colour.html","id":"set-up-the-project","chapter":"5 Analysing film colour","heading":"5.2 Set up the project","text":"","code":""},{"path":"colour.html","id":"create-the-project-folder","chapter":"5 Analysing film colour","heading":"5.2.1 Create the project folder","text":"first tasks create project RStudio new folder called Colour working directory chapter contain associated .rproj file. step completed, can run script projects_folders.R created Chapter 3 create folder structure required project.create additional folders course project.","code":""},{"path":"colour.html","id":"packages-2","chapter":"5 Analysing film colour","heading":"5.2.2 Packages","text":"chapter use packages listed Table 5.1.\nTable 5.1: Packages analysing colour data.\n","code":""},{"path":"colour.html","id":"fuelled","chapter":"5 Analysing film colour","heading":"5.2.3 Fuelled","text":"chapter data set Fuelled (Video 5.1), short animated film created students Sheridan College, Ontario, premiered YouTube December 2021.film tells story Cathy, cat widowed dog murders husband, sets find killer. car runs fuel forest finds also run money, Cathy resorts stealing gas station, leading assault attendant leading explosion destroys gas station. Realising become like killer chasing, Cathy calls 911 waits.\nVideo 5.1: Fuelled (2021) ¬© KilledtheCat Productions. ‚òùÔ∏è\ncan download film YouTube place Data folder. Using av::av_media_info() can get summary video file, including duration film seconds information width, height, number frames, frame rate video sampling bit rates number audio samples audio, well video audio codecs.","code":"\n# Load the here and av packages\npacman::p_load(av, here)\n\n# Get a summary of the video file fuelled.mp4\nav_media_info(here(\"Data\", \"fuelled.mp4\"))\n## $duration\n## [1] 542.6735\n## \n## $video\n##   width height codec frames framerate  format\n## 1  1280    536  h264  13024        24 yuv420p\n## \n## $audio\n##   channels sample_rate codec frames bitrate layout\n## 1        2       44100   aac  23371  128004 stereo"},{"path":"colour.html","id":"sample-and-reduce","chapter":"5 Analysing film colour","heading":"5.2.3.1 Sample and reduce","text":"frame Fuelled made 1280 536 pixels, pixel comprised three channels RGB colour space. Therefore, represent complete colour information single frame need \\(3 \\times 1280 \\times 536 = 2,058,240\\) numbers. frame rate 24 frames-per-second running time 542.6735 seconds, total 13024 frames. completely represent colour information Fuelled requires \\(2,058,240 \\times 13,024 = 26,806,517,760\\) data points. data set comprising almost 27 billion data points large analysis ‚Äì represents small dataset (many data points needed two-hour long 4K Ultra HD film 60fps?). Furthermore, much data redundant frames scene similar one another.analyses film colour therefore based two-stage process order reduce amount data manageable level eliminate redundant data. First, frames sampled film, selecting either every n-th frame n frames per second. second stage reduce colour data sampled frame small set values single value represents frame, average colour frame, colourfulness, colour attributes (lightness, chroma, saturation, etc.).example, movie barcodes popular way representing colour information film constructed selecting set frames film either reducing frame single pixels width smoothing data averaging colours within selected frames. represent colour information Fuelled barcode.methods employed computational analyses film colour, including cinemetrics, z-projections, palettes, treemaps, Color_dT plots (see Flueckiger & Halter (2020) overview), apply version sample--reduce approach.Fuelled .mp4 file want extract large enough number frames sample representative film without oversampling redundant information serve slow processing data.function av::av_video_images() splits video file frames, number frames sampled controlled fps argument. sampling rate \\(\\frac{1}{1}=1\\) return one frame-per-second. Increasing numerator increase sample rate ‚Äì sampling rate \\(\\frac{2}{1}=2\\) sample two frames every second; whereas increasing denominator reduce sampling rate ‚Äì sampling rate \\(\\frac{1}{2}=0.5\\) sample one frame two seconds. Sampling Fuelled rate two frames-per-second give us data set 1085 frames analyse.can create folder store frames within Data folder directly within call av_video_images(). Sampled frames can either .jpg .png files, latter returning lossless images larger file sizes.\npart chromaR toolkit analysing colour motion\npictures, Tommaso Buonocore provided MATLAB script\ncreate .csv file containing average colour\nsRGB colour space every frame video file. can\naccess script (videoProcessing.m) GitHub repository chromaR.\n\norder run script need access MATLAB,\nmuch quicker performing process sampling\nframes calculating average colour R. However, MATLAB \npaid-software cheap.\n\naccess MATLAB afford \npurchase , produced alternative version Buonocore‚Äôs\nMATLAB script run Octave, \nfreely-available open-source alternative MATLAB. can find \nscript instructions using GitHub repository: VideoProcessingOctave.\n\n.csv file containing RGB values frame\ncreated script can used colour analysis chromaR \nmethods illustrated instead creating data frame \nsampling processing frames film.\n","code":"\nav_video_images(here(\"Data\", \"fuelled.mp4\"), destdir = here(\"Data\", \"Frames\"), \n                format = \"jpg\", fps = 2)"},{"path":"colour.html","id":"analysing-fuelled","chapter":"5 Analysing film colour","heading":"5.3 Analysing Fuelled","text":"","code":""},{"path":"colour.html","id":"the-average-colour-of-a-frame","chapter":"5 Analysing film colour","heading":"5.3.1 The average colour of a frame","text":"One common ways reducing colour data frame calculate average colour, defined tuple average values individual attributes colour space. , find average colour frame CIELAB colour space need calculate average L*, *, b* attributes separately form tuple averages. Typically, average used mean, median mode can also used determine average colour frame (Plutino et al., 2021).First, get list images frames folder using list.files() function. want include .jpg files set pattern = \"*.jpg$, * wildcard ignore part filename extension .jpg adding $ extension tell R find pattern jpg end string filename image.loop frames sampled Fuelled extract red, green, blue colour channels image. RGB values multiplied 255 scaling. convert colour values using farver::convert_colour() CIELAB colour space get mean value parameter (\\(\\widehat{L^{*}}\\), \\(\\widehat{^{*}}\\), \\(\\widehat{b^{*}}\\)). Next, convert tuple mean colour CIELAB LCH(ab) colour space RGB plotting. important remember names R case-sensitive, name colour attributes CIELAB LCH(ab) colour spaces using UPPERCASE letters name RGB attributes using lowercase letters. help us avoid confusing b* attribute L**b* colour space blue channel RGB colour space, named B b respectively. farver::convert_colour() gives attributes colour spaces lowercase names default, b* blue represented b, without taking account overwrite b* attribute blue channel final conversion RGB lose key data.loop also couple tasks. calculate time frame can analyse colour evolves time. also calculate saturation frame chroma (C) lightness (L), taking care ensure saturation set 0 achromatic colours (.e., C = 0%) avoiding NA values data frame. easily done using dplyr::if_else, check see C = 0 (NB: note requires double equal signs == represent logical version equal ) return 0 condition TRUE calculate saturation C L FALSE. also tidy go, using function rm() remove objects workspace longer required don‚Äôt large objects hanging around needed.Table 5.2 displays average colour outputted loop selected frames.\nTable 5.2: Average colour selected frames Fuelled. ‚òù\n\nR functions vectorised operate elements\nvector without requiring explicit loop apply\nfunction elements vector one time.\n\nexample, convert colour values RGB colours \n\\([0, 1]\\) scale \\([0, 255]\\) multiply red colour\nchannel RED 255 every individual value \nvector multiplied 255.\n\ncan also add together two vectors equal length\nelementwise, first element vector\nX added first element vector Y,\nsecond element X added second element \nY, .\n\nfundamental operations R vectorised, including\nsubtraction, division, logical tests, matrix operations.\n\nVectorisation makes code easy read efficient run.\nvisualise distribution average frame colours L**b* colour space can plot * b* attributes (Figure 5.4.). limitation Figure 5.4.data points may overlap, especially * = b* = 0, making difficult see distribution colours film. overcome , can plot density data points using ggplot2::geom_density_2d_filled() (Figure 5.4.B).\nFigure 5.4: () distribution (B) density average colour 1085 frames sampled Fuelled L**b* colour space.\nFigure 5.5 plots data LCH(ab) colour space, using coord_polar() plot hue values angle colour wheel.\nFigure 5.5: () Polar representation hue chroma (B) chroma lightness average frame colour 1085 frames sampled Fuelled.\nPlotting average colours sampled frames, see Fuelled uses colour palette characterised two features. First, see dominant axis dark grayish cyans moderate reds, passing achromatic colours origin dark desaturated reds. cyans desaturated reds average colours frames used gas station forest, respectively. moderate reds higher chroma lightness used exclusively flashback sequences Cathy remembers husband. Second, two groups pixels perpendicular main axis comprising yellow orange hues, associated explosion gas station, dark blues, used forest night Cathy runs fuel. Clearly, students made film selected colours carefully purposefully making film, using complementary colours across two axes tetradic palette.also clear polar version colours missing palette, pixels lime green (H = 120¬∞) blue (H = 240¬∞) regions. chroma frames limited strong relationship chroma lightness.","code":"\n# get list of jpg files in the Frames directory\nimages <- list.files(here(\"Data\", \"Frames\"), pattern = \"*.jpg$\")\n\nhead(images)\n## [1] \"image_000001.jpg\" \"image_000002.jpg\" \"image_000003.jpg\" \"image_000004.jpg\"\n## [5] \"image_000005.jpg\" \"image_000006.jpg\"\n# Load the tidyverse and farver packages\npacman::p_load(tidyverse, farver)\n\n# Create an empty data frame to store the result of each frame\ndf_average_colours <- data.frame()\n\n# Set frame to zero; frame is used as an index and to calculate the time of a frame\nframe <- 0\n\nfor (i in images){\n  \n  # Increment frame each time the loop starts\n  frame <- frame + 1\n  \n  # Grab the name of each image to use as an identifier and drop the file extension\n  frame_id <- gsub(pattern = \".jpg$\", \"\", basename(as.character(i)))\n  \n  # Calculate the time of the frame\n  time <- frame * 0.5\n  \n  # Load the i-th frame using imager::load.image()\n  im <- load.image(here(\"Data\", \"Frames\", i))\n  \n  # Separate R, G, B channels and gather as a data frame\n  rgb <- cbind(R(im), G(im), B(im)) %>% as.data.frame %>% \n    rename(r = 1, g = 2, b = 3) %>%\n    mutate(r = r * 255, g = g * 255, b = b * 255) # multiply by 255 to scale correctly\n  \n  # Remove the image from the workspace once we've collected the colour data\n  rm(im)\n  \n  # Convert to RGB to CIELAB colour space using farver::convert_colour()\n  lab <- convert_colour(rgb, from = \"rgb\", to = \"lab\")\n  # Remove the rgb data frame as we no longer need it\n  rm(rgb)\n  \n  # Calculate the mean values of each attribute in the L*a*b* colour space\n  pix_lab <- data.frame(L = mean(lab$l), \n                        A = mean(lab$a), \n                        B = mean(lab$b))\n  # Remove the lab data frame as we no longer need it\n  rm(lab)\n  \n  # Convert to LCH(ab) colour space using farver::convert_colour()\n  pix_lch <- convert_colour(pix_lab, from = \"lab\", to = \"lch\") %>%\n    rename(L = l, C = c, H = h) # using uppercase letters for consistency\n  \n  # Calculate the saturation from the chroma and lightness:\n  # if a colour is achromatic (i.e. C = 0) then the saturation is also 0 - \n  # this avoids NA values in the data frame\n  pix_lch <- pix_lch %>% mutate(S = if_else(C == 0, 0, 100*(C/sqrt(C^2 + L^2))))\n  \n  # Drop the L parameter from pxi_lch because it is the same as the L parameter in pix_lab\n  pix_lch <- pix_lch %>% select(C, H, S)\n  \n  # Convert the mean CIELAB values to RGB using farver::convert_colour() for plotting -\n  # the RGB values will be identified by lowercase letters\n  pix_rgb <- convert_colour(pix_lab, from = \"lab\", to = \"rgb\")\n  \n  # Collect all the results for a frame\n  pix <- cbind.data.frame(frame, frame_id, time, pix_lab, pix_lch, pix_rgb)\n  \n  # Add the results to the data frame for export\n  df_average_colours <- rbind.data.frame(df_average_colours, pix)\n\n}\npacman::p_load(ggpubr, viridis)\n\n# Create the plot\nlab_plot <- ggplot(data = df_average_colours) + \n  geom_point(aes(x = A, y = B, colour = rgb(r, g, b, maxColorValue = 255))) + \n  scale_x_continuous(name = \"Green - Red (a*)\") +\n  scale_y_continuous(name = \"Blue - Yellow (b*)\") +\n  scale_colour_identity() + \n  theme_minimal()\n\nlab_plot_2d <- ggplot(data = df_average_colours) +\n  geom_density_2d_filled(aes(x = A, y = B), bins = 20) +\n  scale_x_continuous(name = \"Green - Red (a*)\") +\n  scale_y_continuous(name = \"Blue - Yellow (b*)\") +\n  guides(fill = \"none\") +\n  theme_minimal()\n\n# Combine the plots into a single figure\nlab_figure <- ggarrange(lab_plot, lab_plot_2d, nrow = 2, align = \"v\", labels = \"AUTO\")\n\n# Call the figure\nlab_figure\n# Plot the average colour of a frame using hue and chroma\nlch_plot <- ggplot(data = df_average_colours) + \n  geom_point(aes(x = H, y = C, colour = rgb(r, g, b, maxColorValue = 255))) +  \n  scale_colour_identity() +  \n  coord_polar(theta = \"x\") +  \n  scale_x_continuous(limits = c(0, 360), breaks = seq(0, 330, 30)) +  \n  scale_y_continuous(name = \"Chroma (%)\", limits = c(0,100)) +  \n  theme_minimal() +  \n  theme(axis.title.x = element_blank())\n\n# Plot chroma against lightness\nchroma_light_plot <- ggplot(data = df_average_colours) +  \n  geom_point(aes(x = C, y = L, colour = rgb(r, g, b, maxColorValue = 255))) +  \n  scale_colour_identity() +  \n  labs(x = \"Chroma (%)\",  \n       y = \"Lightness (%)\") +  \n  theme_minimal()\n\n# Combine the plots into a single figure\nlch_figure <- ggarrange(lch_plot, chroma_light_plot, \n                        ncol = 2, align = \"h\", labels = \"AUTO\")\n\n# Call the figure\nlch_figure"},{"path":"colour.html","id":"colourfulness","chapter":"5 Analysing film colour","heading":"5.3.2 Colourfulness","text":"Colourfulness attribute visual sensation colour refers perceived intensity colour (Fairchild, 2013: 90-96). colourfulness image dependent range vividness colours image, increasing increasing illumination unless image bright, describes degree observer perceives colour less chromatic (Zerman et al., 2019). Although colourfulness measured part perceiver‚Äôs experience, can quantified using low-level attributes colour spaces correlate categories psychometric scales.Hasler & Suesstrunk (2003) define simple colourfulness metric, \\(M_{3}\\), linear combination mean standard deviation distribution pixels opponent sRGB colour space defined \\(rg=R-G\\) \\(yb=0.5(R+G)-B\\). \\(M_{3}\\) metric calculated \\[\nM_{3}=\\sigma_{rgyb}+0.3\\mu_{rgyb}\\ ,\n\\]\\[\n\\sigma_{rgyb}=\\sqrt{\\sigma_{rg}^2+\\sigma_{yb}^2}\n\\]\\[\n\\mu_{rgyb}=\\sqrt{\\mu_{rg}^2+\\mu_{yb}^2}\\ .\n\\]interpretation metric based category scaling experiment subjects rated colourfulness attributes images, following values M3 metric assigned colourfulness attributes:colourful = 0slightly colourful = 15moderately colourful = 33averagely colourful = 45quite colourful = 59highly colourful = 82extremely colourful = 109The \\(M_{3}\\) metric showed 95% correlation subjective experience colourfulness subjects participating experiment.Rather create new function calculate colourfulness frame can import function already created directly GitHub using devtools package. Note order source function GitHub necessary access raw version file.can find code HSM3() function HSM3 repository. can write another loop similar one used get average colour frame.Figure 5.6 plots colourfulness sampled frames.\nFigure 5.6: temporal evolution Colourfulness Fuelled.\nFigure 5.6 can see still lot redundancy data can reduce number frames eliminating sampled frames similar previous frames. One strategy achieve calculate difference colourfulness consecutive frames keep frames difference threshold, keep frames separated large differences \\(M_{3}\\) metric. better strategy calculate difference frames reference frame threshold reached use frame new reference frame subsequent frames compared. retain data takes account changes colourfulness may arise gradual transitions, scale shot changing due camera movements zooms , also retaining sharp differences colourfulness occur cuts scenes locations.achieve goal can create function called frame_picker() store colourfulness first frame sample compare frame subsequent frame absolute difference exceeding pre-specified threshold (t) reached add frame reduced data set. process starts comparing frames new reference frame threshold reached data third frame added reduced data set third frame becomes new reference frame, , frames sample processed.\nControl statements expressions control flow block\ncode based conditions provided statements, include\nlogical tests ((), ifelse()) loops\n((), ()).\n\nR, conditions control statements placed within parentheses\n() code executed condition satisfied \nplaced within braces {}.\n\nstatement execute block code \ncondition satisfied. condition satisfied, block \ncode run.\n\nifelse statement execute one block code \ncondition satisfied execute different block code \ncondition satisfied. Multiple conditions can applied adding\nelseif statements flow.\n\ndplyr package function if_else(), \nstricter conditions R‚Äôs base ifelse() function \nchecks TRUE FALSE conditions \ntype. if_else() function can faster \nexecute.\n\nloop iterates list elements, applying \nblock code element. loops can nested\nwithin one another loop multiple lists elements.\n\nloop iterates list elements applying \nblock code element condition longer met \nloop stops.\n\nconstructing loops important determine parts \ncode need inside loop (.e., braces {})\nparts need outside loop. example, need \nstore outcome pass loop necessary \ncreate data structure outside loop created\nanew time loop runs, thereby losing data produced \nprevious passes.\n\nloop calculate colourfulness \nframes sampled Fuelled, created data frame\ndf_M3 empty data frame outside loop wrote \nresults pass loop adding new row \ndf_M3 using rbind.data.frame(). results\nevery frame persist loop exits. objects\nframe_id, im, M3, \nres created within loop overwritten \nevery pass loop. final values objects relate \nlast image processed loop exits.\nApplying frame_picker() function sample 1085 frames Fuelled threshold t = 4 gives reduced sample 191 frames (Table 5.3).\nTable 5.3: colourfulness picked frames Fuelled using threshold t = 4. ‚òù\nPlotting reduced sample time (Figure 5.7) see even discarding 894 frames (82.4%) sample trends colourfulness course film remain largely intact. can get good idea colourfulness evolves course Fuelled 191 frames representing just 1.5% total number frames film.\nFigure 5.7: temporal evolution colourfulness Fuelled using reduced data set.\nClearly outcome process depends value t. can increase number frames picked lowering threshold reduce sample drastically increasing value t. value t used depend data wish achieve analysis, experimentation necessary find good value. Research just noticeable differences colourfulness may inform selection threshold (Henry, 2002).colourfulness Fuelled increases course film. colourfulness peaks explosion gas station, fills screen bright yellows, \\(M_{3}\\) metric typically lies 15 (slightly colourful) 82 (highly colourful). explosion, fire burns giving yellow orange background shots, range colourfulness decreased 33 (moderately colourful) 82. Cathy calls 911, camera pulls back filling frame dark blues forest night credits roll. point colourfulness frame slowly drops film ends.","code":"\n# Source the function from GitHub - requires the devtools package to be installed\ndevtools::source_url(\"https://github.com/DrNickRedfern/HSM3/blob/master/HSM3.R?raw=TRUE\")\n# Create an empty data frame to store the results\ndf_M3 <- data.frame()\n\n# Loop\nfor (i in images){\n  \n  frame_id <- gsub(pattern = \".jpg$\", \"\", basename(as.character(i)))\n  im <- load.image(here(\"Data\", \"Frames\", i))\n  \n  # Apply the HSM3 to get the colourfulness of an image\n  M3 <- HSM3(im)\n  rm(im)\n  \n  res <- cbind.data.frame(frame_id, M3)\n  df_M3 <- rbind.data.frame(df_M3, res)\n  \n}\n\ndf_M3 <- df_M3 %>% mutate(frame = 1:length(df_M3$M3),\n                    time = frame * 0.5) %>%\n  relocate(frame, .before = \"frame_id\") %>%\n  relocate(time, .after = \"frame_id\")\n\n# Add the RGB values of the average frame colour from df_average_colours for plotting\ndf_M3 <- cbind(df_M3, df_average_colours[10:12])\nggplot(data = df_M3) + \n  geom_path(aes(x = time, y = M3), colour = \"grey40\") +\n  geom_point(aes(x = time, y = M3), \n             colour = rgb(df_M3$r, df_M3$g, df_M3$b, maxColorValue = 255)) +\n  geom_smooth(aes(x = time, y = M3), \n              method = \"loess\", span = 0.2, se = FALSE, colour = \"black\") +\n  scale_x_continuous(name = \"Time (s)\", limits = c(0, 542.6735), \n                     breaks = seq(0, 500, 50), expand = c(0.01, 0.01)) +\n  scale_y_continuous(name = \"Colourfulness\", limits = c(0, 130), \n                     breaks = c(0, 15, 33, 45, 59, 82, 109)) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.grid.minor.y = element_blank())\nframe_picker <- function(x, t){\n  \n  n <- length(x)  # Number of frames in the sample\n  i <- 1; j <- i + 1  # Set initial values for frames to be compared\n  \n  # Create a data frame - \n  # the first value is the colourfulness of the first frame\n  df <- cbind(i, x[i])\n  \n  # Loop over \n  while(j <= n){\n    # Check if the absolute difference between frames is greater than the threshold\n    if (abs(x[j] - x[i]) > t){\n      df_a <- cbind(j, x[j])\n      i <- j  # Update the reference frame\n    } else {\n      i <- i\n      df_a <- NULL} \n    df <- rbind(df, df_a)\n    j <- j + 1  # Increment j\n  }\n  \n  return(df)\n  \n}\n# Pick frames with  threshold of 4\nres <- frame_picker(df_M3$M3, 4)\n\n# Filter in only those frames from the data frame df_M3 that were picked\ndf_M3_reduced <- df_M3 %>% \n  filter(frame %in% res)\nggplot(data = df_M3_reduced) + \n  geom_path(aes(x = time, y = M3), colour = \"grey40\") +\n  geom_point(aes(x = time, y = M3), colour = rgb(df_M3_reduced$r, df_M3_reduced$g, df_M3_reduced$b, maxColorValue = 255)) +\n  geom_smooth(aes(x = time, y = M3), \n              method = \"loess\", span = 0.2, se = FALSE, colour = \"black\") +\n  scale_x_continuous(name = \"Time (s)\", limits = c(0, 542.6735), \n                     breaks = seq(0, 500, 50), expand = c(0.01, 0.01)) +\n  scale_y_continuous(name = \"Colourfulness\", limits = c(0, 130), \n                     breaks = c(0, 15, 33, 45, 59, 82, 109)) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.grid.minor.y = element_blank())"},{"path":"colour.html","id":"palette","chapter":"5 Analysing film colour","heading":"5.3.3 Constructing a palette","text":"Cluster analysis colour data unsupervised machine learning problem partitions set n pixels k groups, group represented exemplar centroid. collection k centroids comprises palette film.can construct palette using Partitioning Medoids (PAM), k-medoids clustering algorithm, generating k clusters represented medoids. cluster exemplar, medoid restricted member data set drawn. commonly used clustering methods, k-means clustering, enforce restriction centroids exemplar colour cluster may , fact, exist data set extracted film. However, limitation PAM much slower method performing cluster analysis k-means cluster analysis. can come using Clustering Large Applications (CLARA) algorithm (Kaufman & Rousseeuw, 2009), draws multiple random samples size m data set, applies PAM sample find optimal set medoids based distance function (e.g.¬†Euclidean Manhattan distance), keeps clustering solution smallest dissimilarity whole data set.implement PAM using pamk() function fpc package, allow us search across range possible values k find optimal result.Clustering colour data best performed using perceptually uniform colour space use CIELAB colour space. However, attributes CIELAB colour space different ranges (L* = \\([0-100\\%]\\), * = b* = \\([-128, 127]\\)), need scale data using z-score transformation, subtracting mean (\\(\\mu\\)) data point dividing standard deviation (\\(\\sigma\\)):\\[\nz = \\frac{x - \\mu}{\\sigma} ,\n\\]attributes mean 0 standard deviation 1. easily done using R‚Äôs scale() function.apply scaling, need get mean standard deviation attribute need later re-scale results cluster analysis.Next, pass scaled data pamk() function set arguments wish use cluster analysis.krange argument sets range values k across PAM search optimal result. Note changing range may change outcome cluster analysis (.e., k optimal values k considered algorithm). Plotting density data points CIELAB colour space deciding range k good way estimate reasonable range possible values. Based density plot Figure 5.4.B, appears relatively small number clusters: least five clusters certainly fifteen, set range possible values k. optimal value k selected based criterion, average silhouette width (asw).‚Äôve already scaled data can set scaling = FALSE. need set usepam = FALSE pamk() employ CLARA algorithm, draw 100 samples 200 data points . CIELAB colour space perceptually uniform distance colours proportional Euclidean distance , use Euclidean distance distance metric.‚Äôve run algorithm, can access number clusters (nc) get optimal value k returned.results cluster analysis show us nine clusters optimal can add column original df_average_colours data frame assigned cluster data point.Figure 5.8 plots individual clusters shows , overall, good discrimination average frame colours applying CLARA algorithm partitioning data.\nFigure 5.8: Clusters average frame colours Fuelled produced using k-mediods clustering\nconstruct palette Fuelled, need get medoids re-scale reversing z-score transformation now represent colours CIELAB colour space convert RGB values plotting. also need access size cluster can calculate percentage 1085 sampled frames cluster.\nTable 5.4: number frames Fuelled associated medoid partioning medoids (k = 9). ‚òù\nUsing treemapify package extend functionality ggplot2, can visualise palette created treemap (Figure 5.9), size tile proportional percentage pixels cluster.\nFigure 5.9: treemap palette Fuelled\nCluster 2 comprises dark moderate pink frames opening scene film, Cathy remembers time husband. Frames cluster 1 dark blues shots Cathy‚Äôs car runs fuel. Clusters 3 8 dark saturated reds forest, clusters 6 7 dark grayish cyans gas station. Cluster 9 includes frames showing explosion gas station. Cluster 5 moderate red frames sampled shots Cathy remembers life husband. Cluster 4 includes blacks frames used separate scenes, Cathy opens closes eyes, closing credits shots forest night.","code":"\n# Get the mean and standard deviation of each attribute for re-scaling\nL_mu <- mean(df_average_colours$L); L_sd <- sd(df_average_colours$L)\nA_mu <- mean(df_average_colours$A); A_sd <- sd(df_average_colours$A)\nB_mu <- mean(df_average_colours$B); B_sd <- sd(df_average_colours$B)\n\n# Select the L, A, and B columns and scale the data\ndf_lab_scaled <- df_average_colours %>% select(L, A, B) %>% scale\npacman::p_load(fpc)\n\nclara_res <- pamk(df_lab_scaled, krange = 5:15, criterion = \"asw\", scaling = FALSE,\n                  usepam = FALSE, samples = 100, sampsize = 200, metric = \"euclidean\")\n\n# Get the optimal number of clusters\nclara_res$nc\n## [1] 9\n# Get the assigned cluster of each data point and add to the data frame\ndf_average_colours <- df_average_colours %>% mutate(cluster = paste(\"Cluster \",\n                                            clara_res$pamobject$clustering))\n# Plot the individual clusters in the CIELAB colour space -\n# note that it is necessary to set the colour command *outside* the \n# aesthetics of the plot and and explicitly call them as columns \n# from the data frame using the $ operator\nggplot(data = df_average_colours) +\n  geom_point(aes(x = A, y = B, group = cluster), \n             colour = rgb(df_average_colours$r, \n                          df_average_colours$g, \n                          df_average_colours$b, \n                          maxColorValue = 255)) +\n  facet_wrap(~cluster) +\n  scale_x_continuous(name = \"Green - Red (a*)\") +\n  scale_y_continuous(name = \"Blue - Yellow (b*)\") +\n  guides(colour = \"none\") +\n  theme_minimal() +\n  theme(strip.text = element_text(hjust = 0))\n# Get the medoids of the optimal solution returned by CLARA\nmedoids <- as.data.frame(clara_res$pamobject$medoids)\n\n# Create a data frame including the number of data points in each cluster, \n# the percentage of data points in each cluster (percent), \n# and the re-scaled medoids in the CIELAB colour space\ndf_medoids_rescaled <- medoids %>% \n  mutate(cluster = 1:9,\n         # The number of data points in each cluster\n         size = clara_res$pamobject$clusinfo[,1], \n         # The percentage of data points in each cluster\n         percent = 100 * round(size/sum(size), 3), \n         L = (L * L_sd) + L_mu,  # Re-scale the attributes using the mean and\n         A = (A * A_sd) + A_mu,  # standard deviation values we collected earlier\n         B = (B * B_sd) + B_mu)\n\n# Convert L*a*b* values to RGB for plotting and combine with the medoids_rescaled data frame\nrgbPalette <- convert_colour(df_medoids_rescaled[1:3],  from = \"lab\", to = \"rgb\")\ndf_medoids_rescaled <- cbind.data.frame(df_medoids_rescaled, rgbPalette)\n\ndf_medoids_rescaled <- df_medoids_rescaled %>%\n  relocate(cluster, .before = L) %>%\n  relocate(size, .before = L) %>% \n  relocate(percent, .before = L) %>%\n  arrange(desc(percent))\npacman::p_load(treemapify)\n\n# Draw a treemap of the palette using treempify::geom_treemap()\nggplot(data = df_medoids_rescaled) +\n  geom_treemap(aes(area = percent), \n               fill = rgb(df_medoids_rescaled$r, df_medoids_rescaled$g, \n                          df_medoids_rescaled$b, maxColorValue = 255)) +\n  theme_minimal()"},{"path":"colour.html","id":"barcode","chapter":"5 Analysing film colour","heading":"5.3.4 Barcode","text":"noted , movie barcode commonly used method sample--reduce approach implemented provide fingerprint chromatic structure film (Burghardt et al., 2018). Movie barcodes used compare different versions film assess impact restoration practices colour film (Plutino et al., 2021; Stutz, 2021). arrange colours film time, useful segmenting films structural analysis (Bhardwaj, 2020; Hohman et al., 2017; Isola et al., 2015).research method, movie barcodes limitations. useful large scale see overall chromatic structure film, support analysis micro-level loss granuality due averaging process (Halter et al., 2019). Barcodes useful descriptive tools lack explanatory power (Kuhn et al., 2012).following code uses average frame colours calculated earlier create barcode Fuelled (Figure 5.10) using geom_rect(). sampled frames film rate 2 fps need subtract reciprocal sampling rate time stamps frame ‚Äì .e., \\(\\frac{1}{2} = 0.5\\) seconds ‚Äì set minimum bound column plot: xmin = time - 0.5.\nFigure 5.10: average colour frames sampled Fuelled represented barcode.\ncan overlay barcode trendlines different colour attributes order see evolve running time film, barcode providing reference attributes can interpreted.\nFigure 5.11: Colour attributes Fuelled time plotted movie barcode.\nFigure 5.11 plots loess trendlines lightness, chroma, saturation Fuelled barcode average frame colours. Based trendlines can segment film four sections. first section runs start film (0.0s) point Cathy realises money buy gas (80s). lightness drops rises move aftermath murder forest flashback, chroma remains constant saturation increases chroma comes play larger role colour experience viewer. 80 seconds 150s, Cathy steals gas, chroma saturation drop lightness increases. third section, conflict gas station owner 150s moment Cathy wakes explosion gas station (300s), lightness remains constant chroma saturation increases. 300 seconds, lightness chroma trendlines follow downward trend Cathy sees killer . Consequently, saturation begin fall away point, peaking 340s cut interior car Cathy comes recognise become remains constant 450 seconds, Cathy calls 911 surrender police.Figure 5.11 challenges sense colour single object shows need consider attributes film‚Äôs colour individually. Analysing different components Fuelled see change points trendlines lightness chroma tend occur narrative events. also true saturation lightness chroma trends saturation tracks chroma. lightness chroma trend final section film, saturation peaks drops away changes emotion Cathy reassesses life become.","code":"\n# Create the barcode plot\n# Note that it is necessary to set the fill command *outside* the \n# aesthetics of the plot and to explicitly call them as columns \n# from the data frame using the $ operator\nfuelled_barcode <- ggplot(data = df_average_colours) +\n  geom_rect(aes(xmin = time - 0.5, xmax = time, ymin = 0, ymax = 100), \n                fill = rgb(df_average_colours$r, \n                           df_average_colours$g, \n                           df_average_colours$b, \n                           maxColorValue = 255)) +\n  guides(fill = \"none\") # don't display a legend\n\n# Set the theme to theme_void to display only the barcode\nfuelled_barcode + theme_void()\nfuelled_barcode_lightness <- fuelled_barcode +  \n  geom_line(aes(x = time, y = L), stat = \"smooth\", method = \"loess\", \n            span = 0.30, se = FALSE, color = \"white\", size = 1) +\n  scale_x_continuous(name = NULL, limits = c(0, max(df_average_colours$time)), \n                     breaks = seq(0, 550, 50), expand = c(0, 0)) +\n  scale_y_continuous(name = NULL, limits = c(0, 100), expand = c(0.01, 0.01),\n                     breaks = seq(0, 100, 25), labels = function(x) paste0(x, \"%\")) +\n  ggtitle(\"Lightness\") +\n  guides(fill = \"none\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(margin = margin(t = 0.1)))\n\nfuelled_barcode_chroma <- fuelled_barcode +  \n  geom_line(aes(x = time, y = C), stat = \"smooth\", method = \"loess\", \n            span = 0.30, se = FALSE, color = \"white\", size = 1) +\n  scale_x_continuous(name = NULL, limits = c(0, max(df_average_colours$time)), \n                     breaks = seq(0, 550, 50), expand = c(0, 0)) +\n  scale_y_continuous(name = NULL, limits = c(0, 100), expand = c(0.01, 0.01),\n                     breaks = seq(0, 100, 25), labels = function(x) paste0(x, \"%\")) +\n  ggtitle(\"Chroma\") +\n  guides(fill = \"none\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(margin = margin(t = 0.1)))\n\nfuelled_barcode_saturation <- fuelled_barcode +  \n  geom_line(aes(x = time, y = S), stat = \"smooth\", method = \"loess\", \n            span = 0.30, se = FALSE, color = \"white\", size = 1) +\n  scale_x_continuous(name = \"Time (s)\", limits = c(0, max(df_average_colours$time)), \n                     breaks = seq(0, 550, 50), expand = c(0, 0)) +\n  scale_y_continuous(name = NULL, limits = c(0, 100), expand = c(0.01, 0.01),\n                     breaks = seq(0, 100, 25), labels = function(x) paste0(x, \"%\")) +\n  ggtitle(\"Saturation\") +\n  guides(fill = \"none\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(margin = margin(t = 0.1)))\n\n# Combine into a single figure\nfuelled_barcodes <- ggarrange(fuelled_barcode_lightness, fuelled_barcode_chroma,\n                              fuelled_barcode_saturation,\n                              nrow = 3, align = \"v\")\n\n# Call the figure\nfuelled_barcodes"},{"path":"colour.html","id":"summary-3","chapter":"5 Analysing film colour","heading":"5.4 Summary","text":"Colour rich source information many different aspects film‚Äôs form style, proceed colour analysis necessary reduce data points working manageable number. chapter applied simple approach sample--reduce methodology used digital humanists explore colour cinema, sampling frames even points film calculating average colour frames. allowed us construct data set representative Fuelled explore different attributes film‚Äôs use colour. seen applying frame_picker() function cluster analysis data set can get good understanding colour film set frames representing small proportion total number frames available can describe colours film using small set numbers.","code":""},{"path":"editingI.html","id":"editingI","chapter":"6 Analysing film editing I","heading":"6 Analysing film editing I","text":"common application quantitative methods analysis film style determine duration shots two () motion pictures two () groups films differ, , , much differ. Comparisons shot lengths motion pictures used identify similarities differences style individual filmmakers, historical periods, genres, national cinemas, impact new technologies motion picture production.chapter cover methods describing comparing shot length data motion pictures. important idea introduced chapter distribution concept underpins everything apply computational methods motion picture shot length data. analyse distribution shot lengths Alfred Hitchcock‚Äôs late-silent early-sound films see tell us style films changed introduction synchronised sound. apply method based quantiles shot length distribution describe data sets numerically learn range methods visualise shot length data. Finally, ask three questions data Hitchcock‚Äôs films see thinking distributionally shot lengths can help us improve quality analyses.","code":""},{"path":"editingI.html","id":"distributional-thinking-about-film-editing","chapter":"6 Analysing film editing I","heading":"6.1 Distributional thinking about film editing","text":"data set comprises collection pieces related information produced measuring properties group objects. data set characterised variation, , simple terms, quality measured property object vary (Makar & Confrey, 2005). Chris Wild argues need statistics flows variation:statistical response variation investigate, disentangle, model patterns variation order learn . Virtually ways statisticians involve looking data lens distribution (Wild, 2006: 21).distribution representation variation data set allows us organise examine data efficiently gain overall understanding data varies. Aisling Leavy argues understanding distribution requires ‚Äòawareness propensity variable vary comprehension variability contributes notion distribution aggregate rather collection individual data points‚Äô (Leavy, 2006: 90). Distributional thinking quantitative reasoning variation, distribution, relationship (Bakker & Gravemeijer, 2004; Prodromou, 2007).talk distribution data set, need describe range features, including overall shape distribution deviations overall shape. describe distribution, need ask following questions various features:centre: mass data located? centre data located? typical value data set?spread: much variability data set?symmetry (skewness): distribution symmetrical? bulk data left distribution long right tail (positively skewed)? bulk data right distribution long left tail (negatively skewed)?modality: many peaks distribution ?peakedness: shape peak(s) flat broad tall pointed?tailedness (kurtosis): much data tails distribution relative centre?outliers: deviations overall pattern data? observations noticeably distinct bulk data?attending features attempt account variation data set deals complexity may arise range different situations.talking editing motion pictures data set comprises duration shot film ‚Äì variable interest ‚Äì shot length distribution way think talk data compare different data sets different films. However, look common applications statistics questions film editing, see concepts ‚Äòvariation‚Äô ‚Äòdistribution‚Äô seldom present. Distributional thinking sort Wild Leavy describe fundamental statistical reasoning data rarely part statistical analysis film style.present, descriptions literature film style address features shot length distributions, relying comparisons average shot lengths (ASLs) alone. many cases, possible get information style film group films concept distribution appear well-understood Film Studies. researchers applying statistical methods analysis shot lengths fact collect data never produce shot length distribution, relying instead single number produced dividing running time film number shots film. Consequently, unable provide information shot length data film beyond ASL. Even researchers collect full data set shot durations film report ASL ignore features shot length distributions potentially interesting, variability data shape shot length distribution. small proportion literature addresses features shot length distributions beyond ASL (see, example, Baxter et al. (2017); Fujita (1989); Kohara & Niimi (2013); Redfern (2020a)). result lack distributional thinking characterises applications quantitative methods analysis film style.","code":""},{"path":"editingI.html","id":"set-up-the-project-1","chapter":"6 Analysing film editing I","heading":"6.2 Set up the project","text":"","code":""},{"path":"editingI.html","id":"create-the-project-folder-1","chapter":"6 Analysing film editing I","heading":"6.2.1 Create the project folder","text":"first tasks create new project folder editing analysis associated .rproj file open run script projects_folders.R created Chapter 3 create folders required project.","code":""},{"path":"editingI.html","id":"packages-3","chapter":"6 Analysing film editing I","heading":"6.2.2 Packages","text":"chapter use packages listed Table 6.1.\nTable 6.1: Packages analysing motion picture shot length data.\nload required packages need . now, need able load wrangle data load tidyverse packages.","code":"\npacman::p_load(tidyverse, here)"},{"path":"editingI.html","id":"data-1","chapter":"6 Analysing film editing I","heading":"6.2.3 Data","text":"data set use chapter Early Hitchcock data set (Redfern, 2021b), comprises shot length data nine films directed Alfred Hitchcock 1927 1932, including four silent films (Ring (1927), Farmer‚Äôs Wife (1928), Champagne (1928), Manxman (1929)), five sound films (Blackmail (1929), Murder! (1930), Skin Game (1931), Rich Strange (1931), Number Seventeen (1932).data available .csv file Zenodo can accessed directly reading file URL download button repository. avoid repeatedly download data save Data directory project folder, can read future needed.load data just read .csv file Data folder.df_hitchcock data frame Table 6.2 wide format, one column data film.\nTable 6.2: Early Hitchcock shot length data wide format.\nneed re-arrange data long format one column variable ‚Äì film SL (Table 6.3). Reshaping data frame long format makes easier apply data wrangling operations data frame. saw Chapter 3, data frame also contains number NA values film different number shots need removed.\nTable 6.3: Early Hitchcock shot length data long format.\nset film titles factor display order specify (.e., order release) rather alphabetical order ggplot2::ggplot() defaults. make easier see shot length distributions Hitchcock‚Äôs film changed time.Finally, add column called type df_hitchcock data frame identifies whether film data set silent film sound film. %% operator used identify element member vector data frame. Combined ifelse() function use check titles four silent films occur df_hitchcock data frame, write value Silent type column occur writes value Sound type column occur. gives us data frame format illustrated Table 6.4 work .\nTable 6.4: Early Hitchcock shot length type film added.\n","code":"\n# Access the data directly from the zenodo repository\ndf_hitchcock <- read_csv(\"https://zenodo.org/record/4871227/files/Hitchcock_SL.csv?download=1\")\n\n# Save the csv file in the Data folder of our project\nwrite_csv(df_hitchcock, here(\"Data\", \"early_Hitchcock.csv\"))\n# To load the saved csv file we can then run\ndf_hitchcock <- read_csv(here(\"Data\", \"early_Hitchcock.csv\"))\ndf_hitchcock <- df_hitchcock %>% \n  pivot_longer(cols = everything(), names_to = \"film\", values_to = \"SL\", \n               values_drop_na = TRUE) %>%  # Remove NA values\n  arrange(film)  # Arrange the results by film title\n# Set film titles as factor\ndf_hitchcock$film <- factor(df_hitchcock$film,\n                            levels = c(\"The Ring (1927)\",\n                                       \"The Farmers Wife (1928)\",\n                                       \"Champagne (1928)\",\n                                       \"The Manxman (1929)\",\n                                       \"Blackmail (1929)\",\n                                       \"Murder (1930)\",\n                                       \"The Skin Game (1931)\",\n                                       \"Rich and Strange (1931)\",\n                                       \"Number Seventeen (1932)\"))\n# Classify the films as either silent or sound\ndf_hitchcock <- df_hitchcock %>% \n  mutate(type = ifelse(film %in% c(\"The Ring (1927)\",\n                                   \"The Manxman (1929)\",\n                                   \"The Farmers Wife (1928)\",\n                                   \"Champagne (1928)\"),\n                       \"Silent\", \"Sound\"))"},{"path":"editingI.html","id":"numerical-summaries-of-shot-length-data","chapter":"6 Analysing film editing I","heading":"6.3 Numerical summaries of shot length data","text":"Descriptive statistics indices distribution (Leavy, 2006), summarizing data sets using small number features make large data sets manageable.conceptually simple method describing comparing shot length distributions use quantiles distributions. quantile (\\(Q_{p}\\)) cut point dividing data set arranged order smallest value largest specified proportion \\(p\\) data set lies point (D. G. Altman & Bland, 1994). \\(p\\)-th quantile data set found using quantile function\\[\nQ_{p} = \\{x:Pr(X \\leq x) = p\\}\n\\]Commonly used quantiles median (\\(Q_{0.5}\\)) data set, dividing range two equal parts, lower (\\(Q_{0.25}\\)) upper quartiles (\\(Q_{0.75}\\)) cut lower upper 25% data set, respectively; quantiles value \\(p\\) can used. one less quantile numbers groups created dividing data set subsets equal size: divide data set 20 equal parts need 19 quantiles.features shot length distribution listed can described terms quantiles. median shot length measure location, interquartile range, difference upper lower quartiles: \\(IQR = Q_{0.75} ‚Äì Q_{0.25}\\), describes spread distribution.symmetry distribution described skewness coefficient,\\[\nS = \\frac{Q_{0.25} + Q_{0.75}-2Q_{0.5}}{Q_{0.75}-Q_{0.25}}\n\\]takes values -1 1. Values \\(S\\) greater 0 indicate positive skewness. Positive skewness indicates right, upper tail distribution longer lower, left tail typical motion picture shot length distributions.kurtosis distribution can also described terms quantiles:\\[\nT = \\frac{(Q_{0.875}-Q_{0.625}) + (Q_{0.375}-Q_{0.125})}{Q_{0.75} - Q_{0.25}}\n\\]Kurtosis measures combined weight mass data tails distribution relative centre, higher values kurtosis indicating lot data points tails (Westfall, 2014). \\(T\\), two terms numerator measure combined weight shoulders distribution denominator \\(IQR\\), describes middle distribution. terms numerator large relatively data shoulders centre distribution resulting higher values \\(T\\) (Moors, 1988). Kurtosis , , measure presence outliers data.Taken together, four statistics ‚Äì median, interquartile range, quantile skewness, quantile kurtosis ‚Äì provide informative, intuitive, robust numerical summary distribution shot lengths motion picture. numerical descriptions require assumptions made possible models shot length distribution.can create simple function calculates quantile-based summary shot length data film. multiple methods calculating quantiles data set. use Harrell-Davis quantile estimator produced using hdquantile() function Hmisc package.can apply function data film data frame using dplyr::group_modify() purrr:map_dfc() rearrange result.\nTable 6.5: Descriptive statistics shot lengths Alfred Hitchcock‚Äôs late-silent early-sound films. ‚òù\nquantile-based descriptive statistics Table 6.5, see four silent films (Ring, Farmer‚Äôs Wife, Champagne, Manxman) shot length distributions relatively consistent, similar median shot lengths interquartile ranges. shift sound filmmaking 1929, Hitchcock‚Äôs early sound films (Blackmail, Murder!, Skin Game) show obvious change editing style increases median shots tended become longer duration varied use shot lengths seen change interquartile range. also change shape shot length distribution indicated increase quantile skewness quantile kurtosis. difference early sound films silent movies preceded increase spread shot lengths median dialogue shots required longer takes, accompanied smaller shift towards rapid editing spread shots median shot length became slightly lower. later sound films see shift editing style characterised shorter takes similar silent films shot length distributions skewed kurtose like first sound films. Rich Strange number rapidly edited montage sequences, Paris sequence leisure activities aboard cruise ship, series drawn-conversational sequences cut slowly; Number Seventeen largely comprised rapidly cut extended chase sequence two slower cut sequences house harbour, maintains distinction dialogue heavy scenes action.","code":"\n# The input to the function is a vector containing the shot lengths for a film\nSL_summary <- function(x){\n  \n  # Calculate the quantiles required to produce the summaries of the data\n  q <- Hmisc::hdquantile(x, probs = seq(0.125, 0.875, 0.125), na.rm = TRUE, \n                         names = FALSE, se = FALSE, weights = FALSE)\n  \n  # Create a list containing the output values\n  SL_sum <- list(`Shots` = length(x),\n             `Mean` = mean(x),\n             `Minimum` = min(x),\n             `Lower quantile` = q[2],\n             `Median` = q[4],\n             `Upper quantile` = q[6],\n             `Maximum` = max(x),\n             `IQR` = q[6] - q[2],\n             `Skewness` = (q[2] + q[6] - (2 * q[4]))/(q[6] - q[2]),\n             `Kurtosis` = ((q[7] - q[5]) + (q[3] - q[1]))/(q[6] - q[2]))\n}\ndf_hitchcock_summary <- df_hitchcock %>% \n  select(film, SL) %>%  # Select only the film and SL columns\n  group_by(film) %>%\n  # Apply the SL_summary function to the data for each film\n  group_modify(~{.x %>% \n      map_dfc(SL_summary)}) %>%\n  mutate(across(everything(), round, 2)) %>%\n  rename(Film = film) %>%  # Looks nicer when we display the table\n  tibble()"},{"path":"editingI.html","id":"visualising-shot-length-distributions","chapter":"6 Analysing film editing I","heading":"6.4 Visualising shot length distributions","text":"Numerical summaries useful descriptions shot length distributions limited amount information contain require skill interpreted meaningfully.Visualising distribution shot lengths film makes easier see structure data, especially working large /multiple data sets. often find information contained numerical summaries begins take concrete meaning visualising data. Effective visual summaries allow us discover patterns different scales analysis gaining overall picture data set macro-level focusing particular details micro-level. also allow us communicate results analysis simply effectively.use graphical methods common practice statistical analysis film style remains case many film scholars rely solely numerical summaries shot length data visualise data (actually data visualise). section describe four methods visualising shot length distributions using kernel density, quantile profile, empirical cumulative distribution function, adjusted boxplot.","code":""},{"path":"editingI.html","id":"kernel-densities","chapter":"6 Analysing film editing I","heading":"6.4.1 Kernel densities","text":"kernel density non-parametric estimate distribution values data set. calculate kernel density, fit kernel function data point sum values kernel point x-axis.kernel density estimator \\[\nf(x) = \\frac{1}{nh} \\sum_{=1}^{n}K\\bigg(\\frac{x-x_{}}{h}\\bigg)\n\\]\\(n\\) number shots film, \\(h\\) smoothing parameter called bandwidth, \\(K\\) kernel function. Figure 6.1 shows detail kernel density Skin Game. Beneath density, black diamond indicates length shot (\\(x_{}\\)). Gaussian kernels fitted data point shown green density point x-axis equal sum kernel functions point. closer data points one another, fitted kernels overlap greater sum kernels , therefore, greater density point.\nFigure 6.1: Kernel density Skin Game point x-axis calculated fitting kernel function (shown green) shot length (\\(x_{}\\), black diamonds) summing values functions \\(x\\). NB: kernels functions drawn scale.\nchoice kernel function relatively unimportant simply use Gaussian kernel default. choice bandwidth determines shape density estimate much important. illustrate , move slider Figure 6.2 adjust bandwidth kernel density estimate distribution shot lengths Skin Game. bandwidth narrow density estimate -fitted noisy lots localised spikes obscuring structure data. Equally, bandwidth wide density estimate -smoothed obscure structure data.\nFigure 6.2: kernel density Skin Game (1931) estimated range bandwidth values. ‚òùÔ∏è\nSetting bandwidth 1 3 Figure 6.2 reasonable compromise lets us see structure data without - -smoothed, enabling us identify key features distribution‚Äôs shape. distribution single tall, sharp peak clearly positively skewed, shots duration less 10 seconds long right, upper tail includes outliers long duration (longest shot film 281 seconds) give large range values.Using ggridges package, can efficiently plot kernel density film early Hitchcock data set single plot (Figure 6.3), reference lines median shot length lower- upper-quartiles added setting quantile_lines = TRUE. Due skewed nature shot length distributions easier see data plotting shot length logarithmic scale.\nFigure 6.3: Shot length distributions films directed Alfred Hitchcock, 1927-1932. quantile lines density plot show lower quartile, median, upper quartile distribution.\nsummary statistics give us overall impression difference shot length distributions Hitchcock‚Äôs late silent early sound films, yet know anything nature difference. Visualising distributions plotting kernel densities Figure 6.3 gives concrete meaning median shot length interquartile range indicated quantile lines plot, making clear descriptive statistics relate data illustrating shot lengths Blackmail, Murder!, Skin Game widely dispersed films shot lengths initially increase shift synchronised sound shortening duration. shape distribution shot lengths Blackmail Figure 6.3 resembles distributions silent film quartiles lower tail quartiles upper tail sound films followed release. pattern emerges due unique production circumstances Blackmail, released silent sound versions described Charles Barr asworks continuously inventive bricolage. Juxtaposing scene scene, one registers set permutations: points variously, () versions use ‚Äòsilent‚Äô visuals; (b) versions use ‚Äòsound‚Äô visuals; (c) silent sound visuals mixed within scene; (d) two films use entirely different visuals (Barr, 1983: 123).change distributional shape indicated quantile skewness kurtosis values evident, similarity shape silent films clear see shift mass data sound films upper tail distribution relative centre distribution.","code":"\n# Load the ggridges package\npacman::p_load(ggridges)\n\nggplot(data = df_hitchcock, \n       aes(x = SL, y = reorder(film, desc(film)), fill = type)) + \n  geom_density_ridges(scale = 1, quantile_lines = TRUE, alpha = 0.8) +\n  scale_x_continuous(name = \"\\nShot length (s)\", \n                     expand = c(0.01, 0), \n                     breaks = c(0.1, 1, 10, 100), \n                     minor_breaks = c(seq(0.02, 0.09, 0.01), \n                                      seq(0.2, 0.9, 0.1), \n                                      seq(2, 9, 1), \n                                      seq(20, 90, 10), \n                                      seq(200, 900, 100)),\n                     labels = c(\"0.1\", \"1.0\", \"10.0\", \"100.0\"), \n                     trans = \"log10\") +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_fill_manual(name = NULL, values = c(\"#440154\", \"#277F8E\")) +\n  theme(legend.position = \"bottom\", \n        axis.title.y = element_blank(),\n        axis.title = element_text(size = 10,face = \"bold\"),\n        panel.background = element_rect(fill = \"gray85\"))"},{"path":"editingI.html","id":"quantile-profile","chapter":"6 Analysing film editing I","heading":"6.4.2 Quantile profile","text":"described distribution shot lengths numerically using quantiles, can visualise distribution plotting quantile profile (Johnson et al., 2015) film data set. quantile profile show information shape distribution accessible way, make easier directly compare different parts shot length distributions set films.First, need calculate quantiles film \\(Q_{0.05}\\) \\(Q_{0.95}\\) intervals 0.05. results displayed Table 6.6.\nTable 6.6: Quantiles Alfred Hitchcock‚Äôs late-silent early-sound films. ‚òù\nFigure 6.4 plots quantile profiles film log-scale interactive visualisation using plotly package.\nFigure 6.4: quantile profiles Alfred Hitchcock‚Äôs late-silent early-sound films. ‚òùÔ∏è\nquantile profiles clearly show silent films much consistent distribution shot lengths sound films, exhibit much greater variation shot length different quantiles. can also see evidence hybrid editing style: exception one sound films (Blackmail), Hitchcock‚Äôs editing style introduction sound meant takes shorter duration slightly shorter silent films , time, longer takes Hitchcock films increased duration.\nplotly R package \ncreating interactive plots using plotly.js JavaScript\nlibrary.\n\nplotly syntax producing interactive plots R, \nggplotly() function provides easy integration \nggplot2 package. example quantile plot chapter\nillustrates, can create ggplot() object\nquantile_plot makes plot interactive \npassing ggplotly():\nggplotly(quantile_plot).\n\nUsers can interact plotly plots different ways.\n\nMoving mouse pointer plot generated plotly bring \nmodebar top right plot window. Users may select \nrange actions modebar, including downloading plot \n.png file; zooming, panning, selecting sections \nplot focus specific elements; choose whether display\ninformation nearest data point hover comparative data\nshown moving mouse data set.\n\nHovering mouse data point line show tool tip\ncontaining information data point. Information can \nadded text aesthetic ggplot() object \ncalled passing aesthetic ggplotly() using\ntooltip = ‚Äútext‚Äù. Tooltips can also specified outside\nggplot() object.\n\nUsers can remove data plot single-clicking element\nlegend. Double-clicking element legend isolates \ndata removing traces plot. Traces can added\nback plot single-clicking elements legend.\n\nPlots can animated show change time, allowing users \nplay animation start finish select frame using \nslider representing snapshot data.\n","code":"\ndf_hitchcock_quantiles <- df_hitchcock %>%\n  group_by(film) %>%\n  group_modify(~ { \n   round(Hmisc::hdquantile(.x$SL, probs = seq(0.05, 0.95, 0.05), na.rm = TRUE,\n                    names = FALSE, se = FALSE, weights = FALSE), 1) %>%\n      enframe(name = \"quantile\", value = \"SL\")\n  }) %>%\n  # Re-add the type column which was dropped when we created the new data frame\n  mutate(type = ifelse(film %in% c(\"The Ring (1927)\", \n                                   \"The Manxman (1929)\", \n                                   \"The Farmers Wife (1928)\", \n                                   \"Champagne (1928)\"),\n                       \"Silent\", \"Sound\"),\n         quantile = seq(0.05, 0.95, 0.05))\n# Create the plot using ggplot as usual\nquantile_plot <- ggplot(data = df_hitchcock_quantiles, \n       aes(x = quantile, y = SL, group = film)) +\n  geom_line(aes(colour = factor(type))) +\n  geom_point(aes(colour = factor(type),  \n                 text = paste(film, \n                              \"<br>Quantile: \", quantile,  # Add a line break <br>\n                              \"<br>SL: \", SL, \"s\"))) +\n  scale_x_continuous(name = \"Quantile\", breaks = seq(0.1, 0.9, 0.1)) +\n  scale_y_continuous(name = \"Shot length (s)\", \n                     trans = \"log10\", \n                     limits = c(0.5, 100), \n                     breaks = c(1, 10, 100), \n                     minor_breaks= c(seq(0.2, 0.9, 0.1), \n                                     seq(2, 9, 1), \n                                     seq(20, 90, 10))) +\n  scale_colour_manual(name = NULL, values = c(\"#440154\", \"#277F8E\")) +\n  theme_minimal() +\n  theme(text = element_text(family = \"Arial\"))\n\n# Make the plot interactive using plotly::ggplotly()\nplotly::ggplotly(quantile_plot, tooltip = \"text\")"},{"path":"editingI.html","id":"the-empirical-cumulative-distribution-function","chapter":"6 Analysing film editing I","heading":"6.4.3 The empirical cumulative distribution function","text":"empirical cumulative distribution function (ECDF) complete description data set simple understand. ECDF proportion data set less equal given value:\\[\nF(x) = Pr(x \\leq X) = \\frac{\\#(x \\leq X)}{N}\n\\]ECDF inverse quantile function described swapping axes plot quantile profile Figure 6.4 gives plot ECDF.easy calculate ECDF: simply arrange shot length values order shortest longest, count number shots film (\\(x\\)) less equal specified value (\\(X\\)), divide number total number shots film (\\(N\\)). illustrate process calculate ECDF Blackmail (Table 6.7) plot function.\nTable 6.7: Calculating ECDF Blackmail (1929).\nFigure 6.5 plots ECDF Blackmail shot length linear (Figure 6.5.) logarithmic (Figure 6.5.B) axes illustrate difference applying logarithmic transformation make easier see structure data.\nFigure 6.5: empirical cumulative distribution function Blackmail (1929) plotted () linear (B) logarithmic axes.\nNow understand ECDF calculated plotted, can simplify process using stat_ecdf() function ggplot2 whole process one simple step (Figure 6.6).\nFigure 6.6: empirical cumulative distribution function Blackmail (1929) plotted using stat_ecdf().\nECDF easy calculate requiring complicated mathematics ‚Äì simply count, divide, add. plot easy produce provides lot information data set every little effort , shall see , also simple way comparing multiple data sets.","code":"\ndf_blackmail_ecdf <- df_hitchcock %>%\n  select(film, SL) %>%\n  # Filter to only the shot length data for Blackmail\n  filter(film == \"Blackmail (1929)\") %>%\n  # Sort the shot length data from shortest to longest\n  arrange(SL) %>%\n  # Group the shots by their length\n  group_by(SL) %>%\n  # Count the number of shots with each duration\n  count(SL) %>%\n  rename(frequency = n) %>%\n  ungroup %>%  # Ungroup the data before doing the calculations\n  # Calculate the cumulative frequency and the ECDF\n  mutate(`cumulative frequency` = cumsum(frequency),\n         ecdf = round(`cumulative frequency`/max(`cumulative frequency`), 3))\n# Plot the ECDF on a linear x-axis\nlinear_scale_ecdf <- ggplot(data = df_blackmail_ecdf) +\n  geom_line(aes(x = SL, y = ecdf), colour = \"#277F8E\") +\n  scale_x_continuous(name = \"Shot length (s)\") +\n  scale_y_continuous(name = \"ECDF\") +\n  theme_minimal()\n\n# Plot the ECDF on a logarithmic x-axis\nlog_scale_ecdf <- ggplot(data = df_blackmail_ecdf) +\n  geom_line(aes(x = SL, y = ecdf), colour = \"#277F8E\") +\n  scale_x_continuous(name = \"Shot length (s)\", \n                     trans = \"log10\",\n                     breaks = c(1, 10, 100),\n                     minor_breaks = c(seq(0.02, 0.09, 0.01), \n                                      seq(0.2, 0.9, 0.1), \n                                      seq(2, 9, 1), \n                                      seq(20, 90, 10))) +\n  scale_y_continuous(name = \"ECDF\") +\n  theme_minimal()\n\n# Arrange the plots into a figure\nfigure <- ggpubr::ggarrange(linear_scale_ecdf, log_scale_ecdf, \n                            nrow = 2, ncol = 1, align = \"v\", labels = \"AUTO\")\n\n# Call the figure\nfigure\ndf_blackmail <- df_hitchcock %>%\n  select(film, SL) %>%\n  filter(film == \"Blackmail (1929)\")\n\nggplot(data = df_blackmail, aes(x = SL)) +\n  # Plot as a step plot without adding additional points to the plot\n  stat_ecdf(geom = \"step\", pad = FALSE, colour = \"#277F8E\") +\n  # Use the same axes as before\n  scale_x_continuous(name = \"Shot length (s)\", \n                     trans = \"log10\",\n                     breaks = c(1, 10, 100),\n                     minor_breaks = c(seq(0.02, 0.09, 0.01), \n                                      seq(0.2, 0.9, 0.1), \n                                      seq(2, 9, 1), \n                                      seq(20, 90, 10))) +\n  scale_y_continuous(name = \"ECDF\") +\n  theme_minimal()"},{"path":"editingI.html","id":"adjusted-boxplot","chapter":"6 Analysing film editing I","heading":"6.4.4 Adjusted boxplot","text":"boxplot excellent way visualising shot length data, conveying information location, spread, shape distribution, well identifying outlying data points. Boxplots also efficient method comparing multiple distributions.boxplot provides graphical representation numerical summary distribution. interquartile range distribution represented box, median indicated line box. whiskers extending box mark limits beyond data points considered outliers.Due positively-skewed nature distribution shot lengths motion pictures necessary use adjusted version boxplot avoid incorrectly identifying data points outliers. adjustment described Hubert & Vandervieren (2008) based quantile skewness measure described . implementation adjusted box plot available robustbase package, includes functions directly plotting adjusted boxplot using adjbox() computing statistics producing adjusted boxplots adjboxStats() can plot using ggplot2.can create function adjboxplot() calculate required statistics drawing adjusted box plot using robustbase::adjboxStats() plot result. input function data frame long format variables film SL.can compare shot length distributions Hitchcock‚Äôs early sound films filtering df_hitchcock data frame leave films type sound applying adjboxplot() function.\nFigure 6.7: Adjusted boxplots Alfred Hitchcock‚Äôs early sound films. box represents middle 50% data set. Outlying data points marked white diamonds.\nFigure 6.7 see shot length distributions Hitchcock‚Äôs early sound films exhibit variety changes. Blackmail, see little variation centre distributions range data change \\(IQR\\), decrease lower quartile increase upper quartile. indicates hybrid style Blackmail, combined shots produced silent sound versions, editing Hitchcock‚Äôs early sound films, Murder! Skin Game, shorter takes became slightly shorter shots third quartile became longer accommodate dialogue shots. shift rapid editing style Rich Strange Number Seventeen evident shift distribution short shot duration, narrower \\(IQR\\), smaller range shot lengths. can also see outlying data points later sound films, visualising information represented quantile kurtosis, \\(T\\), much intuitive way.Figure 6.7 contains much information available Table 6.5, presenting information visually makes similarities differences distributions easier perceive trying read large set numbers tabular format.","code":"\n# Load the robustbase, viridis, and ggpubr packages\npacman::p_load(robustbase, viridis, ggpubr)\n\nadjboxplot <- function(x){\n  \n  # Get overall minimum and maximum shot length to set \n  # axis limits for the figure\n  x_min <- min(x$SL); x_max <- max(x$SL)\n  \n  # Set number of colours in palette\n  v_cols <- if(n_distinct(x$film) > 2){\n    viridis(n_distinct(x$film), begin = 0, end = 1)\n  } else {\n    viridis(2, begin = 0, end = 0.5)\n  }\n  \n  # Split data by film\n  df <- x %>% group_by(film) %>% group_split()\n  \n  # Create an empty plot list\n  plot_list <- list()\n\n  for(i in seq_along(df)){\n    \n    # Get adjusted boxplot stats using robustbase::adjboxStats()\n    Film <- rep(df[[i]]$film[1], 5)  # Replicate the title of the film 5 times\n    ajbs <- adjboxStats(df[[i]]$SL)$stats\n    cols <- c(\"ylf\", \"y25\", \"y50\", \"y75\", \"yuf\")\n    df_film <- cbind.data.frame(Film, cols, ajbs)\n    df_film <- df_film %>% \n      pivot_wider(names_from = \"cols\", values_from = \"ajbs\")\n    \n    # Identify outliers\n    Film <- rep(df[[i]]$film[1], length(df[[i]]$SL))\n    SL <- df[[i]]$SL\n    out <- if_else(df[[i]]$SL < ajbs[1] | df[[i]]$SL > ajbs[5], \"yes\", \"no\")\n    df_film2 <- cbind.data.frame(Film, SL, out)\n    \n    # Plot the adjsuted boxplot for a film\n    p <- ggplot() +\n      # Draw the box and the whiskers based on the statistics gathered above\n      geom_boxplot(data = df_film, \n                   aes(x = Film, \n                       ymin = ylf, lower = y25, middle = y50, \n                       upper = y75, ymax = yuf),\n                   fill = v_cols[i], colour = \"black\",\n                   stat = \"identity\") +\n      # Draw the individual data points with added jitter to avoid overlapping\n      geom_jitter(data = df_film2, \n                  aes(x = Film, y = SL, colour = out, fill = out, shape = out), \n                  position = position_jitter(width = 0.2, height = 0)) +\n      coord_flip() +  # Flip the axes of the plot\n      scale_y_continuous(trans = \"log10\", \n                         limits = c(x_min, x_max), \n                         minor_breaks = c(seq(0.02, 0.09, 0.01), \n                                          seq(0.2, 0.9, 0.1), \n                                          seq(2, 9, 1),\n                                          seq(20, 90, 10), \n                                          seq(200, 900, 100))) + \n      scale_colour_manual(values = c(\"black\", \"black\")) +\n      # Distinguish between non-outlying (grey) and \n      # outlying (white) data points\n      scale_fill_manual(values = c(\"grey40\", \"white\")) +\n      scale_shape_manual(values = c(21, 23)) +\n      labs(title = unique(df_film2$Film)) +\n      theme_minimal() +\n      theme(legend.position = \"none\",\n            axis.title = element_blank(),\n            axis.text.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            plot.title = element_text(face = \"bold\", size = 12),\n            panel.grid.major.y = element_blank(),\n            panel.grid = element_line(colour = \"grey70\"))\n    \n    # Add the plot to the plot list\n    plot_list[[i]] <- p\n    \n  }\n  \n  # Arrange and annotate figure\n  figure <- ggarrange(plotlist = plot_list, align = \"v\", \n                      nrow = n_distinct(x$film))\n  figure <- annotate_figure(figure, \n                            bottom = text_grob(\"Shot length (s)\", size = 10.5))\n  return(figure)\n  \n}\ndf_sound <- df_hitchcock %>%\n  filter(type == \"Sound\")\n\nadjboxplot(df_sound)"},{"path":"editingI.html","id":"three-questions-about-shot-length-distributions","chapter":"6 Analysing film editing I","heading":"6.5 Three questions about shot length distributions","text":"Now know describe visualise distribution shot lengths film, can apply methods ask questions evolution style Hitchcock‚Äôs films transition silent sound filmmaking.","code":""},{"path":"editingI.html","id":"question-1","chapter":"6 Analysing film editing I","heading":"6.5.1 Question 1","text":"One approach comparing shot duration motion pictures ask:typical shot length film \\(Y\\) compare typical shot length film \\(X\\)?Answers question typically presented comparison films‚Äô respective cutting rates measured average shot lengths, describes mean (\\(\\mu\\)) waiting time cuts (Salt, 1974, 1992). Conventionally, size difference average shot lengths two films (\\(\\mu_{Y} - \\mu_{X}\\)) interpreted difference style films \\(Y\\) \\(X\\). dominant approach used statistical analyses film style film scholars Bordwell (2002), Buckland (2006), O‚ÄôBrien (2009), Roggen (2019), Salt (1992), amongst many others, rely ASL means describing differences editing style (many cases, exclusively ).key problem approach differences average shot length reflect differences style. example, Barry Salt challenges Andrew Sarris‚Äôs claim Lewis Milestone‚Äôs Front Page (1931) edited quickly Girl Friday (1940), arguing fact case films ASL. However, also points stylistically similar: ‚Äúaverage shot length movies ; however, Milestone film achieves larger number short shots larger number long shots‚Äù (Salt, 1974: 18).compare means Manxman Rich Strange see latter higher ASL, , according conventional interpretation ASLs, means sound film edited slowly. However, plot distributions shot lengths films (Figure 6.8) see Rich Strange shorter takes longer takes Manxman ‚Äì feature captured ASLs.\nFigure 6.8: distribution shot lengths Manxman (1929) Rich Strange (1931).\n\ncommon (typically ) statistic used \ndescribe editing film average shot length\n(ASL).\n\ndescriptive statistics indices distribution, \nASL tell us distribution shot lengths motion\npicture?\n\nMotion picture shot length distributions positively skewed.\nmean positively skewed distribution pulled \ndirection right tail presence outlying data points, \nASL describe centre shot length distribution.\nFurthermore, ASL contains information spread, symmetry,\nmodality, tailedness, peakedness data, presence\noutliers shot length distribution.\n\nSalt (1992: 146) describes \naverage shot length ‚Äòrather obvious concept,‚Äô meaning \nASL elusive. describe feature shot length\ndistribution, mean mean?\n\nInferences ASLs just ‚Äì inferences ASLs ‚Äì\ntell us nothing distribution shot lengths motion\npictures.\n","code":"\n# Select the data for The Manxman and Rich and Strange\ndf_manxman_rich <- df_hitchcock %>%\n  filter(film %in% c(\"The Manxman (1929)\", \"Rich and Strange (1931)\"))\n\nggplot(data = df_manxman_rich) +\n  geom_density(aes(x = SL, fill = film), \n               bw = 0.1, alpha = 0.8, trim = FALSE) +\n  scale_x_continuous(name = \"Shot length (s)\", \n                     trans = \"log10\",\n                     limits = c(0.1, 100),\n                     breaks = c(0.1, 1, 10, 100),\n                     minor_breaks = c(seq(0.2, 0.9, 0.1), \n                                      seq(2, 9, 1), \n                                      seq(20, 90, 10))) +\n  scale_y_continuous(name = \"Density\") +\n  scale_fill_manual(name = NULL, values = c(\"#440154\", \"#277F8E\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"},{"path":"editingI.html","id":"question-2","chapter":"6 Analysing film editing I","heading":"6.5.2 Question 2","text":"alternative way address problem comparing shot lengths two films systematically compare shot lengths two films ask:shots film \\(Y\\) tend longer shots film \\(X\\)?statistical terms, answer question test two distributions stochastically equal testing probability shot film \\(Y\\) greater shot film \\(X\\) equal probability shot \\(X\\) greater one \\(Y\\).calculate probability shot \\(Y\\) greater duration shot \\(X\\) \\((P(Y>X))\\), count number times number times shot \\(Y\\) duration greater shot \\(X\\) divide product number shots (\\(N_{X}\\) \\(N_{Y}\\)) film:\\[\nP(Y > X) = \\frac{\\#Y_{j}>X_{}}{N_{X}N_{Y}}\n\\]find probability length shot \\(Y\\) shorter length shot \\(X\\) \\((P(Y<X))\\) perform procedure based number times shot \\(Y\\) shorter shot \\(X\\).\\(P(Y>X) = P(Y<X)\\), tendency shots one film shots duration greater/less shots . \\(P(Y>X)\\) greater \\(P(Y<X)\\), shots \\(Y\\) tend longer duration \\(X\\). \\(P(Y>X)\\) less \\(P(Y<X)\\), shots \\(Y\\) tend shorter duration \\(X\\).stochastic equality distributions can assessed using dominance statistics \\(d\\) \\(HL\\Delta\\) (Redfern, 2014a).Cliff‚Äôs \\(d\\) statistic (Cliff, 1993, 1996) simple, non-parametric method testing stochastic equality two distributions:\\[\nd = P(Y > X) - P(Y < X)\n\\]value \\(d\\) ranges -1, every shot \\(Y\\) shorter duration every shot \\(X\\), 1, every shot \\(Y\\) longer duration every shot \\(X\\). Stochastic equality \\((P(Y>X) = P(Y<X))\\) occurs \\(d = 0\\).can calculate \\(d\\) quickly using Wilcoxon-Mann-Whitney test (WMW) test, nonparametric statistical test determine difference two datasets testing one sample stochastically superior . \\(\\) statistic stochastic superiority equal \\(P(Y > X) + 0.5P(Y = X)\\), can calculated \\(= U/N_{Y}N_{X}\\), \\(U\\) test statistic WMW test (Delaney & Vargha, 2002). \\(d\\) linear transformation \\(\\): \\(d = 2A ‚Äì 1\\).Cliff‚Äôs \\(d\\) statistic numerous advantages standard practice comparing ASLs determine differences shot length distributions: require assumptions data; robust outliers unequal variances; invariant monotonic transformation; provides direct answer sort questions researchers often wish ask data. Cliff points ,one‚Äôs primary interest quantification statement ‚Äú\\(X\\)s tend higher \\(Y\\)s,‚Äù [\\(d\\)] provides unambiguous description extent (Cliff, 1996: 125).limitation Cliff‚Äôs \\(d\\) helps us understand shots one film tend shorter/longer shots another tell us much shorter longer tend . answer question need another statistic.Hodges-Lehman difference median pairwise differences every shot \\(Y\\) every shot \\(X\\) (Hodges & Lehmann, 1963):\\[\nHL \\Delta = median\\{Y_{j} - X_{}\\}\n\\]words, subtract length every shot film \\(X\\) every shot film \\(Y\\) find median \\(N_{X} √ó N_{Y}\\) differences.\\(HL\\Delta > 0\\), shots \\(Y\\) tend greater duration \\(X\\), \\(HL\\Delta < 0\\) less zero, shots \\(Y\\) tend shorter duration \\(X\\). \\(HL\\Delta = 0\\) tendency shots one film longer .\\(HL\\Delta\\) analogue \\(d\\), advantage expresses differences style measurement scale data analysis (.e., shot lengths seconds). However, \\(HL\\Delta\\) tell us difference , say 2 seconds, small large difference ‚Äì \\(d\\) measures. statistics therefore used conjunction provide complimentary pieces information comparing shot length distributions.Let‚Äôs compare shot length distributions Ring Number Seventeen. First, let‚Äôs get data two films df_hitchcock data frame split separate objects:can apply Wilcoxon-Mann-Whitney \\(U\\) test using wilcox.test() function stats package, part base installation R. can calculate \\(d\\) test statistic \\(U\\) via \\(\\). Setting conf.int = TRUE return \\(HL\\Delta\\) films (Table 6.8).\nTable 6.8: Shots Number Seventeen (1932) tend shorter duration Ring (1927)\nfact \\(d\\) \\(HL\\Delta\\) negative, conclude shots Number Seventeen tend shorter duration Ring median pairwise difference almost second.limitation Cliff‚Äôs \\(d\\) \\(HL\\Delta\\) tell us shots film \\(Y\\) tend longer shots film \\(X\\) size difference, contain information nature difference. problem may addressed visualising shot length distributions using empirical cumulative distribution function described . see difference shot length distributions lie, can plot film‚Äôs ECDF.\nFigure 6.9: empirical cumulative distribution functions Ring (1927) Number Seventeen (1932).\nFigure 6.9 see majority respective distributions ECDF Ring lies right corresponding function Number Seventeen shot lengths former tend longer latter. time can see longer shots latter film tend longer former.Extending approach compare groups films simple calculating dominance statistics silent sound films. can embed loop within loop complete set pairwise results use tidyr::nest() put data film data frame collect data frames single object (Table 6.9).\nTable 6.9: Comparing shot length distributions Alfred Hitchcock‚Äôs late-silent early-sound films using dominance statistics. Negative differences indicate sound films tend shorter shot lengths. ‚òù\ncan visualise results heat map make patterns data easier see.\nFigure 6.10: Comparing shot length distributions Alfred Hitchcock‚Äôs late-silent early-sound films using () Cliff‚Äôs \\(d\\) (B) \\(HL\\Delta\\). Negative differences indicate sound films tend shorter shot lengths.\nchange colours alone Figure 6.10 can see shift Hitchcock‚Äôs editing Blackmail Number Seventeen compared silent films, shots length first sound films tending longer duration silent films, Rich Strange Number Seventeen tend shots shorter duration. example, see shots Rich Strange tend shorter duration Manxman, opposite ASLs films tell us.\ncommit ecological\nfallacy ascribe individual characteristics \ngroup belong. different versions \necological fallacy one concerned \nerror statistical inference arising assumption \nselect individual group higher mean, individual\nhigher value.\n\nfallacy evident use average shot length \nstatistic film style.\n\nASLs Ring Number Seventeen \n5.01s 5.66s, respectively. According conventional\ninterpretation, means shot lengths Number\nSeventeen longer duration Ring.\n\nHowever, applied dominance statistics approach \ncomparing shot length distributions films see shots\nNumber Seventeen tend shorter \nRing ‚Äì exactly opposite ASLs apparently\ntell us.\n\nUsing difference ASLs films means \ncomparing duration shots lead us make commit\necological fallacy making inferences differences \nshot length distributions films assuming \nNumber Seventeen greater ASL shots film\ngreater duration Ring.\n\nAdditionally, see difference ASLs (5.66 - 5.01 = 0.65\nseconds) estimates pairwise difference shot lengths 0.84\nseconds given Hodges-Lehmann difference.\n\nComparing films based respective ASLs \nrelied upon accurately reflect differences style films \noften find committing ecological fallacy \nrely upon ASL. can difference two ASLs \ninterpreted meaningful estimate size difference\nfilms.\n","code":"\n# Select the two films we want and split the data frame\ndf_ring_seventeen <- df_hitchcock %>% \n  filter(film %in% c(\"The Ring (1927)\", \"Number Seventeen (1932)\")) %>%\n  group_by(film) %>% \n  group_split()\n# Create separate data frames for each film -\n# note that dplyr has arranged the films alphabetically after group_split()\ndf_seventeen <- df_ring_seventeen[[1]]\ndf_ring <- df_ring_seventeen[[2]]\n# Apply the WMW test\nwilcox_res <- wilcox.test(df_ring$SL, df_seventeen$SL, conf.int = TRUE)\n\n# Get the test statistic\nU <- wilcox_res$statistic\n\n# Calculate A\nA <- U/(length(df_seventeen$SL)*length(df_ring$SL))\n\n# Calculate Cliff's d\nd <- 2*A - 1\n\n# Get the Hodges-Lehmann difference\nHLD <- wilcox_res$estimate\n# Re-merge the df_ring_seventeen data frame using do.call()\ndf_ring_seventeen <- do.call(rbind, df_ring_seventeen)\n\n# Plot the ECDF for each film\nggplot(data = df_ring_seventeen, \n       aes(x = SL, colour = reorder(film, desc(film)))) +\n  stat_ecdf(geom = \"step\", pad = FALSE) +\n  scale_x_continuous(name = \"Shot length (s)\", \n                     trans = \"log10\",\n                     minor_breaks = c(seq(0.02, 0.09, 0.01), \n                                          seq(0.2, 0.9, 0.1), \n                                          seq(2, 9, 1),\n                                          seq(20, 90, 10), \n                                          seq(200, 900, 100))) +\n  scale_y_continuous(name = \"ECDF\") +\n  scale_colour_manual(name = NULL, values = c(\"#440154\", \"#277F8E\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n# Split the data between silent and sound films, \n# the shot length data by and nest\ndf_silent <- df_hitchcock %>% \n  filter(type == \"Silent\") %>% \n  group_by(film) %>% \n  nest()\n\ndf_sound <- df_hitchcock %>% \n  filter(type == \"Sound\") %>% \n  group_by(film) %>% \n  nest()\n\n# Pairwise comparisons of films\ndf_results <- data.frame()  # Empty data frame to store the results\n\nfor (i in 1:length(df_silent$film)){  # Loop over the silent films\n  for (j in 1:length(df_sound$film)){  # Loop over the sound films\n    \n    n1 <- length(df_silent$data[[i]]$SL)\n    n2 <- length(df_sound$data[[j]]$SL)\n    \n    res <- wilcox.test(df_sound$data[[j]]$SL, df_silent$data[[i]]$SL, \n                       conf.int = TRUE)\n    d <- 2 * (res$statistic/(n1 * n2)) - 1\n    hld <- res$estimate\n    \n    df_temp <- data.frame(Silent = df_silent$film[i], \n                          Sound = df_sound$film[j], \n                          d = round(d, 2), \n                          hld = round(hld, 2))\n    \n    df_results <- rbind.data.frame(df_results, df_temp)\n  }\n}\n# Heat map of Cliff's d\nd_heatmap <- ggplot(data = df_results) +\n  geom_tile(aes(x = Silent, y = reorder(Sound, desc(Sound)), \n                fill = as.numeric(d)), \n            colour = \"white\", size = 0.25) +\n  labs(x = \"Silent\", y = \"Sound\") +\n  scale_fill_viridis_c(name = \"Cliff's d\") +\n  guides(fill = guide_colourbar(barwidth = 1, barheight = 10, \n                                title.position = \"top\")) +  \n  theme_minimal() +\n  theme(legend.position = \"right\",\n        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n# Heat map of HLD\nhld_heatmap <- ggplot(data = df_results) +\n  geom_tile(aes(x = Silent, y = reorder(Sound, desc(Sound)), \n                fill = as.numeric(hld)), \n            colour = \"white\", size = 0.25) +\n  labs(x = \"Silent\", y = \"Sound\") +\n  scale_fill_viridis_c(name = \"HLD (s)\") +\n  guides(fill = guide_colourbar(barwidth = 1, barheight = 10, \n                                title.position = \"top\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\",\n        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n  \n\n# Combine the plots into a single figure\nfig <- ggpubr::ggarrange(d_heatmap, hld_heatmap, \n                         nrow = 2, ncol = 1, labels = \"AUTO\")\n\n# Call the figure\nfig"},{"path":"editingI.html","id":"question-3","chapter":"6 Analysing film editing I","heading":"6.5.3 Question 3","text":"ASL tells us nothing distribution shot lengths film able interpret difference ASLs Front Page Girl Friday, example, necessary additional information respective shot length distributions.Cliff‚Äôs \\(d\\) \\(HL\\Delta\\) describe differences style global level, order fully understand statistics telling us necessary visualise empirical cumulative distribution functions films analyse.cases necessary refer distribution shot lengths contextualise meaning summary measures used. therefore makes sense begin comparison motion picture shot length thinking distributionally differences film style. can therefore ask:shot lengths specific parts respective distributions compare films?answer question can compare quantiles across two groups films, subtracting value \\(p\\)-th quantile silent film \\(p\\)-th quantile sound film plot distribution differences.can create simple function qdiff() calculate pairwise quantile differences silent sound films early Hitchcock data set. function takes two matrices quantiles inputs(x y), assuming matrices equal shapes step quantiles equal. argument p step increase quantiles, case 0.05.prepare data, need split data frame df_hitchcock_quantiles produced earlier contains quantiles film according type film ‚Äì silent sound ‚Äì pivot data frame wide format. applying qdiff() function data silent sound films first drop columns containing film type used calculations convert data format matrix, input type expected function.Now just need apply qdiff() function, produce result Table 6.10.\nTable 6.10: Quantile differences Alfred Hitchcock‚Äôs late-silent early-sound films.\nFigure 6.11 plots difference distributions negative difference indicates \\(p\\)-th quantile silent film greater \\(p\\)-th quantile sound film positive difference indicates greater shot length \\(p\\)-th quantile sound film.\nFigure 6.11: Difference distributions pairwise differences quantiles shot length distributions films directed Alfred Hitchcock, 1927-1932. Positive differences indicate quantiles sound films sample higher silent films.\nFigure 6.11 see differences lower quantiles (\\(Q_{0.05}\\) \\(Q_{0.35}\\)) centred around negative values, reinforcing fact shorter takes Hitchcock‚Äôs sound films tend shorter silent films. time, see distributions differences upper quartiles lie right 0s, showing longer takes sound films tend greater duration silent films. distribution differences shows two peaks quantiles range \\(Q_{0.55}\\) \\(Q_{0.85}\\) can also identify presence sub-groups within sample, accounted second change Hitchcock‚Äôs editing style Rich Strange Number Seventeen shift mass distributions lower tail occurs use rapidly edited montage chase sequences.","code":"\nqdiff <- function(x, y, p = 0.05){\n\n  df <- data.frame()\n  for (i in 1:dim(x)[2]){\n    # Calculate the differences between the quantiles\n    diff <- as.vector(outer(x[, i], y[, i], '-')) \n    q <- i * p  # Set the value of the quantiles for export\n    df_a <- cbind(rep(q, length(diff)), diff)\n    df <- rbind(df, df_a)\n  }\n  colnames(df) <- c(\"quantile\", \"difference\")\n  return(df)\n  \n}\n# Split the data frame containing the quantiles for each \n# Hitchcock film into silent and sound films\ndf_split <- df_hitchcock_quantiles %>% \n  group_by(type) %>% \n  group_split()\n\n# Create data frames for each type of film and re-arrange to a wide format\ndf_hitchcock_silent <- df_split[[1]] %>% \n  pivot_wider(names_from = quantile, values_from = SL) %>%\n  select(-c(film, type)) %>%  # Drop the columns we don't need\n  as.matrix()  # Convert to matrix data type\n\ndf_hitchcock_sound <- df_split[[2]] %>% \n  pivot_wider(names_from = quantile, values_from = SL) %>%\n  select(-c(film, type)) %>% \n  as.matrix()\n# Calculate the quantile differences\ndf_hitchcock_q_diff <- qdiff(df_hitchcock_sound, df_hitchcock_silent)\n# Load the viridis package\npacman::p_load(viridis)\n\nggplot() +  \n  geom_vline(aes(xintercept = 0), linetype = \"dashed\", colour = \"#DDDDDD\") +\n  geom_density(data = df_hitchcock_q_diff, aes(x = difference, \n                                               group = as.factor(quantile), \n                                               colour = as.factor(quantile)), \n               size = 0.6) + \n  geom_text(aes(x = 5, y = 1.25, \n                label = \"Sound films have higher quantiles \\u2192\"),\n            hjust = 0, show.legend = FALSE) +\n  scale_x_continuous(name = \"Difference (s)\", \n                     limits = c(-10, 90), \n                     breaks = seq(-10, 90, 10)) + \n  scale_y_continuous(name = \"Density\", \n                     limits = c(0, 1.25), \n                     breaks = seq(0, 1.25, 0.25)) +\n  scale_colour_viridis(name = \"Quantile\", \n                       labels = c(\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\",\n                                  \"0.30\",\"0.35\", \"0.40\",\"0.45\",\"0.50\",\n                                  \"0.55\",\"0.60\",\"0.65\",\"0.70\", \"0.75\",\n                                  \"0.80\", \"0.85\",\"0.90\", \"0.95\"), \n                       discrete = TRUE, direction = -1) +\n  guides(colour = guide_legend(ncol = 2)) +\n  theme_classic() +\n  theme(legend.key.width = unit(0.6, \"cm\"), \n        legend.key.height = unit(0.4, \"cm\"),\n        legend.position = c(0.75, 0.595),\n        legend.title = element_text(size = 10), \n        panel.grid.minor = element_blank())"},{"path":"editingI.html","id":"summary-4","chapter":"6 Analysing film editing I","heading":"6.6 Summary","text":"chapter covered range methods thinking shot length data distributionally. summarised visualised shot length data films using quantile-based methods easy implement simple understand.also seen average shot length meaningful statistic shot length data, containing information distribution shot lengths film. ASL accurately reflect differences styles motion pictures accurately estimate differences style. answer questions wish ask film style.want ask question ‚Äòshot lengths Hitchcock‚Äôs late-silent films compare early-sound films?‚Äô, comparing quantiles shot length distributions films shows difference editing films complicated can conveyed comparing average shot lengths. dominance statistics approach using \\(HL\\Delta\\) Cliff‚Äôs \\(d\\) provides accurate global description differences shot lengths films ASLs meaning interpretation statistics correspond closely questions wish ask shot length data. However, automatically lead us consider nature differences. key advantage quantile approach can use simple method tp identify talk complicated nature differences thinking shot lengths distributionally.","code":""},{"path":"editingII.html","id":"editingII","chapter":"7 Analysing film editing II","heading":"7 Analysing film editing II","text":"chapter 6 looked describe compare distribution shot lengths motion picture. chapter look editing time series using range methods analyse cutting rates.Cutting rates affect viewers‚Äô experiences cinema number ways, research shown effect viewers‚Äô attention arousal (Kostyrka-Allchorne et al., 2017; Lang et al., 1999; Ludwig & Bertling, 2017), comprehension (Davies et al., 1985), memory (Lang et al., 2000), emotional responses (Heft & Blondal, 1987) motion pictures.Karen Pearlman (2016: 55) writes cutting rate ‚Äòjust another way saying ‚Äúduration shots‚Äù although two ideas overlap,‚Äô distinguishes different ways motion picture editors shape rhythm film controlling timing shots ‚Äì choosing frame cut , placement shot within sequence, length time shot held screen ‚Äì pacing film, controlling rate cuts occur (Pearlman, 2017).However, generally case film scholars discuss cutting rate film terms shot duration, typically expressed form single figure ‚Äì average shot length (ASL). example, David Bordwell uses concepts ASL cutting rate interchangeably:mid- late-1960s, several American British filmmakers experimenting faster cutting rates. Many studio-released films period contain ASLs six eight seconds, significantly shorter averages (Bordwell, 2002: 17).Similarly, Yuri Tsivian argues can track historical change editing patterns ‚Äòcomparing prevailing ASLs‚Äô different historical eras ‚Äòget sense cutting rates changed last hundred years‚Äô (Tsivian, 2009: 95). Sam Roggen (2019) directly conflates cutting rate ASL define former terms latter, referring ‚Äòcutting rates (quantified average shot length).‚Äô many examples found literature usage ASL.Despite claims researchers analysing cutting rates, aspect film style largely overlooked quantitative research film editing, focused almost exclusively shot lengths. may attributed improper use ASL measure cutting rate forestalled work area. Consequently, range statistical methods profitably employed analyse style cinema ignored. Analysing cutting rates requires mental adjustment way think computationally film editing, leaving behind duration shots focus times cuts occur. , cutting rate interested requires us shift focus timing pacing cinema. chapter learn analyse motion picture cutting rates, thinking film editing terms shot duration point process allow us understand pacing cinema.","code":""},{"path":"editingII.html","id":"key-concepts","chapter":"7 Analysing film editing II","heading":"7.1 Key concepts","text":"","code":""},{"path":"editingII.html","id":"rate","chapter":"7 Analysing film editing II","heading":"7.1.1 Rate","text":"rate measure frequency phenomenon interest per unit time expressed ratio total frequency total time elapsed (Everitt & Skrondal, 2010: 358):\\[\nr = \\frac{frequency}{time}\n\\]writing film, two definitions cutting rate, \\(r\\). Pearlman (2019: 153), example, defines cutting rate terms cuts (c) per unit time calculated total number cuts divided running time film:\\[\nr_{c} = \\frac{number\\ \\ cuts}{running\\ time}\n\\]Cut really means transition, includes fades, dissolves, types transitions along hard cuts, moment transition shot shot B (midpoint dissolve) marked cut.Alternatively, Raymond Spottiswoode (1973: 45) defines cutting rate terms number shots (s) per unit time calculated number shots divided running time film:\\[\nr_{s} = \\frac{number\\ \\ shots}{running\\ time}\n\\]shot defined amount time elapsed two cuts.can represent running time film line mark points film cuts occur (Figure 7.1). one--one correspondence cuts shot lengths, time \\(k\\)-th cut occurs equal sum lengths prior shots. one less cut point number intervals created dividing line segments number shots film always \\(c + 1\\): produce film comprising 100 shots need make 99 cuts. Consequently, \\(r_{c}\\) slightly different \\(r_{s}\\). However, allow end final shot classed cut define duration, two definitions equivalent. example, Figure 7.1 hypothetical film 100 cuts (including end final shot) 100 shots, running time ten minutes give\nFigure 7.1: hypothetical film 100 cuts (including end final shot) 100 shots running time 10 minutes. vertical line marks point cut occurs.\n\nimmediately obvious ASL describe cutting\nrate hypothetical film rate. ASL\ndefined mean time elapsed two cuts expressed \nratio total running time film number cuts\n(shots):\n\n\\[\nASL = \\frac{running\\ time}{number\\ \\ cuts\\ (shots)}\n\\]\n\nhypothetical film Figure 7.1 means must wait \\(1/10\\)-th minute, 6 seconds, \naverage cut occurs. meet definition \nrate express frequency events per unit time.\nPhrasing mean waiting time rate, .e., ‚Äò6 seconds per cut,‚Äô\nimply cut duration intent \npotentially confusing types transition (fades \ndissolves) duration even though treat cuts \npurposes analysis.\n\nHowever, common use ASL measure cutting rate.\nexample, Barry Salt uses \\(r_{s}\\)\nmeasure cutting rate, defines glossary \nMoving Pictures \n\nmany shots fixed length film. Measured \nAverage Shot Length (ASL) (q.v.) (Salt, 2006:\n409, quote rendered source).\n\nleads us look definition ASL gives\n\n\nlength running time film, ‚Ä¶, divided number \nshots (407).\n\ndefinition cutting rate clearly odds \ndefinition ASL Salt conflates two concepts assert\n\n\n\\[\n\\frac{number\\ \\ shots}{running\\ time} = \\frac{running\\ time}{number\\\n\\ shots} ,\n\\] obviously possible.\n\nASL reciprocal\ncutting rate multiplied unit time \\(\\big(\\frac{1}{r} \\times t\\big)\\) \nclearly mathematical relationship two concepts, \nbasic error treat equivalent \nconceptually different, measuring different things requiring\ndifferent methods analysis. ASL reciprocal \ncutting rate can never measure cutting rate.\n\nASL measure cutting rate film \ncontains information temporal structure \nfilm.\n","code":""},{"path":"editingII.html","id":"editing-as-a-point-process","chapter":"7 Analysing film editing II","heading":"7.1.2 Editing as a point process","text":"Analysis motion picture cutting rates requires us think editing film simple point process. point process stochastic process time interval \\((0, T]\\), whose realisations times events comprising process occur. temporal point process can represented simply timeline times events occurred marked line.Every point process associated counting process, defined number events (\\(N\\)) occurred including time \\(t\\): \\(N(t)\\). counting process integer values arenon-negative: \\(N(t) \\geq 0\\)non-decreasing: \\(N(t_{2}) \\geq N(t_{1})\\) \\(t_{2} \\geq t_{1}\\)number events interval \\([t_{1},t_{2})\\) equal difference counts two different times \\((N(t_{2}) - N(t_{1}) = N[t_{1},t_{2}))\\).Figure 7.2 illustrates temporal point process, times events occur marked x-axis representing time, associated counting process y-axis.\nFigure 7.2: temporal point process events times (\\(t_{}\\)) counting process (\\(N(t)\\)).\nmotion picture, set \\(T\\) simply times cuts occur:\\[\nT = \\{t_{},\\ ...,\\ t_{N}\\}\n\\]\\(t_{}\\) time \\(\\)-th cut occurs seconds since beginning film \\(N\\) total number cuts film. practice, \\(t_{N}\\) mark end last shot film. , shot defined time elapsed two cuts: \\(s = t_{} - t_{-1}\\).natural think editing point process reflects editors think time cinema. Non-linear editing software DaVinci Resolve Adobe Premiere Pro timeline-based, video audio clips, effects, transitions arranged chronological order horizontally across screen (Figure 7.3).\nFigure 7.3: Timeline-based editing DaVinci Resolve. ‚òùÔ∏è\nedit decision list (EDL) produced non-linear editing system contains times cuts occur beginning film (Table 7.1) simply set times \\(T\\).\nTable 7.1: Extract edit decision list Blackmail (1929) film produced DaVinci Resolve.\nFilm editors describe work terms point processes, marking points line representing running time film nevertheless key concept editors think time cinema. can also key concept way analyse editing cinema.","code":""},{"path":"editingII.html","id":"setting-up-the-project-1","chapter":"7 Analysing film editing II","heading":"7.2 Setting up the project","text":"","code":""},{"path":"editingII.html","id":"create-the-project-1","chapter":"7 Analysing film editing II","heading":"7.2.1 Create the project","text":"Create new project RStudio new directory using New project... command File menu run script projects_folders.R created Chapter 3 create required folder structure.","code":""},{"path":"editingII.html","id":"packages-4","chapter":"7 Analysing film editing II","heading":"7.2.2 Packages","text":"project use packages listed Table 7.2:\nTable 7.2: Packages time series analysis editing data.\n","code":""},{"path":"editingII.html","id":"data-2","chapter":"7 Analysing film editing II","heading":"7.2.3 Data","text":"chapter continue work Early Hitchcock data set (Redfern, 2021b) used Chapter 6. already downloaded saved .csv file containing data, can make copy save Data folder project.","code":""},{"path":"editingII.html","id":"analysing-cutting-rates","chapter":"7 Analysing film editing II","heading":"7.3 Analysing cutting rates","text":"","code":""},{"path":"editingII.html","id":"the-step-function-plot","chapter":"7 Analysing film editing II","heading":"7.3.1 The step function plot","text":"Perhaps simplest way visualise evolution cutting rate film plot counting process \\(N(t)\\) set times \\(T\\) produce step function plot. One advantage plotting step function rather shot lengths motion picture latter can noisy making harder identify editing changes. cutting rate constant, cumulative number cuts step function plot increase linearly. Slower cutting rates still increase cumulative count shallower angle relative time axis running time movie used without consuming large number cuts; faster cutting rates evident plot rises non-linearly steeper angle due many cuts brief period time.illustrate method use data Blackmail (1929). load data set chapter pivot data frame long format, removing NA values.create two plots single figure: rug plot similar Figure 7.1 step function plot .rug plot 1D-strip plot marks points line. create rug plot use geom_vline, plot vertical line every point marked x-axis shot occurs. use ggplot2‚Äôs geom_rug() function add strip plot directly step function plot visually appealing creating rug plot overlap lower left corner step function obscuring data.also need create data frame plotting step function plot comprises cumulative count shots film (.e., shot number) cut timings. vectors data frame need start 0 create data frame add row head data frame containing zero vector using dplyr::add_row().Now can combine two single figure. setting heights argument ggpubr::ggarrange can control relative height plots figure: case want rug plot take 15% figure step function plot take 85% figure.\nFigure 7.4: editing Blackmail (1929) counting process.\nFigure 7.4 plots counting process Blackmail, shows film can divided sections cutting rate increases slope one, including opening sequence Alice Frank go tea house (0-1263.94) beginning investigation discovery body (2135.89-2835.31); sections slope function less one cutting rate slows, sequence Crewe‚Äôs studio Crewe assault‚Äôs Alice kills (1263.94-2835.31) section beginning famous ‚Äòknife‚Äô sequence Tracy attempts blackmail Alice (2835.31-4292.41); sections slope step function greater one associated rapid cutting rate chase sequence British museum (4292.41-4706.77s). change points editing easy identify, enabling us segment film basis cutting rate. also clear film multiple editing regimes speak multiple editing styles Blackmail rather editing style film singular.compare data two films different numbers shots different running times need normalise data unit vector range \\([0, 1]\\) axis. demonstrate process compare editing Blackmail film preceded , Manxman (1929).need create data frame plot combines normalised counts cut times two films. First, need add row containing time index zero shot data films starts 0 seconds. Now can create two new columns: norm_cuts, normalised shot number equal index \\(\\)-th cut divided total number cuts film (\\(N\\)): \\(/N\\); norm_times, normalised cut times equal time \\(\\)-th cut (\\(t_{}\\)) divided running time (\\(T\\)) film: \\(t_{}/T\\).Now wrangled data appropriate format can plot step function film.\nFigure 7.5: normalised step functions Manxman (1929) Blackmail (1929).\nFigure 7.5 plots step functions Manxman Blackmail. immediately apparent two films different cutting rates. cutting rate Blackmail shows much greater variation Manxman, slope one running time. difference editing styles two films immediately apparent plotting counting process cutting rates.","code":"\npacman::p_load(here, tidyverse)\n\n# Load the data\ndf_hitchcock <- read_csv(here(\"Data\", \"early_Hitchcock.csv\"))\n\n# Pivot to long format\ndf_hitchcock <- df_hitchcock %>% \n  pivot_longer(cols = everything(), names_to = \"film\", values_to = \"SL\", \n               values_drop_na = TRUE) %>%\n  arrange(film)\n# Wrangle the data for plotting\ndf_blackmail <- df_hitchcock %>%\n  filter(film == \"Blackmail (1929)\") %>%\n  # Calculate the cut times as the cumulative sum of the shot lengths using cumsum()\n  mutate(cut_times = cumsum(SL),  \n         cut = 1:length(SL)) %>%  # Index the cuts\n  select(-SL)  # Drop the shot length data as we no longer need this\n\n# Create the rug plot\nrug_plot <- ggplot(data = df_blackmail) +\n  geom_vline(aes(xintercept = cut_times), size = 0.1, colour = \"#277F8E\") +\n  scale_x_continuous(name = element_blank(), \n                     limits = c(0, max(df_blackmail$cut_times)), \n                     expand = c(0, 0)) +\n  scale_y_continuous(name = element_blank(), expand = c(0, 0)) +\n  theme_classic() +\n  theme(axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        plot.margin = margin(0.2, 0.4, 0.2, 0.2, \"cm\"))\n# Add a row to the head of the data frame\ndf_blackmail <- df_blackmail %>% \n  add_row(film = df_blackmail$film[1], # Add the title of the film to the zero row\n          cut_times = 0,               # Add the time of the start of the film\n          cut = 0,                     # Number the `zero` cut\n          .before = 1)                 # Specify where the new row is to be added\n\n# Create the plot\nstep_plot <- ggplot(data = df_blackmail) +\n  geom_step(aes(x = cut_times, y = cut), colour = \"#277F8E\") +\n  scale_x_continuous(name = \"Time (s)\", expand = c(0, 0)) +\n  scale_y_continuous(name = bquote(italic(\"N\")*\"(0,\"*italic(\"t\")*\")\"), # format y-axis label\n                     expand = c(0, 0)) +\n  theme_classic() +\n  theme( plot.margin = margin(0.2, 0.4, 0.2, 0.2, \"cm\"))\npacman::p_load(ggpubr)\n\ncount_plot <- ggarrange(rug_plot, step_plot, nrow = 2, ncol = 1, \n                        align = \"v\", heights = c(0.15, 0.85))\n\ncount_plot\n# Select the data for The Manxman and Blackmail and calculate the cut times\ndf_films_step_plot <- df_hitchcock %>%\n  filter(film %in% c(\"Blackmail (1929)\", \"The Manxman (1929)\")) %>%\n  group_by(film) %>%\n  mutate(cut_times = cumsum(SL),  \n         cut = 1:length(SL)) %>%   \n  select(-SL) \n\n# Add the row with the zero cut for each film\ndf_films_step_plot <- df_films_step_plot %>% \n  group_by(film) %>%\n  group_modify(~ add_row(.x, \n                         film = .x$film[1], \n                         cut_times = 0,     \n                         cut = 0,           \n                         .before = 1))\n\n# Normalise the cut times and cut numbers\ndf_films_step_plot <- df_films_step_plot %>%\n  mutate(norm_times = cut_times/max(cut_times),\n         norm_cut = cut/max(cut))\nggplot(data = df_films_step_plot) + \n  geom_step(aes(x = norm_times, y = norm_cut, colour = film)) +\n  scale_x_continuous(name = \"Normalised running time\", \n                     breaks = seq(0, 1, 0.2), expand = c(0,0)) +\n  scale_y_continuous(name = \"Normalised count\", \n                     breaks = seq(0, 1, 0.2), expand = c(0,0)) +\n  scale_colour_manual(name = NULL, values = c(\"#277F8E\", \"#440154\")) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")"},{"path":"editingII.html","id":"cut-density","chapter":"7 Analysing film editing II","heading":"7.3.2 Cut density","text":"Chapter 6 looked use kernel densities describe distribution shots motion picture. can use method describe pacing film fitting kernel density function timeline film‚Äôs cuts.Cut density uncomplicated way assessing cutting rate film fitting kernel density estimate point process. often referred shot density, mathematical graphical operations based times cuts duration shots properly refer cut density using method. Additionally, ‚Äòshot density‚Äô easily confused use kernel densities describe shot length distribution film.context cutting rate film, density greatest one cut quickly follows another , therefore, faster cutting rate point film; low densities indicate cuts distant one another time axis.calculate cut density Number Seventeen (1932). First, select data calculate cut times. Next, calculate density cut_times vector using R‚Äôs built density() function. , need decide bandwidth parameter (bw) want use. also set value n, number points x-axis density calculated. Setting n = 500 means density calculated 500 evenly-spaced points x-axis representing running time film. become relevant want compare densities multiple films. commands set range density calculated. want density cover whole range f films running time set 0 set total running time film, maximum value cut_times vector.density() function returns list, want access x variable, locates points x-axis density calculated, y variable, stores density values. can extract list save time density variables.Rather create separate rug plot, time add rug plot actual cut times df_seventeen using geom_rug() command. Note need use cut times df_seventeen data frame time variable df_seventeen_density marks points density evaluated timings cuts.\nFigure 7.6: cut density Number Seventeen (1932).\nFigure 7.6 plots cut density Number Seventeen shows cutting rate increases course film shift rapid cutting second half film. Within large-scale trend, localised moments rapid editing key feature latter part film density moments drop ground level exits higher contribute overall trend rapid editing. key advantage visualising cutting rate way, , lets us see different things editing film different scales, allowing us identify events micros-scale individual moments film well describing editing macro-scale film, see relationship structures scales.harder localise change points cutting rates plotting cut density compared counting process, easier look editing structures different scales plotting density.Comparing time series data multiple films presents us problems. running time motion pictures varies time series editing data different lengths, number cuts motion picture varies different number events recorded time series. uniform editing pattern events recorded irregular intervals.problems easily come comparing cut densities different films can normalise running time different films unit vector comparing step functions . can calculate cut density equal number points time axis setting n value calculating density film. give us time series equal numbers points sampled regular intervals \\(1/n\\) normalised interval \\([0, 1]\\).also need standardise cut density \\(f(y)\\) film interval \\([0, 1]\\) subtracting minimum value \\(y\\) \\(\\)-th value density \\(y\\) dividing range:\\[\nf(y) = \\frac{y_{} - y_{min}}{y_{max} - y_{min}}\n\\]sets point cut density highest film value 1; point density lowest set value 0; rest values scaled within interval. compare non-normalised densities, important remember different films exhibit different variances densities, may lead incorrect interpretations data counts high low density different film.Note process similar normalisation process comparing step functions two () films, x- y-axes plot cover range \\([0, 1]\\). However, normalising cut densities different film number rows data frame (500) irrespective number shots film, whereas number data points film remained equal number cuts normalising data step function plot.illustrate process compare cut density Number Seventeen Ring (1927). ‚Äôve already created data fame containing cut density Number Seventeen, need repeat process create data frame Ring combine two together.combined data frames can normalise running times density values.Figure 7.7 plots cut densities Number Seventeen Ring. can see Ring different editing structure, comprised three large sections account approximately one-third running time , declining trend course first two sections increasing trend cutting rate final third.\nFigure 7.7: Comparing cut densities Number Seventeen (1932) Ring (1927).\ncan extend comparison cut density films Early Hitchcock data set. need use original data file containing shot length data Hitchcock‚Äôs films rather df_hitchcock data frame working far.loop column .csv file calculate density film 500 points x-axis. also normalise running times density values. instance, store results loop need explicitly specify empty data frame 500 rows.data frame df_hitchcock_density wide format, plot data need pivot data frame long format. also need add column, time, containing points density evaluated film.Figure 7.8 plots densities films Early Hitchcock data set sorted type, loess trend line fitted group.\nFigure 7.8: normalised cut densities Alfred Hitchcock‚Äôs late-silent early-sound films, fitted loess trendlines. ‚òùÔ∏è\nComparing silent sound films see cut densities former tend higher latter running time film, loess trendline silent films tracking standardised density approximately 0.45 normalised times films compared level ~0.25 sound films.key similarities cutting rates two groups. groups films peak trendline cutting rate increases closing sections film, tends occur slightly earlier sound films (~90% normalised running time) compared silent films (~95%). Several films sample (Champagne, Manxman, Skin Game, Rich Strange, Number Seventeen) large scale structure density second half film different compared first half, exhibiting either less variation cutting rate second half film; absence (Champagne) presence (Number Seventeen) trend cutting rate second half film, reversal trend cutting compared first half film (Manxman ‚Äì film densities falls first 52.2% film minimum level reaching normalised density 0.994 94.4% film); absence high densities second half (Skin Game). suggests Hitchcock‚Äôs style change several key respects introduction sound technologies, 1931 editors returned elements style late-silent films.","code":"\n# Load the data\ndf_seventeen <- df_hitchcock %>%\n  filter(film == \"Number Seventeen (1932)\") %>%\n  mutate(cut_times = cumsum(SL))\n\n# Calculate the density\nseventeen_density <- density(df_seventeen$cut_times, bw = 25, n = 500, \n                          from = 0, to = max(df_seventeen$cut_times))\n\n# Create a data frame for plotting\ndf_seventeen_density <- data.frame(film = rep(\"Number Seventeen (1932)\", 500), \n                                   times = seventeen_density$x,\n                                   density = seventeen_density$y)\n\nhead(df_seventeen_density)##                      film     times      density\n## 1 Number Seventeen (1932)  0.000000 9.649560e-06\n## 2 Number Seventeen (1932)  7.426854 1.559920e-05\n## 3 Number Seventeen (1932) 14.853707 2.319523e-05\n## 4 Number Seventeen (1932) 22.280561 3.170452e-05\n## 5 Number Seventeen (1932) 29.707415 3.974960e-05\n## 6 Number Seventeen (1932) 37.134269 4.571711e-05\n# Set the scipen option to suppress scientific notation on axis labels\noptions(scipen = 9999)\n\nggplot() +\n  geom_line(data = df_seventeen_density, aes(x = times, y = density), \n            colour = \"#277F8E\", size = 1) +\n  geom_rug(data = df_seventeen, aes(x = cut_times), colour = \"#277F8E\") +\n  scale_x_continuous(name = \"Time (s)\") +\n  scale_y_continuous(name = \"Density\") +\n  theme_classic()\n# Load the data\ndf_ring <- df_hitchcock %>%\n  filter(film == \"The Ring (1927)\") %>%\n  mutate(cut_times = cumsum(SL))\n\n# Calculate the density\nring_density <- density(df_ring$cut_times, bw = 25, n = 500, \n                        from = 0, to = max(df_ring$cut_times))\n\n# Create a data frame for plotting\ndf_ring_density <- data.frame(film = rep(\"The Ring (1927)\", 500), \n                              times = ring_density$x,\n                              density = ring_density$y)\n\n\n# Combine the data frames for Number Seventeen and The Ring\ndf_films_density <- rbind(df_seventeen_density, df_ring_density)\n# Normalise the times and density vectors for each film\ndf_films_density <- df_films_density %>%\n  group_by(film) %>%\n  mutate(norm_times = times/max(times),\n         norm_density = (density - min(density))/(max(density) - min(density)))\nggplot(data = df_films_density) +\n  geom_line(aes(x = norm_times, y = norm_density, colour = film), size = 1) +\n  scale_x_continuous(name = \"Normalised time\", breaks = seq(0, 1, 0.2), \n                     labels = scales::percent) +\n  scale_y_continuous(name = \"Normalised density\", breaks = seq(0, 1, 0.2)) +\n  scale_colour_manual(name = NULL, values = c(\"#277F8E\", \"#440154\")) +\n  facet_wrap(~film, nrow = 2) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        strip.background = element_blank(),\n        strip.text = element_text(face = \"bold\", hjust = 0, size = 10.5))\n# Load the original data\ndf_original <- read_csv(here(\"Data\", \"early_Hitchcock.csv\"))\n\n# Get a list of the column names (i.e., films)\nfilms <- colnames(df_original)\n\n# Create an empty data with 500 rows\ndf_hitchcock_density <- data.frame()[1:500,]\n\nfor (i in 1:dim(df_original)[2]){\n  \n  dat <- na.omit(df_original[i])\n  dat <- dat %>% mutate(cuts = cumsum(dat[[1]]))\n  \n  df_temp <- data.frame(density(dat[[2]], bw = 20, n = 500, \n                                from = 0, to = max(dat))[2]) %>%\n    rename(density = 1) %>%\n    # Normalise the density\n    mutate(norm_density = (density - min(density))/(max(density) - min(density))) %>%\n    select(norm_density)\n  \n  names(df_temp)[1] <- films[i]\n  \n  df_hitchcock_density <- cbind(df_hitchcock_density, df_temp)\n  \n}\n\n# Save the data frame with the normalised density estimates as a .csv file\nwrite_csv(df_hitchcock_density, here(\"Data\", \"norm_density.csv\"))\n# Load the normalised density data if necessary\ndf_hitchcock_density <- read_csv(here(\"Data\", \"norm_density.csv\"))\n\n# Re-arrange the data for plotting\ndf_density_plot <- df_hitchcock_density %>% \n  pivot_longer(cols = 1:9, names_to = \"film\", values_to = \"density\") %>%\n  arrange(film) %>%\n  mutate(density = round(density, 3),\n         time = rep(1:500/500, 9),\n         type = ifelse(film %in% c(\"The Ring (1927)\",\n                                   \"The Manxman (1929)\",\n                                   \"The Farmers Wife (1928)\",\n                                   \"Champagne (1928)\"),\n                       \"Silent\", \"Sound\")) %>%\n  relocate(time, .after = film)\n\n# Assert the ordering of the films as a factor\ndf_density_plot$film <- factor(df_density_plot$film,\n                               levels = c(\"The Ring (1927)\",\n                                          \"The Farmers Wife (1928)\",\n                                          \"Champagne (1928)\",\n                                          \"The Manxman (1929)\",\n                                          \"Blackmail (1929)\",\n                                          \"Murder (1930)\",\n                                          \"The Skin Game (1931)\",\n                                          \"Rich and Strange (1931)\",\n                                          \"Number Seventeen (1932)\"))\n# Load the plotly package to create an interactive plot\npacman::p_load(plotly)\n\n# Create the plot\ndensity_plot <- ggplot(data = df_density_plot) +\n  geom_line(aes(x = time, y = density, group = film, colour = film,\n                text = paste0(film,\n                              \"<br>Time: \", time * 100, \"%\",\n                              \"<br>Density: \", density))) +\n  geom_smooth(aes(x = time, y = density, group = type), \n              colour = \"black\", method = \"loess\", span = 0.15, \n              se = FALSE, size = 0.7) +\n  scale_x_continuous(name = \"Normalised time\\n\", breaks = seq(0, 1, 0.2), \n                     labels = scales::percent) +\n  scale_y_continuous(name = \"Normalised density\", breaks = seq(0, 1, 0.2)) +\n  scale_colour_viridis_d(name = \"Film\") +\n  facet_wrap(~type, nrow = 2) +\n  theme_classic() +\n  theme(strip.background = element_blank(),\n        panel.spacing = unit(25, \"pt\"),\n        strip.text = element_text(face = \"bold\", hjust = 0, size = 10.5))\n\n# Pass the plot to plotly to create an interactive version\nggplotly(density_plot, tooltip = c(\"text\")) %>% \n  layout(legend = list(orientation = 'h', xanchor = \"left\", y = -0.2,\n                       title = list(text = '<b> Film <\/b>'), \n                       font = list(size = 10.5)))"},{"path":"editingII.html","id":"clustering","chapter":"7 Analysing film editing II","heading":"7.3.3 Clustering","text":"can look relationships cut densities Hitchcock‚Äôs films applying clustering methods group films according cutting rates. section apply two different methods cluster Hitchcock time series data: principal components analysis agglomerative hierarchical clustering. Redfern (2014b) Baxter et al. (2017) previously applied principal component analysis motion picture editing data, remains largely unused means exploring questions film style.","code":""},{"path":"editingII.html","id":"PCA","chapter":"7 Analysing film editing II","heading":"7.3.3.1 Principal component analysis","text":"Principal component analysis (PCA) type geometric data analysis exploring visualisng high dimensional data sets. used reduce dimensionality data set transforming large number correlated variables small set new, uncorrelated variables called principal components dimensions simplify structure data eliminating redundancy variables retaining information. principal components linear combinations original variables maximise variance observations plotting transformed data set low-dimensional space principal components axes, can see relationships individuals data set similar data points cluster together.cover linear algebra behind principal components analysis . interested, Jolliffe & Cadima (2016) provides overview mathematics PCA.PCA sensitive variances original variables data must standardised (applying \\(k\\)-medoids clustering Chapter 5) normalised prior applying analysis. already taken care step normalised range cut densities can proceed calculating PCA.step need perform transpose data frame films move columns rows. PCA terms, means transforming films variables individuals. using R‚Äôs t() function create new data frame df_hitchcock_density_t 500 columns numerical variables (densities) observed nine individuals (films). also add variable identifying film silent sound, though use variable reducing dimensionality via PCA.use FactoMineR package perform PCA. perform principal component analysis drop type variable, leaving first column data frame: df_hitchcock_density_t[,-1]. use variable later graphical parameter. factoextra package designed work FactoMineR, taking output PCA streamlining process visualising results style ggplot2 plots. use package plot results rather use default plotting options FactoMineR, styled base R graphics. Consequently, set graph = FALSE call PCA(). Note FactoMineR::PCA() uses correlation matrix rather variance-covariance matrix.determine many dimensions retain need look eigenvalues results PCA. eigenvalue represents proportion total variance explained dimension sum eigenvalues equal number variables entered PCA ‚Äì example, 500 points cut density evaluated. eigenvalues , see dimension 1 eigenvalue 203.07 accounts 40.6% total variance data frame cut densities.scree plot plots proportion variance eigenvalue arranged order size, point curve plot levels (‚Äòelbow‚Äô) taken cut point retaining dimensions. Figure 7.9 plots scree plot Early Hitchcock cut densities shows elbow plot comes first two dimensions, account 53.3% total variance data set. retain two dimensions.\nFigure 7.9: Scree plot eigenvalues principal components analysis cut densities Alfred Hitchcock‚Äôs late-silent early sound films.\ncan get information well dimensions represent data films accessing numerical summaries individuals. get coordinates film first two dimensions call res_pca$ind$coord[, 1:2]. coordinates used plot data points Figure 7.10 projection film‚Äôs distance origin given principal component multidimensional space.assess much film contributes variance dimension look contributions. larger value film contributes variance dimension. sum contributions dimension 100%. example, see Number Seventeen contributes relatively little variance first dimension accounts almost third variance second dimension.Finally, can look squared cosine (cos2) principal component see well component represents individual. closer value cos2 1 better film represented dimension. Looking squared cosine Number Seventeen see relatively poorly represented first dimension represented much better second dimension.films poorly represented dimensions (e.g., Rich Strange) better represented dimensions. However, dimensions may dominated single film provide useful information others sample. may reflect stylistic uniqueness film, single principal component captures key aspects style. may also reflect fact film strongly associated single dimension ‚Äôs variance spread across multiple principal components, case Rich Strange.factoextra::fviz_pca_ind() can plot first two dimensions PCA, using type variable assign colour identifying types factors passing factors habillage argument visualising distribution points. Films cutting rates negatively correlated lie opposite sides origin plot, films positively correlated cutting rates lie side origin.\nFigure 7.10: first two principal components cut densities Alfred Hitchcock‚Äôs late-silent early sound films.\nFigure 7.10 see first dimension separates films sample based type. reflects fact clear difference cutting densities Hitchcock‚Äôs late-silent early sound films. Figure 7.8 know silent films tend higher densities course running time sound films. clustered data according type.second dimension indicates subgroups within silent sound films, reflecting local similarities differences cut density. Films lie direction origin Figure 7.10 stylistically similar one another films orientated different direction. silent films, Ring Farmer‚Äôs Wife exhibit change density approximately half way see Champagne Manxman; times peaks troughs occur density plots former two films tend similar one another latter two, tend similar one another. sub-grouping splits silent films chronologically, suggesting evolution editing style late-1927 early-1929. interesting see editing Hitchcock‚Äôs late-silent films Downhill (1927) Easy Virtue (1928) compare four silent films analysed .sub-groupings sound films, however, arranged chronological order, indicating cutting rates Hitchcock‚Äôs films changed shift synchronised sound production editing style Hitchcock‚Äôs early sound films evolve consistent manner changed film film. cut densities Murder Skin Game similar one another sound films. Interestingly, cut density Blackmail similar Rich Strange either films sample released two films. may Blackmail includes sequences rapid editing (chase British Museum) absent Murder Skin Game common sound films late-1931 1932 learnt Chapter 6 edited quickly silent films sample. Number Seventeen different sound films, possibly due trend increasing cutting rate course running time. Hitchcock appear settled stable editing style early sound period adapted demands new filmmaking technologies.","code":"\n# Transpose the data frame df_hitchcock_density\ndf_hitchcock_density_t <- df_hitchcock_density %>% t() %>% data.frame()\n\n# Set the type of film\ndf_hitchcock_density_t <- df_hitchcock_density_t %>% \n  mutate(type = c(rep(\"Silent\", 4), rep(\"Sound\", 5))) %>%\n  relocate(type)\n\nhead(df_hitchcock_density_t[, 1:5])##                           type          X1         X2        X3        X4\n## The Ring (1927)         Silent 0.176046922 0.33191802 0.5038065 0.6604278\n## The Farmers Wife (1928) Silent 0.075249837 0.16259650 0.2656994 0.3794499\n## Champagne (1928)        Silent 0.004760131 0.06277451 0.1457368 0.2542118\n## The Manxman (1929)      Silent 0.016365188 0.08136534 0.1700914 0.2745400\n## Blackmail (1929)         Sound 0.296025741 0.42090597 0.5189201 0.5762711\n## Murder (1930)            Sound 0.186047943 0.22193604 0.2283834 0.2359117\n# Load the packages for PCA\npacman::p_load(FactoMineR, factoextra)\n\n# Perform PCA after dropping the type column\nres_pca <- PCA(df_hitchcock_density_t[,-1],  graph = FALSE)\n# Extract eigenvalues/variances from the results of the \n# principal components analysis\nget_eig(res_pca)##       eigenvalue variance.percent cumulative.variance.percent\n## Dim.1  203.06792        40.613585                    40.61358\n## Dim.2   63.49873        12.699746                    53.31333\n## Dim.3   49.07460         9.814920                    63.12825\n## Dim.4   46.73724         9.347448                    72.47570\n## Dim.5   41.47382         8.294764                    80.77046\n## Dim.6   37.46899         7.493798                    88.26426\n## Dim.7   32.05351         6.410703                    94.67496\n## Dim.8   26.62519         5.325037                   100.00000\n# Visualize variances of the dimensions\nfviz_screeplot(res_pca, addlabels = TRUE, ylim = c(0, 45), \n               barfill = \"#277F8E\", barcolor = \"#277F8E\")\nres_pca$ind$coord[, 1:2]##                              Dim.1      Dim.2\n## The Ring (1927)          14.545192  -6.157487\n## The Farmers Wife (1928)  19.319367  -9.554902\n## Champagne (1928)         13.644166   5.634511\n## The Manxman (1929)       12.895895   4.123103\n## Blackmail (1929)        -10.757834   5.888765\n## Murder (1930)           -15.926924  -6.422667\n## The Skin Game (1931)    -20.959320 -10.331085\n## Rich and Strange (1931)  -6.036238   2.518432\n## Number Seventeen (1932)  -6.724303  14.301329\nres_pca$ind$contrib[, 1:2]##                             Dim.1     Dim.2\n## The Ring (1927)         11.575908  6.634366\n## The Farmers Wife (1928) 20.422173 15.975150\n## Champagne (1928)        10.186152  5.555267\n## The Manxman (1929)       9.099533  2.974685\n## Blackmail (1929)         6.332363  6.067936\n## Murder (1930)           13.879697  7.218107\n## The Skin Game (1931)    24.036462 18.676021\n## Rich and Strange (1931)  1.993650  1.109821\n## Number Seventeen (1932)  2.474063 35.788647\nres_pca$ind$cos2[, 1:2]##                             Dim.1      Dim.2\n## The Ring (1927)         0.4056101 0.07269036\n## The Farmers Wife (1928) 0.5739696 0.14039628\n## Champagne (1928)        0.3761996 0.06415594\n## The Manxman (1929)      0.3571538 0.03650907\n## Blackmail (1929)        0.2834381 0.08492922\n## Murder (1930)           0.4988665 0.08112438\n## The Skin Game (1931)    0.6279706 0.15257270\n## Rich and Strange (1931) 0.1145736 0.01994400\n## Number Seventeen (1932) 0.1043563 0.47203789\n# Set the type of a film as a factor\ntype <- as.factor(df_hitchcock_density_t$type)\n\n# Plot the films on the first two principal components\nfviz_pca_ind(res_pca,\n             habillage = type,  # Colour by type\n             palette = c(\"#440154\", \"#277F8E\"),\n             legend.title = \"Type\", title = \"\",\n             addEllipses = TRUE, ellipse.type = \"convex\",  # Add convex hulls\n             repel = TRUE, labelsize = 4)  # Format data labels"},{"path":"editingII.html","id":"AHC","chapter":"7 Analysing film editing II","heading":"7.3.3.2 Agglomerative hierarchical clustering","text":"alternative approach looking relationships among style Hitchcock‚Äôs films hierarchical clustering.Hierarchical clustering process sorting set unlabelled data sets groups within-group similarity minimized -group dissimilarity maximized. Agglomerative clustering begins object cluster proceeds build clusters bottom-merging together closest clusters, moving hierarchy clusters merged together.PCA, necessary standardise normalise data remove baseline differences differences scale applying clustering algorithm.first stage hierarchical clustering analysis calculate distance matrix pairwise distances cut densities Hitchcock‚Äôs films. , every pair films sample calculate distance corresponding points cut densities. use Euclidean distance, square root sum squared distances data points two objects:\\[\n||-b||_{2} = \\sqrt{\\sum_{}(a_{}-b_{})^{2}}\n\\]calculate distance matrix use R‚Äôs dist() function. use data frame df_hitchcock_density_t used principal component analysis , , drop type column clustering.calculated distance matrix need assign objects clusters using linkage criteria determines point distances interpreted. Various criteria available:single linkage: clusters merged based minimum distance two points existing clusters.complete linkage: clusters merged based distance distant elements existing clusters.unweighted pair group method arithmetic mean: clusters merged based average pairs data points existing clusters.Ward‚Äôs method: clusters merged minimizing total within-cluster variance.example, use Ward‚Äôs method.output hierarchical clustering dendrogram. dendrogram tree diagram representing nested grouping data objects. individual data objects placed end branches dendrogram called leaves. group data objects called clade, may contain single leaf. Data objects within clade similar one another objects another clade. point branches dendrogram join represent distance clusters merged together. Items similar one another combined low heights, dissimilar items combined greater heights.create ggplot2-style dendrogram use ggdendro package extends functionality ggplot2. reformat data suitable plotting ggplot2 object. also create data frame labels can assign colours films dendrogram according type.Now can plot dendrogram.\nFigure 7.11: Dendrogram results agglomerative hierarchical clustering cut densities Alfred Hitchcock‚Äôs late-silent early-sound films.\ndendrogram Figure 7.11 distinguishes silent sound films, grouping films type one another. distance silent sound films greater distances films within groups, indicating majority part variation editing films films made introduction synchronised sound. , reflects fact silent films higher standardised cut densities course running time compared sound films.sub-groupings silent films within types reflects PCA results , films closest one another multidimensional space principal components analysis clustered together lower level smaller distance dendrogram.sub-groupings sound films, however, different compared PCA results. Number Seventeen Blackmail similar one another among sound films , dendrogram clusters Murder Rich Strange similar one another Skin Game, whereas results PCA place Murder Skin Game similar one another Rich Strange. may reflect differences way clustering algorithms group films, one method sensitive features data method sensitive different set features. Nonetheless, results hierarchical clustering analysis reinforce interpretation PCA results Hitchcock yet settle stable style early sound period reflected fact sound films grouped release date. seen, earliest (Blackmail) latest (Number Seventeen) films group clustered together similar one another.Finally, important remember fact two films clustered together dendrogram mean edited way. means similar objects sample subjected clustering algorithm. Cluster analysis interpret data; responsibility analyst. Cluster analysis subjective, value derives fact allows us search large amounts data alert us possibilities relationships films wish analyse.","code":"\n# Calculate the distance matrix using the Euclidean distance\nmat_distance <- dist(df_hitchcock_density_t[, -1], method = \"euclidean\")\n# Assign clusters\nhclust_ward <- hclust(mat_distance, method = \"ward.D2\")\n# Load the ggdendro package\npacman::p_load(ggdendro)\n\n# Convert the clustering solution to a format suitable for ggplot2\ndendr <- dendro_data(hclust_ward, type=\"rectangle\")\n\n# Access the labels for the films\nlabs <- label(dendr)\n\n# Add the type of film to the labels\nlabs$group <- c(rep(\"Silent\", 4), rep(\"Sound\", 5))\n\nlabs##   x y                   label  group\n## 1 1 0         The Ring (1927) Silent\n## 2 2 0 The Farmers Wife (1928) Silent\n## 3 3 0        Champagne (1928) Silent\n## 4 4 0      The Manxman (1929) Silent\n## 5 5 0        Blackmail (1929)  Sound\n## 6 6 0 Number Seventeen (1932)  Sound\n## 7 7 0    The Skin Game (1931)  Sound\n## 8 8 0           Murder (1930)  Sound\n## 9 9 0 Rich and Strange (1931)  Sound\n# Plot the dendrogram\nggplot(segment(dendr)) +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_text(data = label(dendr), \n            aes(x = x, y = -0.2, label = paste0(\"  \", label), colour = labs$group), \n            size = 3, hjust = 1) +\n  scale_y_continuous(name = \"Distance\", expand = c(0.5, 0), \n                     breaks = seq(0, 12.5, 2.5)) +\n  scale_colour_manual(name = NULL, values = c(\"#440154\", \"#277F8E\"),\n                      labels = c(\"Silent\", \"Sound\")) +\n  coord_flip(ylim = c(0, 9)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        axis.line.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title.y = element_blank(),\n        panel.grid = element_blank())"},{"path":"editingII.html","id":"summary-5","chapter":"7 Analysing film editing II","heading":"7.4 Summary","text":"advantage using methods described chapter think editing point process simple interpret describe cutting rate directly way reflects editors work timeline. Comparatively, shot durations may shorter longer, often want describe film‚Äôs editing quicker slower intuitive depict increasing cutting rate rising curve plotting step function instantaneous rate rather plotting decrease shot duration. Plotting step function, immediately obvious cutting rate changes, making easier identify changes film‚Äôs editing occur. cut density smooths data loss information sensitive changes editing advantage representing cutting rate intend analyse. Furthermore, cut density provides means can represent editing film suitable cluster analysis. applying principal components analysis hierarchical clustering editing time series can explore relationships among stylistic formal features groups films; , given size complexity data sets record style motion pictures, valuable tools can make life much simpler analyst.","code":""},{"path":"categorical.html","id":"categorical","chapter":"8 Analysing shot data","heading":"8 Analysing shot data","text":"chapter analyse categorical variables film style, shot scales, types camera movement, types relationships shots. area computational film analysis largely overlooked. Certainly compared analysis shot length data, relatively studies type data derived films, shown distribution shot scale film may important identifier style individual film, style individual author across body work, various narrative affective functions film, including viewer‚Äôs assessment film‚Äôs mood narrative engagement (Benini et al., 2016, 2022; Savardi et al., 2018, 2021; Svanera et al., 2019).revisit analysis film style narration Rashomon (Kurosawa Akira, 1950) (Redfern, 2013b). one pieces research computational film analysis provided tutorial demonstrate analysis performed replicate analysis, good opportunity fill gap. look describe categorical variables look temporal relationships shots. Finally, apply multiple correspondence analysis categorical shot data can look relationships different aspects film style.","code":""},{"path":"categorical.html","id":"categorical-variables","chapter":"8 Analysing shot data","heading":"8.1 Categorical variables","text":"categorical variable measurement scale comprising set categories. Categorical variables often called qualitative variables observation assigned category possess quality.Categorical variables also called nominal variables data values names categories describing object take numerical value. example, shot can categorised ‚Äòtracking shot‚Äô data value recorded shot. nominal variables implied ordering categories therefore called ordinal variables. example, shot scale can considered ordinal variable ordering based relationship subject camera, big close long shot.individual can categorised many different variables time, individual can belong one category variable. example, shot film may point--view shot close-; point--view shot point--view shot close-long shot simultaneously.Analysis categorical variables involves looking frequency event occurs (e.g., many panning shots film?).methods use analyse categorical variables different used analyse quantitative variables (shot duration). example, calculate mean shot scale film data numerical. simply make sense speak mean shot scale film.","code":""},{"path":"categorical.html","id":"setting-up-the-project-2","chapter":"8 Analysing shot data","heading":"8.2 Setting up the project","text":"","code":""},{"path":"categorical.html","id":"create-the-project-2","chapter":"8 Analysing shot data","heading":"8.2.1 Create the project","text":"Create new project RStudio new directory using New project... command File menu run script projects_folders.R created Chapter 3 create folder structure required project.","code":""},{"path":"categorical.html","id":"packages-5","chapter":"8 Analysing shot data","heading":"8.2.2 Packages","text":"project use packages listed Table 8.1.\nTable 8.1: Packages analysing shot data.\n","code":""},{"path":"categorical.html","id":"data-3","chapter":"8 Analysing shot data","heading":"8.2.3 Data","text":"data used chapter comprises data collected analysis film style narration Rashomon. data can accessed ResearchGate profile Excel spreadsheet. spreadsheet contains data four main narratives film: Bandit‚Äôs story, Wife‚Äôs story, Husband‚Äôs story, Woodcutter‚Äôs second account latter stages film. shots narratives included shots courtyard excluded narrated priest woodcutter. brief narratives priest police agent excluded comprise handful shots small effective data analysis. also excluded woodcutter‚Äôs walk forest discovery husband‚Äôs body. None shots scenes priest, woodcutter, commoner Rashomon included. limiting data set four narratives rape murder, comparing narratively similar sections film focus set events.collected data four variables film style, shot scale, camera movement, camera angle shot type, variable several categories.","code":""},{"path":"categorical.html","id":"shot-scale","chapter":"8 Analysing shot data","heading":"Shot scale","text":"use seven shot scales based relative position subject, typically human body: big close-(BCU), close-(CU), medium close-(MCU), medium shot (MS), medium long shot (MLS), long shot (LS) long shot (VLS).","code":""},{"path":"categorical.html","id":"camera-movement","chapter":"8 Analysing shot data","heading":"Camera movement","text":"assign shots one four broader categories camera movement:shots camera move lens rotated (RO) horizontal /vertical planes (pan, tilt, pan--tilt, etc.).mobile shots (MO) camera moves (tracking shots, dolly shots, etc.).hybrid shots (HY) track--pans camera moves lens rotated.static shots (ST), camera movement.","code":""},{"path":"categorical.html","id":"camera-angle","chapter":"8 Analysing shot data","heading":"Camera angle","text":"vertical position camera relative framed material:LOW angle camera placed subject looking .LEVEL, neutral angle looking straight-scene irrespective camera height.HIGH angle camera placed subject looking onto scene.","code":""},{"path":"categorical.html","id":"shot-type","chapter":"8 Analysing shot data","heading":"Shot type","text":"Three shot types included study describe shot related previous shot.Point--view (POV) refers shots framed position one characters viewer able see sees. Shots framed character‚Äôs position scene classed point--view shots (X.POV).Reverse angle (RA) shots photographed opposite direction preceding shot, typically part shot/reverse shot pattern POV shots. Shots meet definition classed reverse-angle shots (X.RA).Finally shots classified either framed along close axis lens (AXIAL) previous shot (X.AXIAL).downloaded spreadsheet, save MCA spreadsheet .csv file name rashomon.csv Data folder project load file R.Table 8.2 presents data use chapter.\nTable 8.2: Shot type data four narrators Rashomon (1950). ‚òù\n","code":"\npacman::p_load(here, tidyverse)\ndf_rashomon <- read_csv(here(\"Data\", \"rashomon.csv\"))"},{"path":"categorical.html","id":"describing-shot-data","chapter":"8 Analysing shot data","heading":"8.3 Describing shot data","text":"section focus describing single variable ‚Äì shot scale. look analyse multivariate data .R categorical variables called factors categories factor called levels. Working within tidyverse suite packages, levels factor conventionally organised alphabetically .csv file loaded R, nominal variables stored character class objects (indicated <chr>). However, shot scale data order close distant based relationship subject camera wish preserve ordering analysis. need make scale factor set levels, using factor() levels() functions, respectively. R order levels factor based sequence set user.can convert character variables factors using dplyr check column data frame type character , , convert factor (indicated <fct>).method efficient order factors order occur column. may ordering want enforcing ordering factors variable individually may better approach.","code":"\n# Confirm that the scale variable is of class character\nclass(df_rashomon$scale)## [1] \"character\"\n# Enforce the ordering of levels of a factor\ndf_rashomon$scale <- factor(df_rashomon$scale, \n                            levels = c(\"BCU\", \"CU\", \"MCU\", \"MS\", \"MLS\", \"LS\", \"VLS\"))\n\n# Confirm that the scale variable is now a factor\nclass(df_rashomon$scale)## [1] \"factor\"\ndf_rashomon <- df_rashomon %>%\n  mutate(across(where(is_character), as_factor))"},{"path":"categorical.html","id":"numerical-summaries-of-categorical-data","chapter":"8 Analysing shot data","heading":"8.3.1 Numerical summaries of categorical data","text":"important concept categorical data analysis frequency, defined number data points (\\(n\\)) category (\\(\\)): (\\(n_{}\\)). relative frequency \\(\\)-th category equal frequency divided total number data points (\\(N\\)) data set, expressed either proportion (\\(p_{}\\)) percentage (\\(100 * p_{}\\)):\\[\np_{} = \\frac{n_{}}{N}\n\\]sum proportions variable equal 1 sum percentages equal 100%.frequency distribution representation data points data set distributed across categories (\\(K\\)) either tabular graphical form.calculate frequency distribution shot scales different narrators Rashomon, can use dplyr::count() grouped data narrator variable. group data narrator, count() return frequency distribution whole data set (.e., whole film). default, count() drop categories observations often useful know categories count zero inhibit behaviour setting .drop = FALSE. also add columns data frame relative frequency shot scales proportion (p) percentage (percent). Table 8.3 presents proportions shot scales narrative.\nTable 8.3: frequency relative frequency shot scales four narrators Rashomon (1950). ‚òù\nsummarise frequency distribution numerically, want describe typical value variability.mode categorical variable category occurs least frequently category. Unlike mean median, R built function calculating mode data set. (R mode() function classifies objects workspace according basic structure related class() function). DescTools package function called Mode() can use (note name function begins capital letter).Note mode data set may unique may one category satisfies criteria ‚Äòoccurring frequently,‚Äô hence definition modal category occurs least frequently another category.Summarising variability categorical variable based relative frequencies categories. numerous methods available, focus three concepts: unalikeability, diversity, evenness.coefficient unalikeability (\\(u_{2}\\)) describes often observations differ one another (Kader & Perry, 2007). different concept variability used describing quantitative variables motion picture shot lengths, describes much observations data set differ. \\(u_{2}\\) represents proportion possible comparisons categories unalike (including comparisons category ), ranges value 0, categories contain equal numbers observations, 1, categories different one another, equal one minus sum squared relative frequencies category:\\[\nu_{2} = 1 - \\sum_{=1}^{K} p_{}^2\n\\]measure much variation within categorical variable need able measure diversity. Shannon entropy (\\(H\\)) equal \\[\nH = -\\sum_{=1}^{K} p_{} \\log_{2} p_{}\n\\]describes uncertainty predicting type randomly selected observation. value \\(H\\) increases increasing diversity. shots film framed way (example, close ups), uncertainty predicting scale randomly selected shot \\(H = 0\\). Shannon entropy sensitive categories containing fewer observations.Simpson‚Äôs reciprocal diversity index (\\(S\\)) sensitive frequently occurring category variable equal reciprocal probability two randomly selected observations data set belong category:\\[\nS = \\frac{1}{\\sum_{=1}^{K} p_{}^{2}}\n\\]increases value increasing diversity, minimum value 1 (.e., shots film framed way \\(S = 1\\)).describe evenness distribution shots across seven categories scale, can calculate statistics derived \\(H\\) \\(S\\) (see Heip et al. (1998) overview). Pielou‚Äôs \\(J\\) ratio observed entropy maximum possible entropy \\(K\\) categories: \\(J = \\frac{H}{log_{2}(K)}\\). Simpson‚Äôs evenness index equal \\(S\\) divided total number categories: \\(SE = \\frac{S}{K}\\). \\(J\\) \\(SE\\) range 0 1, value 0 representing maximally uneven distribution (.e., observations belong single category) value 1 representing perfectly even distribution.calculate entropy use entropy.empirical() entropy package, argument units = \"log2\". set units \\(H\\) bits, units irrelevant ‚Äì matters consistent logarithm choose work . mathematics calculating \\(S\\), \\(J\\), \\(SE\\) simple enough can hand.Table 8.4 presents numerical summary distribution shot scales four selected narrators Rashomon.\nTable 8.4: Numerical summary shot scales different narrators Rashomon.\nTable 8.4 see Kurosawa prefers mid-scale framing four narratives, modal class medium close , shots Wife‚Äôs narrative tend framed closer subject three narratives, frequently occurring shot type medium shot. Wife‚Äôs narrative also shows less variability others, lower unalikeability, diversity, evenness. Woodcutter‚Äôs version events greater variability narrators. learn case later .","code":"\ndf_scale <- df_rashomon %>% \n  select(narrator, scale) %>%\n  group_by(narrator) %>%\n  count(scale, .drop = FALSE) %>%\n  mutate(p = round(n/sum(n), 3),\n         percent = 100 * p)\npacman::p_load(DescTools)\n\ndf_scale_summary <- df_rashomon %>% \n  group_by(narrator) %>%\n  summarise(mode = Mode(scale))\n\ndf_scale_summary## # A tibble: 4 √ó 2\n##   narrator   mode \n##   <chr>      <fct>\n## 1 Bandit     MS   \n## 2 Husband    MS   \n## 3 Wife       MCU  \n## 4 Woodcutter MS\npacman::p_load(entropy)\n\n# Get the number of categories \nK <- (unique(df_scale$scale)) %>% length\n\ndf_scale_summary <- df_scale_summary %>%\n  mutate(\n    # Coefficient of unalikeability\n    df_scale %>% \n      group_by(narrator) %>%\n      mutate(u = round(1 - sum(p^2), 2)) %>%\n      distinct(u),\n    # Entropy\n    df_scale %>% \n      group_by(narrator) %>%\n      mutate(H = round(entropy.empirical(p, unit = \"log2\"), 2)) %>%\n      distinct(H),\n    # Simpson's reciprocal diversity\n    df_scale %>% \n      group_by(narrator) %>%\n      mutate(S = round(1/sum(p^2), 2)) %>%\n      distinct(S)\n  ) %>%\n  mutate(\n    # Pielou's J\n    J = round(H/log2(K), 2),\n    # Simpson's evenness\n    SE = round(S/K, 2)) %>%\n  rename(\"u~2~\" = u)  # Add the subscript to the column name for display"},{"path":"categorical.html","id":"visualising-categorical-data","chapter":"8 Analysing shot data","heading":"8.3.2 Visualising categorical data","text":"illustrate produce simple informative visualisations frequency distribution first look shot scales Bandit‚Äôs narrative compare four narratives side--side.begin, create data frame data Bandit‚Äôs narrative filtering df_scale data frame narrator variable. Figure 8.1 illustrates four ways visualising data bar chart.\nFigure 8.1: Visualising shot scale data Bandit‚Äôs narrative Rashomon. () Barchart showing frequency shot scales. (B) Flipped Barchart shot scale frequencies. (C) Relative frequency shot scales. (D) Relative frequency shot scales text labels.\nFigures 8.1.8.1.B plot frequency shot scales vertically horizontally, latter plot created adding coord_flip() creating plot. plots present information bar charts encode information height/length bars, choice whether use vertical horizontal bars clear cut. Vertical bars can better representing categorical variables order read left--right x-axis, showing relationships sequence. vertical bar chart may, however, harder read. Horizontal bar charts easier read (mimicking left-right direction reading Western cultures), especially space labels categories x-axis may limited.Figure 8.1.C plots proportion (p) shots category using scales::percent() y-axis automatically plotted percent nicely formatted tick labels make easy reader understand shown. result achieved plotting percentages formatting labels using anonymous function, scales package makes type formatting routine. can dispense y-axis completely add data values labels bars using percentage values (Figure 8.1.D), reducing amount ink chart without losing information.compare relative frequency shot scales four narratives can plot data Table 8.2 stacked bar chart (Figure 8.2). makes much easier see shots distributed across different scales. difference style Wife‚Äôs narrative immediately apparent plot, height segment medium shots much smaller narratives. also revealed plot Wife‚Äôs version events also big close-ups close-ups narrators, clearly indicating Kurosawa frames shots Wife‚Äôs narrative closer subject points film. three shot scales account 72.4% shots Wife‚Äôs narrative, explains variability statistics narrative lower. information available frequency distribution presented tabular format hard see pattern among data, obscured entirely numerical summaries Table 8.4.\nFigure 8.2: Stacked barchart relative frequency shot scales different narrators Rashomon. ‚òùÔ∏è\nStacked bar charts may easier see distribution observations variable across set categories within grouping (narrator Rashomon) limitations, especially comes making comparisons across groups. Comparing segments stack shot scale across different narrators difficult segments begin point. Consequently, takes much effort part reader make sense data presented . avoided adding labels plot, can challenging height segment stack small accommodate text result labels strewn across plot removing need bars. comparing data across different narrators might well used Table 8.2, effectively reproduced table adding text plot. Figure 8.2 presented interactive plot, allowing user select individual shot scales comparison clicking label legend precise information shown tooltip hovering mouse segment. interactivity rather visualisation lot work communicating structure data user.Another way compare shot scale data across four narratives employ principle small multiples, plot bar charts narrator separately within grid using scale individual plot. idea small multiple developed Edward Tufte take advantage repetition graphical presentation data:Small multiples economical: viewers understand design one slice, immediate access data slices. Thus eye moves one slice next, constancy design allows viewer focus changes data rather changes grahicl design (Tufte, 1983: 48).Figure 8.3 plots shot scale data grid bar charts using facet_wrap() function ggplot2. bar chart created horizontal bar chart narrator passed variable facet_wrap() (note tilde (~) variable name). tells ggplot2 make bar chart individual narrator, layout specified number columns (rows) passed ncol (nrow). data presented scale facet, making direct comparison different different slices quick easy (though feature can turned appropriate).\nbar charts encode information length bar,\ndifferences numerical values represented differences\nlengths bars.\n\nTruncating y-axis bar chart classic strategy lying\nstatistics.\n\ntruncating range y-axis, differences data \nlonger correspond relative differences length \nbars. can make small differences appear large, leading \nreader misinterpret meaning data. example, two\nplots show data, truncating axis B plot\ncan make value 2022 appear five times larger \n2018, implying 500% time, even though \n3.2% difference actual data values (3.14 3.15,\nrespectively).\n\norder avoid deceiving reader, y-axis bar\nchart begin zero differences representation\ndata correspond actual numerical differences \ndata.\n\nrule necessarily apply types plot, \nbox plots line\ncharts, encode data different ways bar charts.\n\nFigure 8.3: Small multiples plot relative frequency shot scales different narrators Rashomon.\nFigure 8.3 presents data stacked bar chart leads us conclusions. closer framing Wife‚Äôs narrative clear see, tendency medium shots dominate sections film. greater variability Woodcutter‚Äôs narrative comparison narrators evident, shows different version story compared Husband, particular. requires much less effort understand data presented small multiples.\nAlthough visually similar, bar charts histograms two\ndifferent types graph provide different types information\ndifferent types variables. Like bar chart, histogram \nproduced visualising data bars numerical scale\nrepresenting frequency two types charts different use\ncases conceptually quite different.\n\nBar charts visualise count data probability\nmass function discrete qualitative categorical variable (e.g.,\nshot scales), allowing us compare number data points \ndifferent categories variable.\n\nHistograms simple form density estimation allow us \nsee overall distribution continuous quantitative numerical\nvariable (e.g., shot length data) visualising frequency data\npoints sorted ranges values called bins probability\ndensity function.\n\nbar chart, frequency expressed height \nbar width bar meaning. histogram, frequency\nexpressed area bar. means bins \nhistogram need equal width.\n\nbar chart gaps categories represent discrete\nnature variable ‚Äì categories variable discontinuous.\nvariable represented histogram continuous\nneighbouring bars histogram must touch indicate \nvariable continuous: point one bin ends point\nnext bin begins gap bars indicates \nbin contained data points.\n\nBar charts count individual entities, whereas histograms group data\nrange values. Consequently, bar charts display exact values ‚Äì \nknow exactly many data points belong category variable.\nUsing histogram loss precision ‚Äì know many data\npoints lies within range values defined bin \nknow exactly many data points specific values.\n\nconcepts use describe features histogram \nrelevant describing bar chart. example, can speak \nskewness histogram estimates density \ndata, skewness meaning talking bar chart. \ncategories bar chart intrinsic ordering\n(ordinal variables can arranged left--right right--left\nrepresent information); whereas data \nhistogram must arranged order size left--right\nincreasing size smallest--greatest.\n\nimportant know difference bar chart\nhistogram use correctly.\n","code":"\n# Filter the shots in the Bandit's narrative\ndf_bandit_scale <- df_scale %>% \n  filter(narrator == \"Bandit\")\n\n# Bar chart of frequencies\nfrequency_plot <- df_bandit_scale %>% \n  ggplot(aes(x = scale, y = n)) +\n  geom_bar(stat=\"identity\", fill = \"#277F8E\") +\n  scale_x_discrete(name = \"Shot scale\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_minimal()\n\n# Flip the coordinates - note reordering of scale to put BCU at top of the y-axis\nflipped_plot <- df_bandit_scale %>% \n  ggplot(aes(x = reorder(scale, desc(scale)), y = n)) +\n  geom_bar(stat=\"identity\", fill = \"#277F8E\") +\n  scale_x_discrete(name = \"Shot scale\") +\n  scale_y_continuous(name = \"Frequency\") +\n  coord_flip() +\n  theme_minimal()\n\n# Use scales::percent to format the y-axis as percentages automatically\npercent_plot <- df_bandit_scale %>% \n  ggplot(aes(x = scale, y = p)) +\n  geom_bar(stat=\"identity\", fill = \"#277F8E\") +\n  scale_x_discrete(name = \"Shot scale\") +\n  scale_y_continuous(name = \"Relative frequency\", labels = scales::percent) +\n  theme_minimal()\n\n# Add text labels to end of bars\ntext_labels_plot <- df_bandit_scale %>% \n  ggplot(aes(x = scale, y = percent)) +\n  geom_bar(stat=\"identity\", fill = \"#277F8E\") +\n  geom_text(aes(label = paste0(percent, \"%\")), vjust = -0.4, size = 3.2) +\n  scale_x_discrete(name = \"Shot scale\") +\n  scale_y_continuous(limits = c(0, 40), expand = c(0, 0)) +\n  theme_classic() +\n  theme(axis.line = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.y = element_blank(),\n        panel.grid = element_blank())\n\n# Combine into a single figure\npacman::p_load(ggpubr)\nfigure <- ggarrange(frequency_plot, flipped_plot, \n                    percent_plot, text_labels_plot, \n                    nrow = 2, ncol = 2, align = \"hv\", labels = \"AUTO\")\n\n# Call the figure\nfigure\npacman::p_load(plotly, withr)\n\nstacked_barchart <- df_scale %>% \n  ggplot(aes(x = narrator, y = percent, fill = scale,\n             text = paste0(\"Scale: \", scale,\n                           \"\\nRelative freq: \", percent, \"%\"))) +\n  geom_bar(position = \"fill\", stat=\"identity\") +\n  scale_x_discrete(name = \"Narrator\") +\n  scale_y_continuous(name = \"Relative frequency\", \n                     labels = scales::label_percent()) +\n  scale_fill_viridis_d(name = \"Scale\") +\n  guides(fill = guide_legend(nrow = 1, title.position = \"top\")) +\n  theme_minimal()\n\n# Use withr::with_options() to set the number of decimal places for the \n# labels in the interactive plot\nwith_options(list(digits = 1), ggplotly(stacked_barchart, tooltip = \"text\"))\ndf_scale %>% \n  ggplot(aes(x = reorder(scale, desc(scale)), y = p, fill = scale)) +\n  geom_bar(stat=\"identity\") +\n  scale_x_discrete(name = \"Shot scale\") +\n  scale_y_continuous(name = NULL, \n                     labels = scales::label_percent(accuracy = 1)) +\n  scale_fill_viridis_d(name = \"Scale\") +\n  coord_flip() +\n  facet_wrap(~narrator, ncol = 2) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 10, face = \"bold\", hjust = 0),\n        # Prevent overlap of labels from adjacent facets\n        panel.spacing = unit(15, \"pt\"),  \n        plot.margin = margin(0.2, 0.4, 0.2, 0.2, \"cm\"))"},{"path":"categorical.html","id":"temporal-structure","chapter":"8 Analysing shot data","heading":"8.4 Temporal structure","text":"can analyse temporal structure categorical variables film style exploring sequences events time.Markov chain stochastic model comprised set transitions described probability distribution traces sequence possible events \\(X_{1}, X_{2}, X_{3}, ...\\). case shot scales Rashomon, possible events different scales used frame shots temporal sequence: example, \\(LS, MCU, LS, MS, ‚Ä¶\\).Markov chains memoryless, means probability sequence current state determined previous state (\\(Pr(X_{n+1} = x|X_{n} = x_{n})\\)) take account full chain prior states. may realistic assumption analysing film style, may longer patterns shot scales. example, classical Hollywood style opening scene establishing shot moving closer characters though sequence dolly > master > two-shot > > > single > single implies structure degree memory. , however, lack research sequencing shot scales general, certainly none 1950s Japanese cinema particular, tells degree memory might . (research memory shot length data: see Cutting et al. (2018)). Research area likely require development marked point process models film style, topic lies beyond scope book.therefore proceed analysis temporal structure shot scales Rashomon basis describing aspect film style Markov chain can tell us something useful style film , least, learning methods can use answer bigger questions style cinema, including questions temporal sequences cinema.","code":""},{"path":"categorical.html","id":"transition-matrix","chapter":"8 Analysing shot data","heading":"8.4.1 Transition matrix","text":"transition probabilities markov chain stored transition matrix. transition matrix square matrix probability transitioning state \\(\\) state \\(j\\) represented \\((,j)\\)-th entry matrix. Thus, matrix \\(P\\), describes Markov chain shot scale transitions Bandit‚Äôs narrative, entry \\(P_{(CU,MCU)}\\) describes probability cutting close medium close . transition probabilities moving state \\(\\) next state \\(j\\) sum 1.first step producing transition matrix get shot scale data four narrators original df_rashomon data frame created separate data data frame narrator. mean need create four separate data frames. Using dplyr‚Äôs group_by() nest() functions, can collect data narrator sample store inner data frames within single overall data frame. result data frame, df_narrators, two variables: narrator, list narrators sample, data, list containing shot scale data narrator nested data frame df_narrators data frame.access shot scale data narrator need access data variable df_narrators calling index using double square brackets. data variable second variable df_narrators index 2: [[2]]. access data frame within list data frames stored data want use, calling index using double square brackets, extract shot scale data using $ operator interact data.use function createSequenceMatrix() markovchain package calculate transition matrix, looping nested data frames df_narrators. loop produce two outputs narrator: transition probabilities moving one shot scale another, compiled single data frame df_transitions plotting transition matrices small multiples facet plot; list, matrix_transitions, collect matrix transition probabilities narrator use create chord diagrams .created two new variables df_transitions data frame necessary define variables factors re-assert order levels.Figure 8.4 plots shot scale transition matrices four narrators Rashomon.\nFigure 8.4: Shot scale tranisition probabilities four narrators Rashomon. Note columns matrix sum one.\nFigure 8.4 also allows us see Kurosawa terms sequencing shots. example, none narratives include long shots followed big close . Transitions close ups medium close-ups medium-long, long, long shots occur less common transitions closer framed shots (.e., medium shots closer) shots similar scale. , tendency Wife‚Äôs narrative dominated closer frame shots evident transition matrix section film, largest probabilities concentrated North West corner matrix indicating Kurosawa tends cut closer framed shots closer shots narrative.none narratives Kurosawa cut big close anything wider medium shot rarely cuts wide shot big close . Kurosawa thus avoids cutting one extreme range shot scales another. key exception . One stand features transition matrices Husband‚Äôs narrative, wider-framed shots (.e., MLS, LS, VLS) tend followed shots framed medium shots closer rarely followed wider-framed shots, pattern see sections film. narrative Rashomon long shot followed big close . shall see come multivariate analysis film, Husband‚Äôs narrative tends include transitions shots framed along axis lens, moving towards subject distant close framing, whereas narratives depends use reverse-angle cuts POV-shots matched shot scales.narratives Bandit Woodcutter exhibit greater variety transitions shot scales, though due fact narratives many shots ‚Äì , therefore, transitions ‚Äì Wife Husband. Like, transitions matrix Wife‚Äôs narrative, matrix Bandit‚Äôs narrative also shows tendency transitions shots framed medium shots closer, though unlike Wife‚Äôs narrative also exhibits variety transitions wider-framed shots across range shot scales excepting big close-ups.Woodcutter‚Äôs narrative high transition probabilities matrix ‚Äì probability big close followed another big close 0.67, 0.6 probability medium long shot followed medium shot ‚Äì show particular pattern among transition probabilities. original study style narrative Rashomon, argued Woodcutter‚Äôs narrative employed elements common three versions events (Redfern, 2013b: 33). also evident Woodcutter‚Äôs transition matrix. Like Bandit Wife, Woodcutter‚Äôs account features transitions closely-framed shots north-west corner matrix, probabilities tend lower elsewhere. Like matrices Bandit Husband, see variety transitions close-framing wide framing south-west corner, prevalent elsewhere. saw , medium shots modal shot scale Woodcutter‚Äôs narrative, unlike Bandit Husband, transitions medium shots evenly distributed across shot scales. Woodcutter‚Äôs narrative thus exhibits stylistic similarities three sections, . reflects status narrative film Woodcutter recounts events narrators position participant events forest, external observer.","code":"\ndf_narrators <- df_rashomon %>%\n  select(narrator, scale) %>%\n  group_by(narrator) %>%\n  nest()\n\ndf_narrators## # A tibble: 4 √ó 2\n## # Groups:   narrator [4]\n##   narrator   data              \n##   <chr>      <list>            \n## 1 Bandit     <tibble [139 √ó 1]>\n## 2 Wife       <tibble [29 √ó 1]> \n## 3 Husband    <tibble [26 √ó 1]> \n## 4 Woodcutter <tibble [65 √ó 1]>\n# Get the shot scale data for the Husband's narrative:\n# 2 is the index of the list of data frames in df_narrators\n# 3 is the index of the data frame containing the shot scale data for the Husband's narrative\n# scale is the variable containing the shot scale data\nhead(df_narrators[[2]][[3]]$scale)## [1] VLS CU  MCU MS  MLS CU \n## Levels: BCU CU MCU MS MLS LS VLS\n# Load the markovchain package\npacman::p_load(markovchain)\n\n# Initialise data structures to store the results of the loop\ndf_transitions <- data.frame()\nmatrix_transitions <- list()\n\nfor (i in 1:length(df_narrators[[1]])){\n  \n  narrator <- df_narrators[[1]][i]\n  dat <- factor(df_narrators[[2]][[i]]$scale)\n  \n  # Calculate the transition matrix for the sequence of shot scales\n  dat_sequence_matrix = createSequenceMatrix(dat, toRowProbs = TRUE)\n  # Add the matrix to the list of matrices\n  matrix_transitions[[i]] <- dat_sequence_matrix\n  \n  # Convert matrix to data frame and rearrange for plotting\n  df_sequence_matrix <- dat_sequence_matrix %>%\n    data.frame %>% \n    rownames_to_column() %>%\n    gather(colname, value, -rowname) %>%\n    rename(From = rowname, To = colname, p = value) %>%\n    mutate(narrator = rep(narrator, length(From))) %>%\n    relocate(narrator) %>%\n    arrange(From)\n  \n  df_transitions <- rbind(df_transitions, df_sequence_matrix)\n  \n}\n# We need to reassert factor levels for newly created variables\ndf_transitions$From <- factor(df_transitions$From, \n                              levels = c(\"BCU\", \"CU\", \"MCU\", \"MS\", \"MLS\", \"LS\", \"VLS\"))\n\ndf_transitions$To <- factor(df_transitions$To, \n                              levels = c(\"BCU\", \"CU\", \"MCU\", \"MS\", \"MLS\", \"LS\", \"VLS\"))\nggplot(data = df_transitions) +\n  geom_tile(aes(x = From, y = To, fill = p)) +\n  geom_text(aes(x = From, y = To, label = round(p, 2)), colour = \"white\", size = 3) +\n  scale_y_discrete(limits = rev) +  # Reverse the direction of the y-axis\n  scale_fill_viridis_c(name = NULL) +\n  facet_wrap(~narrator, ncol = 2) +\n  coord_fixed() +  # Render the square matrix as a square plot\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 12, face = \"bold\", hjust = 0))"},{"path":"categorical.html","id":"chorddiagrams","chapter":"8 Analysing shot data","heading":"8.4.2 Chord diagrams","text":"can visualise data stored transition matrices chord diagrams using circlize package. Chord diagrams represent weight connections categories variable set chords across circle, categories arranged order around circumference. size chord proportional weight connections, shot scale data determined probability transitioning one shot scale another.circlize uses R -built graphics package rather ggplot2, arrange four chord diagrams shot scale data need tell R want draw \\(2 \\times 2\\) matrix plots fill rows setting parameter using par(mfrow = c(2, 2)), fill plot matrix left right across row moving next row . Alternatively, set par(mfcol = c(2, 2)), also create \\(2 \\times 2\\) matrix plots, fill columns, moving top--bottom column moving next column right. add title plot matrix use margin text function mtext(), allows us control placing formatting text. add text top margin plot need set side = 3.\nFigure 8.5: Chord diagrams shot scale transitions four narrators Rashomon.\nFigure 8.5 presents us information transition matrices aesthetically pleasing way. chord diagrams obscure small transition probabilities resulting thin arcs, allow us quickly pick patterns glance (supported use small multiples) rather becoming focused detailed numerical information transition matrices may lead us overlook general picture. example, immediately apparent narratives Bandit Wife feature heavily-weighted transitions big close ups close ups medium shots, Husband‚Äôs narrative features heavily-weighted transitions big close ups medium shots close ups. chord diagram Woodcutter‚Äôs narrative shows neither properties, heavily-weighted transitions big close ups big close ups long shots.","code":"\n# Load the circlize package. Also load viridis for colouring the plot\npacman::p_load(circlize, viridis)\n\n# Get the number of colours required for the palette\nk <- max(dim(dat_sequence_matrix))\n\n# Set the grid for plotting two rows and two columns of plots\npar(mfrow = c(2, 2))\n\n# The Bandit's narrative\nchordDiagram(matrix_transitions[[1]], grid.col = viridis(k),  \n             annotationTrack = c(\"name\", \"grid\"), annotationTrackHeight = c(0.03, 0.05))\nmtext(df_narrators$narrator[1], side = 3, adj = 0, line = 0, cex = 1.1, font = 2)\n\n# The Wife's narrative\nchordDiagram(matrix_transitions[[2]], grid.col = viridis(k),  \n             annotationTrack = c(\"name\", \"grid\"), annotationTrackHeight = c(0.03, 0.05))\nmtext(df_narrators$narrator[2], side = 3, adj = 0, line = 0, cex = 1.1, font = 2)\n\n# The Husbands's narrative\nchordDiagram(matrix_transitions[[3]], grid.col = viridis(k),  \n             annotationTrack = c(\"name\", \"grid\"), annotationTrackHeight = c(0.03, 0.05))\nmtext(df_narrators$narrator[3], side = 3, adj = 0, line = 0, cex = 1.1, font = 2)\n\n# The Woodcutter's narrative\nchordDiagram(matrix_transitions[[4]], grid.col = viridis(k),  \n             annotationTrack = c(\"name\", \"grid\"), annotationTrackHeight = c(0.03, 0.05))\nmtext(df_narrators$narrator[4], side = 3, adj = 0, line = 0, cex = 1.1, font = 2)"},{"path":"categorical.html","id":"multivariate","chapter":"8 Analysing shot data","heading":"8.5 Multivariate analysis","text":"far explored single categorical variable film style isolation. However, shot film requires filmmakers make multiple decisions framed relates shots. understanding different elements film‚Äôs style organized produce formal structure requires analysing several variables simultaneously.requires us shift univariate statistical methods multivariate statistical methods:Multivariate statistical analysis simultaneous statistical analysis collection variables, improves upon separate univariate analysis variable using information relationships variables. Analysis variable likely miss uncovering key features , interesting ‚Äòpatterns‚Äô , multivariate data (Everitt & Hothorn, 2011: 2, original emphasis).Rashomon data set includes six categorical variables, three describing behaviour camera (shot scale, camera movement, camera angle) three encoding relationships shots (POV, reverse angle, axial). explore relationships variables use mutliple correspondence analysis (MCA) multivariate approach geometric data analysis nominal categorical data.MCA descriptive method revealing patterns complex data sets locating observed individuals variables low-dimensional Euclidean space. Interpretation representation variables MCA based geometry points space (see Abdi & Valentin, 2007):two individuals close one another tend levels nominal variables. Rashomon data set, observed individuals shots film shots lie close one another map produced MCA tend stylistically similar.proximity categories different variables means levels tend occur together. example, proximity X.RA category reverse angle variable AXIAL category axial variable indicates shots related cutting along axis lens tend related reverse angle cuts, exactly expect. Categories lie opposite sides axis contrasted one another, along axis point lies, greater contribution determination axis.proximity different categories variable indicates associated groups observations similar, individuals stylistically similar one another nominal categories exclusive (.e., shot either close ).treat narrator supplementary variable. means variable used calculating results MCA contribute definition axes distribution individual shots variables. narrator associated categories projected onto low-dimensional space defined active variables can see narrator relates various stylistic variables data set.MCA performed using indicator matrix derived df_mca data frame create . column matrix represents one category variable row matrix takes binary value depending whether individual (1) (0) belong category. necessary interact indicator matrix useful understand data represented different stages MCA data set.several R packages geometric data analysis. use FactoMineR package, range tools performing different forms geometric data analysis, including MCA via MCA() function. also use GDAtools package aid visualisation data. use factoextra (Kassambara & Mundt, 2020) package produce plots derived results applying multiple correspondence analysis Rashomon data. , want understand FactoMineR returns results can navigate different components results, create summaries visualisations scratch.object returned FactoMineR::MCA() dense contains lot information. can get summary main features interested using FactoMineR::summary.MCA(). default, function returns first results part output MCA(). summarise results variables categories data set (nbelements = Inf ‚Äì note simply return variables categories rather infinite result), suppress printing results every shot data set setting number individuals (.e., shots) zero (nbind = 0). allows us focus variables. also limit results first two dimensions (ncp = 2).summary output presents us lot information:Call: call made MCA() reference .Eigenvalues: information describes variance associated dimension absolute value percentage total variance analysis. Even though requested results first two dimensions results dimensions always displayed section summary. FactoMineR performs multiple correspondence analysis using indicator matrix apply correction eigenvalues amount variance associated dimension terribly informative (R packages geometric data analysis apply correction).Categories: category data set get coordinate category dimension (Dim.1 Dim.2) output; contribution (ctr) category total variance dimension percentage; quality representation (cos2) category dimension, better represented categories higher values; standardised score Normal distribution test determine mean category different overall mean (v.test), sign indicates value less greater overall mean distance overall mean increasing absolute value statistic.Categorical variables (eta2): square correlation variable dimension, use construct variables map .Supplementary categories: coordinate, quality, difference overall categories used calculate results analysis.Supplementary categorical variables (eta2): square correlation supplementary variable dimension.easier explore information visually, create plots see instantly categories contribute variance dimension related one another.can access indicator matrix via res_mca$call$Xtot. Comparing head indicator matrix head df_mca data frame see encode information different ways. first row df_mca data frame records shot static (ST) long shot (LS) high angle (HIGH) POV shot (X.POV), reverse angle cut (X.RA) axial cut (X.AXIAL). indicator matrix presents information value 1 categories 0 elsewhere. also see categories narrator variable moved right-hand side matrix narrator supplementary variable.use res_mca$var$eta2 access squared correlation variables dimensions active variables, var short variable. get data narrator variable use res_mca$quali.sup$eta2, quali.sup short qualitative supplementary variable.Figure 8.6 plots variables map. clear distinction variables associated Dimension 1 (axial, pov, ra) associated Dimension 2 (movement, scale). represents distinction way shots combined present different perspectives narrators way information presented viewer use framing camera movement. angle contributes axes perspectival presentational aspects: example, low camera presents visual information viewer may also may represent perspective character. narrator lies close Dimension 1 indicates difference four narratives data set matter perspective rather presentation.\nFigure 8.6: Variables map film style four narratives Rashomon. narrator supplmentary variable.\nFigure 8.7 plots contributions variable categories first two dimensions analysis. reference line \\(100\\% \\times (1/K)\\), \\(K\\) number number active categories (.e., categories supplementary variables included). data set, \\(K = 20\\) reference line 5%, categories contributions reference point making significant contribution overall variance dimension. data ctr column summary . results confirm interpretation derived variables map persepctival features contribute Dimension 1 presentational features contribute Dimension 2, especially categories camera movement. category contribution reference line dimensions LOW, tells us camera angle accounts dual perspectival-presentational role camera angles neither categories variable contributing significantly either dimension.\nFigure 8.7: Percentage contributions variable categories first two dimensions multiple correspondence analysis four narratives Rashomon.\naccess coordinates categories active variables using res_mca$var$coord coordinates supplementary variable using res_mca$quali.sup$coord. Figure 8.8 plots categories let us see relationships different formal decisions filmmakers. Dimension 1 shows contrast different ways shots related. POV RA close one another indicating shots types tend related one another. expected point--view shots typically framed reverse angle preceding shot. Shots related cut along lens axis (X.AXIAL) also closely associated types shots; axial cut reverse angle cut. Shots related axial cuts (AXIAL) relative preceding shot associated non-POV (X.POV) non-RA (X.RA) shots. Dimension 1 contrasts different ways perspective created Rashomon.\nFigure 8.8: Categories map different elements film style four narratives Rashomon. narrators included supplmentary variables.\nnarrators orientated along Dimension 1 according perspective created viewer. terms film form, Bandit‚Äôs narration similar Wife dissimilar narration Husband. narratives told Bandit Wife feature larger proportion POV shots (27% 38%, respectively) make extensive use RA cuts (55% 52%), whereas shots framed along lens axis relative previous shot account 8% 10%, respectively. contrast, Husband‚Äôs narration uses fewer POV shots (12%) RA shots (23%), AXIAL shots occur much frequently (35%). can see categories map points representing Bandit Wife lie direction POV RA shot relationships, Husband lies opposite side origin direction AXIAL cuts. version events told Woodcutter falls extremes. final narrative, POV (23%) shots RA shots (37%) frequent Husband‚Äôs occur less frequently Wife Bandit (albeit slightly less case latter). time, AXIAL cuts (18%) account shots tales Bandit Wife smaller proportion shots Husband‚Äôs tale.Wife lies slightly Dimension 1 shows vertical displacement indicating association (albeit limited) Dimension 2 evident narrators lie closer shot scale categories MCU CU. reinforces conclusion arrived Wife‚Äôs narrative tends use medium close ups close ups rather medium shots.second dimension Figure 8.8 contrasts shot scale movement. particular relationships presentational variables different shot types described indicating Kurosawa varies framing shot movement camera creating character‚Äôs perspective rather relying subset stylistic choices narrator. Shots featuring hybrid camera movements (HY) follow general pattern movement account handful shots film contribute little variance data.Low angle shots tend associated point--view shots, shots neutral camera angle (LEVEL) tend associated shots represent characters point view. However, categories angle variable associated particular axis lie along diagonal line across centre categories map. aspect film style contribute perspectives different narratives also functions presentational device lies two perspectival presentational dimensions.can also look relationships variables plotting data points shot. access coordinates individual shot active variables use res$ind$coord, ind short ‚Äòindividuals‚Äô. First, look distributions style variables, creating plot six active variables. use GDFAtools package plot mean category variable (represented label category) along 95% confidence ellipse ggadd_ellipse() function.Figure 8.9 plots distributions shots data set coded according scale, movement, angle. association scale movement Dimension 2 clear vertical arrangement mean values categories variables, shots category , part, distributed evenly across Dimension 1. diagonal trend angle associates variable dimensions apparent. can also see detail relationships shots: static shots tend closely framed, mobile rotational camera movements tend framed medium shots distant.\nFigure 8.9: distribution shots Rashomon across three formal variables. ‚òùÔ∏è\nFigure 8.10 plots three perspectival variables shows strongly associated Dimension 1. pov shots contrasted axial shots, clear split shots linked reverse-angle cuts . clear point--view shots associated reverse-angle cuts, Kurosawa makes extensive use reverse-angle cuts situations well. three variables, shots distributed evenly along Dimension 2, indicating perspectival stylistic features Rashomon associated full range shot scales camera movements limited particular subset presentational devices. tendency shots linked axial cuts associated LEVEL camera angle, though expected shots related subject matter.\nFigure 8.10: distribution shots Rashomon across three perspectival variables. ‚òùÔ∏è\nFigure 8.11 plots distribution shots encoded narrator. plot confirms conclusion narratives Bandit Wife tend associated POV RA shots, Husband tends associated AXIAL cuts. can also see narrative uses wide range different types shots despite tendencies, diverse categories map suggests. map also reinforces Woodcutter‚Äôs narrative lies narratives Bandit Wife narrative Husband tendency towards particular perspectival formal attributes, shots narrative distributed across map.\nFigure 8.11: distribution shots Rashomon across different narrators.\nresults applying multiple correspondence analysis style different narratives Rashomon reveal different ways perspectives narrators created.perspectives Bandit Wife created use point--view shots reverse-angle cuts, providing direct access narrator‚Äôs perspective viewer sees see , time, given access imagine others see . example, Wife‚Äôs narrative includes exchange nine point--view shots husband: shots Husband Wife‚Äôs position show viewer saw (contempt Husband‚Äôs face); shots framed Husband‚Äôs position show us Wife imagines seen pleads understanding. Similar exchanges occur early Bandit‚Äôs narrative first sets eyes Wife later three-way exchange POV shots rape Bandit Husband prepare duel. use POV shots establishes Bandit Wife active narrators, merely recounting events past aligning viewer physically psychologically perspective.Husband‚Äôs narrative relative scarcity shots stands . Husband passive narrator forced watch events unable affect , using AXIAL cuts place POV RA shots Kurosawa shows us events happening Husband response without admitting viewer direct access perspective. see Husband conceives others sees eyes.witness events forest, Woodcutter source first-hand account available Priest Commoner (, extension, viewer). version events presented apparently objective account third-person narrator standing outside events able describe actions observes complexity without prejudice self-serving perspectives. content story tells combines material prior three versions (duel, husband‚Äôs rejection wife, wife‚Äôs flight, etc.), thereby corroborating part without definitively ruling one version reliable others. time account employs elements narration three versions, combining active components testimonies Bandit Wife passivity Husband‚Äôs narration. POV shots used way narration Bandit Wife, cutting Bandit Husband stalk one another prior duel, fraught exchange glances Wife Husband; axial cuts used show character‚Äôs looking looked refusing glance-object structure POV shot, even repeating set-ups Husband‚Äôs section. One key difference earlier narratives axial cuts now associated Bandit well Husband, attributing Bandit passive demeanour, especially beginning sword fight lacks virility narrative.computational approach applied resolve ambiguities heart Rashomon; examining simultaneously various aspects film style across hundreds shots begin understand fundamental role played film style creating ambiguities.","code":"\n# Load the packages for multiple correspondence analysis\npacman::p_load(FactoMineR)\n\n# Remove the first column of the data frame that lists the shot number\ndf_mca <- df_rashomon %>% select(-shot)\n\n# Calculate the results of the multiple correspondence analysis with narrator \n# set as a supplementary qualitative variable by index (1)\nres_mca <- MCA(df_mca, quali.sup = 1, graph = FALSE)\nsummary.MCA(res_mca, nbelements = Inf, nbind = 0, ncp = 2)## \n## Call:\n## MCA(X = df_mca, quali.sup = 1, graph = FALSE) \n## \n## \n## Eigenvalues\n##                        Dim.1   Dim.2   Dim.3   Dim.4   Dim.5   Dim.6   Dim.7\n## Variance               0.297   0.215   0.207   0.192   0.187   0.171   0.166\n## % of var.             12.727   9.211   8.861   8.241   8.005   7.320   7.135\n## Cumulative % of var.  12.727  21.939  30.800  39.041  47.046  54.366  61.501\n##                        Dim.8   Dim.9  Dim.10  Dim.11  Dim.12  Dim.13  Dim.14\n## Variance               0.161   0.155   0.138   0.134   0.123   0.113   0.074\n## % of var.              6.915   6.641   5.902   5.735   5.284   4.837   3.185\n## Cumulative % of var.  68.416  75.057  80.959  86.694  91.978  96.815 100.000\n## \n## Categories\n##                Dim.1     ctr    cos2  v.test     Dim.2     ctr    cos2  v.test\n## BCU        |  -0.126   0.028   0.001  -0.362 |  -1.514   5.491   0.073  -4.342\n## CU         |  -0.088   0.073   0.002  -0.637 |  -0.781   8.026   0.125  -5.672\n## MCU        |  -0.375   1.860   0.043  -3.344 |  -0.602   6.611   0.112  -5.364\n## MS         |  -0.018   0.006   0.000  -0.192 |   0.269   1.758   0.033   2.918\n## MLS        |   0.484   0.558   0.010   1.637 |   0.877   2.532   0.034   2.966\n## LS         |   0.148   0.157   0.003   0.911 |   0.893   7.884   0.117   5.483\n## VLS        |   0.903   3.711   0.072   4.309 |   1.058   7.041   0.099   5.049\n## HY         |   1.058   1.455   0.027   2.617 |  -0.641   0.738   0.010  -1.585\n## MO         |  -0.016   0.002   0.000  -0.098 |   1.502  22.963   0.341   9.378\n## RO         |   0.085   0.065   0.001   0.599 |   0.937  11.050   0.170   6.624\n## ST         |  -0.053   0.107   0.006  -1.249 |  -0.489  12.683   0.517 -11.545\n## HIGH       |  -0.123   0.210   0.005  -1.134 |  -0.076   0.110   0.002  -0.696\n## LEVEL      |   0.385   4.112   0.145   6.114 |  -0.286   3.137   0.080  -4.543\n## LOW        |  -0.618   5.544   0.133  -5.863 |   0.619   7.682   0.134   5.872\n## POV        |  -1.226  21.498   0.514 -11.516 |   0.104   0.215   0.004   0.979\n## X.POV      |   0.419   7.352   0.514  11.516 |  -0.036   0.073   0.004  -0.979\n## RA         |  -0.831  18.254   0.615 -12.595 |  -0.102   0.383   0.009  -1.552\n## X.RA       |   0.740  16.256   0.615  12.595 |   0.091   0.341   0.009   1.552\n## AXIAL      |   1.462  16.218   0.334   9.285 |  -0.325   1.109   0.017  -2.065\n## X.AXIAL    |  -0.228   2.534   0.334  -9.285 |   0.051   0.173   0.017   2.065\n##             \n## BCU        |\n## CU         |\n## MCU        |\n## MS         |\n## MLS        |\n## LS         |\n## VLS        |\n## HY         |\n## MO         |\n## RO         |\n## ST         |\n## HIGH       |\n## LEVEL      |\n## LOW        |\n## POV        |\n## X.POV      |\n## RA         |\n## X.RA       |\n## AXIAL      |\n## X.AXIAL    |\n## \n## Categorical variables (eta2)\n##              Dim.1 Dim.2  \n## scale      | 0.114 0.507 |\n## movement   | 0.029 0.612 |\n## angle      | 0.176 0.141 |\n## pov        | 0.514 0.004 |\n## ra         | 0.615 0.009 |\n## axial      | 0.334 0.017 |\n## \n## Supplementary categories\n##               Dim.1   cos2 v.test    Dim.2   cos2 v.test  \n## Bandit     | -0.187  0.040 -3.229 |  0.033  0.001  0.570 |\n## Husband    |  0.684  0.052  3.672 | -0.087  0.001 -0.468 |\n## Wife       | -0.271  0.009 -1.545 | -0.246  0.008 -1.402 |\n## Woodcutter |  0.247  0.020  2.292 |  0.074  0.002  0.688 |\n## \n## Supplementary categorical variables (eta2)\n##              Dim.1 Dim.2  \n## narrator   | 0.089 0.009 |\nhead(df_mca, 3)## # A tibble: 3 √ó 7\n##   narrator scale movement angle pov   ra    axial  \n##   <chr>    <fct> <chr>    <chr> <chr> <chr> <chr>  \n## 1 Bandit   LS    ST       HIGH  X.POV X.RA  X.AXIAL\n## 2 Bandit   MCU   ST       LEVEL X.POV X.RA  AXIAL  \n## 3 Bandit   LS    RO       LOW   X.POV X.RA  X.AXIAL\nhead(res_mca$call$Xtot, 3)##   BCU CU MCU MS MLS LS VLS HY MO RO ST HIGH LEVEL LOW POV X.POV RA X.RA AXIAL\n## 1   0  0   0  0   0  1   0  0  0  0  1    1     0   0   0     1  0    1     0\n## 2   0  0   1  0   0  0   0  0  0  0  1    0     1   0   0     1  0    1     1\n## 3   0  0   0  0   0  1   0  0  0  1  0    0     0   1   0     1  0    1     0\n##   X.AXIAL Bandit Husband Wife Woodcutter\n## 1       1      1       0    0          0\n## 2       0      1       0    0          0\n## 3       1      1       0    0          0\n# Load the ggrepel package for better labels\npacman::p_load(ggrepel)\n\n# Gather eta2 details from res_mca and format data frames \n# for the variables and supplementary variables\ndf_var_eta2 <- data.frame(res_mca$var$eta2) %>%\n  select(Dim.1, Dim.2) %>%\n  rownames_to_column(., var=\"variable\")\n\ndf_narrators_eta2 <- data.frame(res_mca$quali.sup$eta2) %>%\n  select(Dim.1, Dim.2) %>%\n  rownames_to_column(., var=\"variable\")\n\n# Combine the two data frames\ndf_eta2 <- rbind.data.frame(df_var_eta2, df_narrators_eta2)\n\nggplot(data = df_eta2, aes(x = Dim.1, y = Dim.2)) +\n  geom_point(aes(colour = variable)) +\n  geom_text_repel(aes(label = variable, colour = variable)) +\n  scale_x_continuous(name = \"Dimension 1 (12.7)%\") +\n  scale_y_continuous(name = \"Dimension 2 (9.2%)\") +\n  scale_colour_viridis_d() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n# Plot the contribution of each category to a dimension\ncat_contrib <- data.frame(res_mca$var$contrib) %>% \n  select(Dim.1, Dim.2) %>% \n  # Turn the row names of the matrix into a variable in the data frame\n  rownames_to_column(., var=\"category\")\n\ndim_1_plot <- ggplot(data = cat_contrib, \n                     aes(x = reorder(category, desc(Dim.1)), y = Dim.1)) +\n  geom_bar(stat = 'identity', fill = \"#277F8E\") + \n  geom_hline(aes(yintercept = 5), colour = \"#440154\", linetype = \"dashed\") +\n  scale_x_discrete(name = NULL) +\n  scale_y_continuous(name = \"Contributions (%)\") +\n  ggtitle(\"Contributions of variable categories to Dimension 1\") + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\ndim_2_plot <- ggplot(data = cat_contrib, \n                     aes(x = reorder(category, desc(Dim.2)), y = Dim.2)) +\n  geom_bar(stat = 'identity', fill = \"#277F8E\") + \n  geom_hline(aes(yintercept = 5), colour = \"#440154\", linetype = \"dashed\") +\n  scale_x_discrete(name = NULL) +\n  scale_y_continuous(name = \"Contributions (%)\") +\n  ggtitle(\"Contributions of variable categories to Dimension 2\") + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n# Combine the plots into a single figure\ndim_plot <- ggarrange(dim_1_plot, dim_2_plot, nrow = 2, align = \"v\")\n\ndim_plot\ndf_categories_coords <- data.frame(res_mca$var$coord) %>%\n  select(Dim.1, Dim.2) %>% \n  rownames_to_column(., var=\"category\") %>%\n  # Identify each category by its variable\n  mutate(variable = c(rep(\"shot scale\", 7), \n                      rep(\"camera movement\", 4), \n                      rep(\"camera angle\", 3),\n                      rep(\"POV\", 2),rep(\"RA\", 2), \n                      rep(\"Axial\", 2)))\n  \ndf_narrators_coords <- data.frame(res_mca$quali.sup$coord) %>%\n  select(Dim.1, Dim.2) %>% \n  rownames_to_column(., var=\"category\") %>%\n  mutate(variable = rep(\"narrator\", length(category)))\n\n# Combine the data frames together\ndf_categories_plot <- rbind.data.frame(df_categories_coords, \n                                       df_narrators_coords)\n\nggplot(data = df_categories_plot, aes(x = Dim.1, y = Dim.2)) +\n  geom_point(aes(colour = variable)) +\n  geom_text_repel(aes(label = category, colour = variable)) +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n  geom_vline(aes(xintercept = 0), linetype = \"dashed\") +\n  scale_x_continuous(name = \"Dimension 1 (12.7%)\", limits = c(-1.5, 1.5), \n                     breaks = seq(-1.5, 1.5, 0.5)) +\n  scale_y_continuous(name = \"Dimension 2 (9.2%)\", limits = c(-1.6, 1.6), \n                     breaks = seq(-1.5, 1.5, 0.5)) +\n  scale_colour_viridis_d() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n# Get the coordinates for the individual shots for dimensions 1 and 2\ndf_individuals <- data.frame(res_mca$ind$coord) %>% select(1:2)\n\n# Add the coordinates for each shot to the df_mca data frame\ndf_mca <- cbind.data.frame(df_mca, df_individuals) \n\n# Drop the narrator column because we will deal with supplementary variables later\ndf_mca2 <- df_mca %>% select(-narrator)\n\n# Load the GDAtools package\npacman::p_load(GDAtools)\n\n# Create an empty plot list to store the output of the loop\nmca_plot_list <- list()\n\n# Loop over each variable and plot the individuals along with\n# the mean of each category of each variable with a 95% confidence ellipse\nfor (i in 1:6){\n  \n  df_temp <- df_mca2 %>% select(c(i, 7:8)) \n  title <- colnames(df_temp)[1]\n  df_temp <- df_temp %>% rename(variable = 1)\n  \n  p <- ggplot(data = df_temp) +\n    geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n    geom_vline(aes(xintercept = 0), linetype = \"dashed\") +\n    geom_point(aes(x = Dim.1, y = Dim.2, colour = variable)) + \n    scale_x_continuous(name = NULL) +\n    scale_y_continuous(name = NULL) +\n    ggtitle(title) +\n    scale_colour_viridis_d() +\n    theme_minimal() +\n    theme(aspect.ratio = 1/1)\n  \n  # Add the ellipses to the plot using GDAtools::ggadd_ellipses() - \n  # make sure to add the variable as a factor\n  pp <- ggadd_ellipses(p, res_mca, as.factor(df_temp$variable), \n                       label = TRUE, legend = \"none\")\n  \n  # Add the plot to the list\n  mca_plot_list[[i]] <- pp\n  \n}\npacman::p_load(ggpubr)\npresentational_figure <- ggarrange(mca_plot_list[[1]], \n                                   mca_plot_list[[2]], \n                                   mca_plot_list[[3]], \n                                   ncol = 3, align = \"h\")\n\nannotate_figure(presentational_figure, \n                bottom = text_grob(\"Dimension 1 (12.7%)\"), \n                left = text_grob(\"Dimension 2 (9.2%)\", rot = 90))\nperspectival_figure <- ggarrange(mca_plot_list[[4]], \n                                 mca_plot_list[[5]],\n                                 mca_plot_list[[6]], \n                                 ncol = 3, align = \"h\")\n\nannotate_figure(perspectival_figure, \n                bottom = text_grob(\"Dimension 1 (12.7%)\"), \n                left = text_grob(\"Dimension 2 (9.2%)\", rot = 90))\n# We added the coordinates of the individual shots to the df_mca data frame \n# earlier so we can use that for plotting the points for the narrators\nnarrators_mca_plot <- ggplot(data = df_mca) +\n    geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n    geom_vline(aes(xintercept = 0), linetype = \"dashed\") +\n    geom_point(aes(x = Dim.1, y = Dim.2, colour = narrator)) + \n    scale_x_continuous(name = \"Dimension 1 (12.7%)\") +\n    scale_y_continuous(name = \"Dimension 2 (9.2%)\") +\n    scale_colour_viridis_d() +\n    theme_minimal() +\n    theme(aspect.ratio = 1/1)\n\nnarrators_mca_plot <- ggadd_ellipses(narrators_mca_plot, res_mca, \n                                     as.factor(df_mca$narrator), \n                                     label = TRUE, legend = \"none\")\n\nnarrators_mca_plot"},{"path":"categorical.html","id":"summary-6","chapter":"8 Analysing shot data","heading":"8.6 Summary","text":"Choosing appropriate method data analysis means knowing difference different types variables ‚Äì qualitative/quantitative, discrete/continuous, categorical/numerical. Discrete qualitative categorical variables film style include shot scale, camera movement, shot transitions, way choose label shot. Analysing variables requires different set methods continuous quantitative numerical variables (shot length data).","code":""},{"path":"texts.html","id":"texts","chapter":"9 Analysing lexical texts","heading":"9 Analysing lexical texts","text":"Pity poor screenwriter, poet. use metaphor simile, assonance alliteration, rhythm rhyme, synecdoche metonymy, hyperbole meiosis, grand tropes. Instead, work must contain substance literature literary. literary work finished complete within . screenplay waits camera. literature, screenwriter‚Äôs ambition? describe way reader turns pages, film flows imagination.  Robert McKee (1997: 395)film film singly; exists centre textual web includes production materials (screenplay, shooting script, costumes costume designs, props prop designs, lighting designs, etc.), paratexts (trailers, posters, reviews, merchandise, social media, fan-produced texts, etc.) (Gray, 2010). Many texts connected film lexical texts; , texts related derived motion picture made words can analysed stylometrically.Computational analyses cinema‚Äôs lexical texts major part new phase technological development cinema, data-driven methods using machine learning natural language processing (NLP) models applied assessing screenplays prior production predict success movie based script.Cinelytic founded Los Angeles 2013. company makes predictions future success movie pre-production phase using talent analytics, combines information actors‚Äô popularity, box office, reviews, age, social media interactions, film analytics, considers likely box office performance based film‚Äôs genre compares performance different distributors. outputs process intended support producers making decisions films greenlight production, thereby minimising risk investing product.\nFigure 9.1: Cinelytic dashboard. ‚òùÔ∏è\nBelgian company ScriptBook, founded 2015, analyses 400 different parameters, identifying genre predict MPAA rating script; conduct sentiment analysis determine emotions scene; estimate degree audiences find characters likeable; assess gender equality screenplay (including Bechdel test), looking percentage male female characters, percentages screen presence spoken lines characters, distribution gender interactions. ScriptBook system also identify target audiences film based script, segmenting potential audience gender age; make range forecasts likely success film predicting budget, box office gross, return investment, make recommendation whether film based script greenlit.\nFigure 9.2: ScriptBook dashboard. ‚òùÔ∏è\nLargo.AI, based Switzerland founded 2018, analyses content screenplays detect genre, predict audience reactions, propose talent film pre-production phase. Largo.AI can also used enhance film different stages production analysing texts, videos, audio creating ‚Äòfilm recipe‚Äô, proposing changes film maximise profitability. Largo.AI‚Äôs analysis directed towards identifying target audience film changes proposes screenplay directed towards end. audience may vary country country Largo.AI localises proposed changes vary single film according differences genre popularity countries.\nFigure 9.3: ScriptBook dashboard. ‚òùÔ∏è\nstylometric studies form basis new filmmaking technologies Cinelytic, ScriptBook, Largo.AI use wide range methods explore different aspects different types lexical texts connected motion pictures. (discussion practicalities using different types lexical texts see ).Murtagh et al. (2009) Murtagh et al. (2010) explored structure screenplays individual films television programmes using correspondence analysis, hierarchical clustering, tag clouds identify linear hierarchical relationships scenes sequences, demonstrating significant degree formal continuity film television screenplays.Schmidt (2015) used subtitles identify plot arcs different types films television episodes, combining sentiment analysis principal components analysis plot structures motion pictures sorted according types identified using topic modelling. Del Vecchio et al. (2020) analysed subtitles 6000 films, applying sentiment analysis cluster analysis group films emotional arcs identifying six basic narrative shapes, comparing structure films box office performance determine types narratives popular audiences.Analysis transcriptions dialogue films television programmes shown screenwriters tend adapt style demands genre (Ho≈Çobut et al., 2016), though network analysis dialogue revels strong authorial signals within genres cinematic universes (Ho≈Çobut & Rybicki, 2020). Ho≈Çobut & Rybicki (2020) applied sentiment analysis track changes sentiments time, finding proportions positive negative sentiments romance films thrillers remained stable since 1940s action films, superhero films, vampire films exhibit small increases proportion negative sentiments. Byszuk (2020) analysed transcriptions dialogue episodes Doctor , identifying strong stylistic differences classic series (produced 1963 1989), ‚Äòcommunities‚Äô episodes within group reflecting generic topical differences, new series (produced since 2005), shows strong authorial signal showrunners. study also identified strong authorial styles Doctor ‚Äôs showrunners writing process, using machine learning (specifically, support vector machines) classify dialogue according authorship shared writer show runner. Zyl & Botha (2016) identified differences speech patterns characters Big Bang Theory (2007-2019), showing main characters distinct ‚Äòvoice‚Äô speech Sheldon Cooper character linguistically distinct characters show reflecting specific linguistic choices made screenwriters constructing characters.Audio descriptions blind visually impaired cinemagoers used explore semantic content motion pictures, charting narratively significant character actions course film, distinguishing actions characteristic different genres, associating characters narrative events (Salway et al., 2005; Salway, 2007; Salway et al., 2007). Matthews & Glitre (2021) applied topic modelling plot summaries 32000 films identify lexical signatures different genres track evolution signatures time. Even movie titles analysed order predict box office success motion pictures (Bae & Kim, 2019).several packages text analysis R covering wide range methods, including:natural language processing: cleanNLP (T. B. Arnold, 2020) coreNLP (T. Arnold & Tilton, 2016).sentiment analysis: SentimentAnalysis (Proellochs & Feuerriegel, 2021), sentimentr (T. Rinker, 2021), sentometrics (Ardia et al., 2021), syuzhet (Jockers, 2020), vader (Roehrick, 2020).text mining text analysis: quanteda (Benoit et al., 2018), stylo (Eder et al., 2016), tidytext (Silge & Robinson, 2016), tm (Feinerer et al., 2008).data wrangling text data: textclean (T. W. Rinker, 2018b) stringr (Wickham, 2019).topic modeling: topicmodels (Gr√ºn & Hornik, 2011).also specialist packages working specific types cinematic lexical texts. Packages srt (Eder et al., 2020) subtools (Keck, 2019) make working .srt subtitle files straightforward.chapter attempt cover every possible method analysing lexical texts. Rather, focus three areas: processing movie subtitles produce tidy data set analysis; clustering films based word frequencies proportions; sentiment analysis structure films. like explore methods can adapt approaches described Silge & Robinson (2017) cinema‚Äôs lexical texts.","code":""},{"path":"texts.html","id":"setting-up-the-project-3","chapter":"9 Analysing lexical texts","heading":"9.1 Setting up the project","text":"","code":""},{"path":"texts.html","id":"create-the-project-3","chapter":"9 Analysing lexical texts","heading":"9.1.1 Create the project","text":"begin creating new project RStudio new directory using New project... command File menu run script projects_folders.R created Chapter 3. create additional subfolders within structure go make easier manage data.","code":""},{"path":"texts.html","id":"packages-6","chapter":"9 Analysing lexical texts","heading":"9.1.2 Packages","text":"chapter use packages listed Table 9.1:\nTable 9.1: Packages analysis cinemas lexical texts.\n","code":""},{"path":"texts.html","id":"data-4","chapter":"9 Analysing lexical texts","heading":"9.1.3 Data","text":"data set chapter subtitles top twenty grossing films US box office (adjusted inflation) genres horror, drama, action 2015 2019 based data Numbers. Table 9.2 lists films sample.\nTable 9.2: top 20 grossing horror, drama, action films US box office 2015 2019. Box office gross adjusted inflation. Source: Numbers. ‚òù\nsubtitles film sample downloaded opensubtitles.org .zip files extracted access .srt files..srt file accompanied .nfo file, contains information opensubtitles.org subtitles downloaded. use information files type can ignored.make easier manage subtitle files can create subfolder genre within Data folder project. .srt files films genre stored respective folders.","code":"\ngenre_sub_folders <- c(\"action\", \"drama\", \"horror\")\n\nfor (f in genre_sub_folders){dir.create(here::here(\"Data\", f))}"},{"path":"texts.html","id":"working-with-.srt-files","chapter":"9 Analysing lexical texts","heading":"9.2 Working with .srt files","text":"subtitles film sample contained .srt files include number subtitle, start end points hours:minutes:seconds,milliseconds, text subtitle. first six subtitles Aquaman (2018) shown .need re-arrange data useable format analysis. srt package makes working .srt files simple, extracting information file arranging data frame includes number subtitle, start end times subtitle, converted seconds, text subtitle. Table 9.3 displays first six subtitles Aquaman arranged data frame loading using srt::read_srt() function.\nTable 9.3: first six subtitles Aquaman (2018) arranged data frame loading using srt::read_srt().\nread .srt files film genre loop subfolders Data directory loop file within subfolder create data frame df_subtitles contains subtitles film.First, get list directories project‚Äôs Data folder. result returned character vector folder names. setting full.names = FALSE get just names folders Data folder rather full path directory. Setting recursive = FALSE means Data folder included object folders.Within folder, loop .srt files read add data frame df_subtitles store data set..srt files opensubtitles.org naming convention whereby title film followed year release details source media. example, filename .srt file Bohemian Rhapsody (2018) Bohemian.Rhapsody.2018.720p.BluRay.x264-SPARKS.srt. need title film use stringr::str_split() split filename two parts release year using pattern .20 (films example released since 2015 sequels .2 filename) keep string index 1 (.e., everything .20 filename), title film. also want replace dots (.) film‚Äôs title space using stringr::str_replace_all() ‚Äì notice dots need escaped using double backslashes (\\\\.) order R recognise characters.experience, found srt::read_srt() always assign id number first subtitle data frame, leaving cell value NA. can use dplyr::replace_na() assign correct value 1 case.","code":"1\n00:01:07,120 --> 00:01:08,440\n<i>Jules Verne once wrote,<\/i>\n\n2\n00:01:09,040 --> 00:01:11,999\n<i>\"Put two ships in the open sea<\/i>\n\n3\n00:01:12,160 --> 00:01:14,550\n<i>without wind or tide,\nthey will come together.\"<\/i>\n\n4\n00:01:37,200 --> 00:01:41,274\n<i>That's how my parents met.\nLike two ships destined for each other.<\/i>\n\n5\n00:01:45,640 --> 00:01:48,039\n<i>We are something\nelse. Try Arthur Treacher...<\/i>\n\n6\n00:01:55,280 --> 00:01:56,280\nIt's okay.\npacman::p_load(srt)\naquaman_subtitles <- read_srt(here(\"Data\", \"action\", \"Aquaman.2018.720p.BluRay.x264.srt\"),\n                              collapse = \" \")\nfolders <- list.dirs(here(\"Data\"), full.names = FALSE, recursive = FALSE)\n\nfolders\n## [1] \"action\" \"drama\"  \"horror\"\n# Create an empty data frame to store the results\ndf_subtitles <- data.frame()\n\n# Loop over each folder in the Data directory\nfor(f in seq_along(folders)){\n  \n  genre <- folders[f]\n  \n  # Get a list of the .srt files in a folder\n  files <- list.files(here(\"Data\", genre), pattern = \"*.srt$\")\n  \n  # Loop over each .srt file in a folder\n  for (i in seq_along(files)){\n    \n    file <- files[i]\n    film <- str_split(file, pattern = \".20\", simplify = TRUE)[1]\n    film <- str_replace_all(film, pattern = \"\\\\.\", replacement = \" \")\n    \n    dat <- read_srt(here(\"Data\", genre, files[i]), collapse = \" \")\n    \n    dat <- dat %>% mutate(genre = rep(genre, length(n)),\n                          film = rep(film, length(n)),\n                          n = replace_na(n, 1)) %>%\n      relocate(film) %>%\n      relocate(genre) \n    \n    df_subtitles <- rbind(df_subtitles, dat)\n    \n  }\n  \n}"},{"path":"texts.html","id":"removing-html-tags","chapter":"9 Analysing lexical texts","heading":"9.2.1 Removing html tags","text":"can see Table 9.3, subtitles intended presented italics tagged using html italic tags: <> ‚Ä¶ <\/>. need removed prior analysis. can using str_replace_all() function stringr package. remove tags replace nothing, .e.¬†\"\" (Table 9.4).\nTable 9.4: first six subtitles Aquaman (2018) html tags removed.\n","code":"\ndf_subtitles <- df_subtitles %>% \n  mutate(subtitle = str_replace_all(subtitle, c(\"<i>\" = \"\", \"<\/i>\" = \"\")))"},{"path":"texts.html","id":"removing-extended-ascii-and-unicode-characters","chapter":"9 Analysing lexical texts","heading":"9.2.2 Removing extended ASCII and unicode characters","text":"subtitles indicate lyrics topping tailing subtitle musical note (ùÖ†). rendered characters df_subtitles data frame appear nonsensical set symbols (e.g., √É¬¢√¢‚Äû¬¢√Ç¬™) unicode character musical note represented ASCII character. also need removed. Rather use stringr package, time use gsub() function regular expressions (regex) remove non-ASCII characters hex codes 20 7E (though use stringr regular expressions wished ). ASCII character encoding format enables text characters represented numerically computer. example, ASCII code 65 (41 hex). ASCII characters hex codes range 20 7E printable characters include letters (upper lower case) main punctuation symbols English.replace ASCII characters outside desired range character subtitle column df_subtitles data frame need tell gsub() (1) look (\"[^\\x20-\\x7E]\") ‚Äì ^ symbol logical regex; (2) replacement string (\"\"); (3) text source (df_subtitles$subtitle).means song lyrics now treated dialogue data frame, may desirable depending design particular study. may want remove text completely data frame. However, let worry us .ASCII characters range 20 7E include accented characters (.e., letters acute, grave, circumflex, umlauts, accents), part extended ASCII. Consequently, words accented characters altered. example, fianc√© become fiance. Working languages English thus require consideration character map used represent letters. companies require subtitlers use UTF-8 character encoding ensure proper display foreign characters, words including characters may suitable sentiment analysis may recognised sentiment lexicon.","code":"\ndf_subtitles$subtitle <- gsub(\"[^\\x20-\\x7E]\", \"\", df_subtitles$subtitle)"},{"path":"texts.html","id":"removing-subtitle-credits","chapter":"9 Analysing lexical texts","heading":"9.2.3 Removing subtitle credits","text":"Subtitles opensubtitles.org submitted members public may add subtitle identifying work (Table 9.5).\nTable 9.5: sample rows data frame df_subtitles containing string Subtitles\nwant remove rows data frame filter rows contain string Subtitles. use stringr::str_detect() function find rows contain particular string characters, prefacing exclamation mark !, symbol logical dplyr. following code thus leaves behind rows contain string Subtitles.require us re-number subtitles films affected process.Looking example Avengers Endgame (2019) Table 9.6, can see first subtitle containing credit creator .srt file removed subtitles re-numbered.\nTable 9.6: first five subtitles Avengers Endgame (2019) filtering re-numbering.\nNow wrangled .srt files tidy data frame cleaned text, can begin analysis.","code":"\ndf_subtitles <- df_subtitles %>%\n  filter(!str_detect(subtitle, \"Subtitles\"))\ndf_subtitles <- df_subtitles %>%\n  group_by(film) %>%\n  mutate(n = 1:length(film))"},{"path":"texts.html","id":"word-frequencies-correlation-and-cluster-analysis","chapter":"9 Analysing lexical texts","heading":"9.3 Word frequencies: correlation and cluster analysis","text":"Silge & Robinson (2016) describe method comparing frequency words texts based correlations proportions words pair texts.extend method produce heatmap lets us explore relationships subtitles films sample cluster texts based correlations.First, need load tidytext package can create tidy data frame one row word film displays frequency word occurs film. unnest_tokens() function splits text subtitle tokens specified level, token meaningful unit text word, n-gram, sentence. interested tokens level individual words subtitle column df_subtitles data frame pass keyword arguments word subtitle function. Stop words commonly occurring words , , , , filtered sample useful analysing texts. can removed using anti-join() function, filters contents data frame (unnested word counts) match contents second data frame (list stop words). store results new data frame df_word_counts.can see head data frame commonly occurring words Quiet Place (2018) protect, safe, sound, stop, though frequencies low film features little dialogue, signed.can use data frame df_word_counts calculate proportion words film‚Äôs subtitles pivot data frame wide format words sample assigned row names data frame.Looking head data frame containing word proportions see protect occurs Quiet Place Alien: Covenant (2017), though accounts lower proportion total dialogue latter. protect occur Star Born (2018) value NA.calculate correlation proportion words films sample, first need convert data frame matrix: m_word_props. calculate pairwise correlations every film sample using cor() function, use argument sent complete.obs missing values handled case-wise deletion. means correlation pair films based set variables shared films variables least one missing value dropped. Setting use = pairwise.complete.obs handles missing data pair-wise deletion pair variables data available included correlation matrix. result matrix different correlation coefficients based different set variables (.e., words).correlation matrix large read visualise information contains heatmap. first, little data wrangling.able identify genre films heatmap need create list films sample genres associated assign colour genre. can creating data frame df_annotations() containing list films sample, using dplyr::distinct() get list film title appears . (df_subtitles data frame title film occurs multiple times). also need convert column film titles row names data frame. done count number different genres sample assign colour genre pass heatmap.visualise relationships films use pheatmap package, give us heatmap correlations films along dendrogram showing subtitles films cluster. df_films passed list annotations along annotation colours annotations_pal. can select hierarchical clustering method options method argument hclust() function base stats packages. use complete linkage method.\nFigure 9.4: Heatmap cluster analysis sample movie subtitles three genres. ‚òùÔ∏è\nheatmap dendrogram Figure 9.4 shows reasonable separation different genres (reading columns left right), horror films grouping left (Annabelle Comes Home (2019) (2017)), action films central cluster (Annabelle: Creation (2017) Spider-man Far Home (2019)), drama films right (Hidden Figures (2016) Sully (2016)), though clustering perfectly separate different classes films. Little Women (2019), example, cluster drama films finds among horror films. Little Women appears correlate highly Quiet Place films relatively high proportion words relating family (e.g.¬†mother). centre heatmap comprised superhero films, mixing together films Marvel DC universes, superhero movies found elsewhere heatmap. Deadpool (2016) found among horror films, Captain Marvel (2019) Avengers Endgame (2019) found among group dominated drama films. Interestingly, films cluster sequels. Deadpool Deadpool 2 (2018) cluster together. four Avengers films; : Chapter 2 (2019); Creed (2015) Creed II (2018); Fifty Shades Grey (2015) Fifty Shades Darker (2017) Fifty Shades Freed (2019), though latter two films part larger cluster drama films first movie franchise .Looking heights branches dendrogram correlation films within three main clusters, see films cluster dominated drama films tend correlated, lower branches dendrogram. indicates proportions words films cluster similar one another films cluster dominated horror films, exhibits less correlation films within cluster branches dendrogram tend higher indicating greater distance films. said, three groups within horror-dominated cluster much similar one another films cluster.","code":"\npacman::p_load(tidytext)\n\ndf_word_counts <- df_subtitles %>% \n  group_by(film) %>%\n  unnest_tokens(word, subtitle) %>% \n  count(word, sort = TRUE) %>% \n  anti_join(stop_words) %>%\n  arrange(film)\n\nhead(df_word_counts)## # A tibble: 6 √ó 3\n## # Groups:   film [1]\n##   film          word        n\n##   <chr>         <chr>   <int>\n## 1 A Quiet Place protect     5\n## 2 A Quiet Place safe        5\n## 3 A Quiet Place sound       5\n## 4 A Quiet Place stop        5\n## 5 A Quiet Place mother      4\n## 6 A Quiet Place sounds      4\ndf_word_props <- df_word_counts %>%\n  group_by(film) %>%\n  mutate(proportion = n / sum(n)) %>%\n  select(-n) %>%  # Drop the count column\n  pivot_wider(names_from = film, values_from = proportion) %>%\n  column_to_rownames(var = \"word\")\n\ndf_word_props[1:5, 1:3]##         A Quiet Place A Star is Born Alien Covenant\n## protect     0.0390625             NA    0.000427899\n## safe        0.0390625   0.0002915452    0.002995293\n## sound       0.0390625   0.0008746356    0.000427899\n## stop        0.0390625   0.0011661808    0.001711596\n## mother      0.0312500             NA    0.007274283\n# Change the class of the word proportions to a matrix\nm_word_props <- as.matrix(df_word_props)\n\n# Correlations between films using case-wise deletion\nm_word_correlations <- cor(m_word_props, method = \"pearson\", \n                           use = \"complete.obs\")\n\nm_word_correlations[1:5, 1:3]##                      A Quiet Place A Star is Born Alien Covenant\n## A Quiet Place           1.00000000     -0.2829213     0.08645856\n## A Star is Born         -0.28292131      1.0000000    -0.38288213\n## Alien Covenant          0.08645856     -0.3828821     1.00000000\n## Annabelle 2 Creation    0.38273895      0.1306393     0.58962252\n## Annabelle Comes Home   -0.20766965     -0.0621203     0.30373570\ndf_annotations <- df_subtitles %>% select(genre, film) %>%\n  distinct(genre, film, .keep_all = TRUE) %>% \n  column_to_rownames(var = \"film\")\n\n# Count the number of genres in the sample\nk <- length(unique(df_annotations$genre))\n\n# Create a palette with k colours using the mako palette\npal <- viridis::mako(k)\n\n# Create a list associating a genre with each colour in the palette\nannotations_pal <- list(genre = c(\"horror\" = pal[1], \n                                  \"drama\" = pal[2], \n                                  \"action\" = pal[3]))\npacman::p_load(pheatmap)\n\npheatmap(m_word_correlations, \n         # Add annotations to the columns and rows of the heatmap\n         annotation_col = df_annotations, annotation_row = df_annotations,\n         # Colours for the annotations\n         annotation_colors = annotations_pal, \n         # Suppress the label for the annotation row and column\n         annotation_names_row = FALSE, annotation_names_col = FALSE,\n         # Set the palette for the heatmap with 50 levels\n         color = viridis::viridis(50), \n         # Set the method for hierarchical clustering\n         clustering_method = \"complete\",\n         # Control display elements of the heatmap\n         fontsize_row = 8, fontsize_col = 8, cellheight = 8, cellwidth = 8)"},{"path":"texts.html","id":"sentitment-analysis-emotion-and-structure","chapter":"9 Analysing lexical texts","heading":"9.4 Sentitment analysis: emotion and structure","text":"Torben Grodal‚Äôs mental flow model experience watching film takes film genres prototypical narrative forms constructed evoke characteristic emotions viewers (Grodal, 1997, 1999, 2005). characteristic emotion action films aggressive tensity, fear characteristic emotion horror films. Tragedies, comedies, romances characterised sorrow, laughter, romantic love, respectively. genres associated particular emotions expect see reflected use emotional words dialogue films belonging particular genre.Sentiment analysis natural language processing technique automatically identify, quantify, classify emotional valence text; evaluate text‚Äôs polarity (positive, negative, neutral) presence absence different affective states (anger, joy, sadness, surprise); analyse temporal evolution sentiments course text.valence text determined sentiment score. common way assigning sentiment score match words text lexicon terms identified possessing particular polarity, exhibiting affective state, value representing strength emotion. Commonly used lexicons include AFINN, bing, nrc. sentiment score can assess different textual levels, including individual words, sentences, whole texts.section, explore frequency different emotions across genres sample emotional structure films using sentimentr syuzhet packages.","code":""},{"path":"texts.html","id":"emotion-words","chapter":"9 Analysing lexical texts","heading":"9.4.1 Emotion words","text":"illustrate simple sentiment analysis use subtitles Annabelle: Creation (2017), filter df_subtitles data frame. extract sentences film use get_sentences() function sentimentr package create list sentences subtitle text data frame.assign emotions sentences use sentimentr::emotion() function. emotion() function returns data frame counts number words sentence number words emotion type. assigns score emotion sentence dividing number words assigned emotion total number words sentence.example using nrc lexicon lexicon package. lexicon assigns value -1 negative words 1 positive words. also use valence shifters lexicon package order recognise emotions negated terms negative valence (e.g., happy) mistaken terms positive valence (happy) negation contrasting conjunction ignored. Ignoring negation problem R packages sentiment analysis users make sure read manuals vignettes package understand handles word valences. valence shifters also amplify de-amplify emotions order modulate strength emotion.first sentence subtitles Annabelle: Creation (‚ÄúThink hide , ?‚Äù) eight words, one tagged fear word (hide), give fear score \\(\\frac{1}{8} = 0.125\\). score emotions zero (Table 9.7).\nTable 9.7: emotions words first sentence subtitles Annabelle: Creation (2017). ‚òù\ncan count number words assigned emotion plot bar chart analyse frequency different emotions. Along way also want little data wrangling, converting names emotions factor; formatting names replace underscores sentimentr::emotion() function uses append negated category emotion (e.g.¬†anger_negated) space; capitalizing names emotions.Figure 9.5 shows dialogue Annabelle: Creation uses broad range emotions. Positive emotions joy trust frequent categories, negative emotions (anger, disgust, fear, sadness) slightly less numerous aggregate. nrc emotion lexicon negative categories positive categories way assigns words across categories may introduce bias analysis, indicating false diversity terms across particular valence. context horror genre, anticpation surprise words may considered negative emotion serve creating frightening experience viewer. another genre, romance, words likely positive valence. Negated emotional terms relatively rare, indicating emotions conveyed dialogue Annabelle: Creation direct.\nFigure 9.5: frequency words different emotions Annabelle: Creation (2017).\ncan plot trendlines different emotions course film fitting loess smoother emotion scores subtitles. ‚Äònarrative time‚Äô numbering subtitles order correspond running time film. use timings obtained loaded subtitles using srt::read_srt() ; however, presents us problem one sentence subtitle. timings returned srt::read_srt() subtitles sentences. split time allocated subtitle half work timings sentence individually, also artificial.negated emotions infrequent Annabelle: Creation drop data frame focus direct emotions . done , select columns need group data emotion_type variable fit smoother. Different values span parameter return different loess smoothers; use span 0.4 emotions.Now fitted smoothers can plot results see different emotions evolve course film. Figure 9.6 shows first half subtitles emotions anticipation, trust joy exhibit similar trends. However, second half data set relationship broken joy increases subtitles towards end film, recovering lowest point midway film, anticpation trust rise level fall away end. Interestingly, anticpation suprise similar trendlines, indicating words emotions correlate. Negative emotions tend stronger second half subtitles, sadness disgust increasing markedly. relationship anger, disgust, fear complicated different pairs emotions similar trendlines different points film. anger fear closely tied first part film disgust fear track together use anger words falls away remains low nearly half subtitles. , second half, three emotions exhibit similar trends disgust increases others fall away. sadness increases joy decreases vice versa film, one might expect; though final quarter two emotions similar trends.\nFigure 9.6: Fitted loess trendlines individual emotions Annabelle: Creation. ‚òùÔ∏è\nObviously, type analysis excludes much emotional information available viewer performance, facial expressions, mise-en-sc√©ne, music; analysis frequency temporal distribution emotion words text can determine filmmakers use full range formal devices realise emotions indicated script.Next, let us look frequency different emotion words every film sample sorted genre. need create nested data frame df_films three parts: character vector identifying genre film, character vector identifying title film, list 60 data frames, one per film, contains subtitles film.created nested data frame loop element part build single data frame df_emotions get sentences films subtitles emotion words sentences . part loop create list unique indices emotion word using dplyr::group_indices(). necessary step extract sentences, subtitles containing one sentence value element_id column uniquely identified value sentence_id column, making later processing trickier. occurs sentimentr treats every subtitle individual text rather part single text. Creating id column based grouping together elmenent_id sentence_id make lives easier.Now obtained emotion words film can find many words occur type emotion film.can check head data frame ensure proper format.Now can plot data begin analysis.\nFigure 9.7: frequency emotion terms genre. ‚òùÔ∏è\nalso worth recalling films genres use different emotion words, used different. quantitative analysis answer ‚Äò‚Äô questions, can help us pick potentially explicable patterns frame questions worth exploring.Figure 9.7 shows us frequency emotion words films across different genres similar action drama films. Horror films tend far fewer anticipation trust words, marginally fewer words emotions. Horror films create anticpation surpise use startles (shock cuts acoustic startles) may use words emotions less frequently genres. example, characters horror films tend reactive, responding horrifying situations find ; whereas drama films focus characters desires goals future (part canonic form Hollywood narration; see Stam et al. (1992): 189). may account fact horror films appear use fewer anticipation words relative drama films.Interestingly, horror films slight tendency use fewer fear words either action drama genres. may fear dominant mood films genre less need characters express emotion dialogue different elements film style (mise-en-scene, cinematography, editing, sound design) already created emotional state viewer. contrast, fear mood action drama films, may necessary characters verbalise fear explicitly. come across studies examining role dialogue shaping emotional experience viewers horror films, though many analyses visual sonic style achieve . basis brief study, project worth considering future.Negated emotions much less frequent direct emotions films three genres differences difference genres frequency negated emotions occur. Action films exhibit less variation frequency different emotion words genres, reflecting relative homogeneity films class ‚Äì action films sample superhero movies ‚Äì whereas genres heterogeneous. Straight Outta Compton (2015) stands emotion words films sample, particularly anger, disgust, fear, sadness, trust. film also anger negated disgust negated words.Figure 9.7 uses raw counts emotion words. repeat analysis using emotion scores produced dividing number emotion words sentence total number words sentence (.e., emotion column Table 9.7) order compare relative strength emotional content films. summing emotion scores calculated way, can take account verbosity film. fact Straight Outta Compton many anger words may simply much dialogue films sample. Perhaps reason horror films fewer emotion words drama films simply due fact horror films fewer words total. may case dialogue horror films emotional proportion emotion words higher genres. code can easily adapted plot emotion score film rather raw frequencies ‚Äì leave exercise reader perform analysis.","code":"\n# Select the data for Annabelle: Creation\ndf_annabelle_2 <- df_subtitles %>%\n  filter(film == \"Annabelle 2 Creation\")\n\n# Load the required packages\npacman::p_load(sentimentr, syuzhet)\n\n# Extract the sentences from the subtitles\nannabelle_2_sentences <- get_sentences(df_annabelle_2$subtitle)\nannabelle_2_emotions <- emotion(annabelle_2_sentences,\n                                emotion_dt = lexicon::hash_nrc_emotions,\n                                valence_shifters_dt = lexicon::hash_valence_shifters,\n                                drop.unused.emotions = FALSE)\nannabelle_2_emo_counts <- annabelle_2_emotions %>% \n  group_by(emotion_type) %>% \n  summarise(count = sum(emotion_count)) %>% \n  mutate_if(is.factor, as.character) %>% \n  arrange(emotion_type) %>% \n  mutate_at(\"emotion_type\", str_replace, \"_\", \" \") %>% \n  mutate(emotion_type = R.utils::capitalize(emotion_type))\n\nhead(annabelle_2_emo_counts)## # A tibble: 6 √ó 2\n##   emotion_type         count\n##   <chr>                <int>\n## 1 Anger                   33\n## 2 Anger negated            2\n## 3 Anticipation            96\n## 4 Anticipation negated     8\n## 5 Disgust                 35\n## 6 Disgust negated          3\nggplot(data = annabelle_2_emo_counts) +\n  geom_col(aes(x = reorder(emotion_type, desc(emotion_type)), \n               y = count, fill = emotion_type)) +\n  geom_text(aes(x = emotion_type, y = count, label = count), \n            hjust = -0.4, size = 3.2) +\n  scale_x_discrete(name = \"Emotion\") +\n  scale_fill_viridis_d() +\n  coord_flip() +\n  theme_classic() +\n  theme(axis.line = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        legend.position = \"none\",\n        panel.grid = element_blank())\n# Drop the negated emotion and do a little formatting\nannabelle_2_emotions <- annabelle_2_emotions %>% \n  filter(!str_detect(emotion_type, \"_negated\")) %>%\n  mutate(emotion_type = R.utils::capitalize(emotion_type))\n\n# Fit a loess smoother to each emotion\nannabelle_2_fitted_values <- annabelle_2_emotions %>%\n  select(element_id, emotion_type, emotion) %>%\n  group_by(emotion_type) %>%\n  mutate(model = loess(emotion ~ element_id, span = 0.4)$fitted) %>%\n  mutate(model = round(model, 5))\n\nhead(annabelle_2_fitted_values)## # A tibble: 6 √ó 4\n## # Groups:   emotion_type [6]\n##   element_id emotion_type emotion   model\n##        <int> <chr>          <dbl>   <dbl>\n## 1          1 Anger          0     0.0506 \n## 2          1 Anticipation   0     0.0785 \n## 3          1 Disgust        0     0.00795\n## 4          1 Fear           0.125 0.0478 \n## 5          1 Joy            0     0.0788 \n## 6          1 Sadness        0     0.0082\nannabelle_2_emotion_plot <- ggplot(data = annabelle_2_fitted_values) +\n  geom_line(aes(x = element_id, y = model, \n                group = emotion_type, colour = emotion_type,\n                text = paste(\"Narrative time: \", element_id,\n                             \"<br>\", emotion_type, \": \", model))) +\n  scale_colour_viridis_d(name = NULL) +\n  labs(x = \"Narrative time\", y = \"Emotion\") +\n  theme_light()\n\n# Load the plotly package to create an interactive plot\npacman::p_load(plotly)\n\nggplotly(annabelle_2_emotion_plot, tooltip = \"text\") %>%\n  # Format the legend of the interactive plot\n  layout(legend = list(orientation = 'h', xanchor = \"left\", y = -0.2,\n                       title = list(text = '<b>Emotion<\/b>'), \n                       font = list(size = 10.5)))\n# Created a nested data frame of subtitles\ndf_films <- df_subtitles %>%\n  select(genre, film, n, subtitle) %>%\n  group_by(genre, film) %>%\n  nest()\n\nhead(df_films)## # A tibble: 6 √ó 3\n## # Groups:   genre, film [6]\n##   genre  film                              data                \n##   <chr>  <chr>                             <list>              \n## 1 action Aquaman                           <tibble [1,128 √ó 2]>\n## 2 action Avengers Age of Ultron            <tibble [1,852 √ó 2]>\n## 3 action Avengers Endgame                  <tibble [2,339 √ó 2]>\n## 4 action Avengers Infinity War             <tibble [1,724 √ó 2]>\n## 5 action Batman v Superman Dawn of Justice <tibble [1,538 √ó 2]>\n## 6 action Black Panther                     <tibble [1,569 √ó 2]>\n# Create an empty data frame to collect the results of the loop\ndf_emotions <- data.frame()\n\nfor(i in 1:length(df_films[[1]])){\n  \n  genre <- df_films[[1]][i] # Get the genre of a film\n  film <- df_films[[2]][i]  # Get the title of a film\n  subs <- df_films[[3]][i]  # Get the data frame containing a film's subtitles\n  \n  # Extract sentences the sentences\n  sentences <- get_sentences(subs[[1]]$subtitle)\n  \n  # Get the emotion words\n  df <- emotion(sentences, drop.unused.emotions = FALSE)\n  \n  df <- df %>% mutate(genre = rep(genre, length(element_id)),\n                      film = rep(film, length(element_id)),\n                      id = group_indices(., element_id, sentence_id)) %>% \n    relocate(id) %>%\n    relocate(film) %>%\n    relocate(genre) %>%\n    select(-c(element_id, sentence_id))\n  \n  df_emotions <- rbind.data.frame(df_emotions, df)\n  \n}\ndf_emotion_counts <- df_emotions %>% \n  group_by(genre, film, emotion_type) %>% \n  summarise(count = sum(emotion_count)) %>% \n  mutate_if(is.factor, as.character) %>% \n  arrange(emotion_type) %>% \n  mutate_at(\"emotion_type\", str_replace, \"_\", \" \") %>% \n  mutate(emotion_type = R.utils::capitalize(emotion_type),\n         genre = R.utils::capitalize(genre))\nhead(df_emotion_counts)## # A tibble: 6 √ó 4\n##   genre  film                              emotion_type count\n##   <chr>  <chr>                             <chr>        <dbl>\n## 1 Action Aquaman                           Anger          113\n## 2 Action Avengers Age of Ultron            Anger          132\n## 3 Action Avengers Endgame                  Anger          155\n## 4 Action Avengers Infinity War             Anger          173\n## 5 Action Batman v Superman Dawn of Justice Anger          157\n## 6 Action Black Panther                     Anger          127\nsample_emotions_plot <- ggplot(data = df_emotion_counts) +\n  geom_jitter(aes(x = reorder(genre, desc(genre)), y = count, colour = genre,\n                  text = paste(\"Film: \", film,\n                               \"<br>Frequency: \", count)), \n              size = 1, width = 0.2) +\n  scale_colour_viridis_d(begin = 0, end = 0.8) +\n  labs(x = NULL, y = \"Frequency\") +\n  facet_wrap(~emotion_type, nrow = 8) +\n  coord_flip() +\n  theme_light() +\n  theme(legend.position = \"none\",\n        panel.spacing.y = unit(10, \"pt\"),\n        strip.background = element_blank(),\n        strip.text = element_text(colour = \"black\"),\n        text = element_text(size = 10))\n\n# Make the plot interactive\nggplotly(sample_emotions_plot, tooltip = \"text\")"},{"path":"texts.html","id":"how-to-edit-an-r-function","chapter":"9 Analysing lexical texts","heading":"9.4.2 How to edit an R function","text":"One key features R open source; , anyone can see alter code base functions packages. means function want , can edit function get want.simple_plot() function syuzhet package returns plot trend sentiments text, fitting three smoothers sentiments sentences text ‚Äì moving average, loess smoother, discrete cosine transform (DCT) ‚Äì describe trend sentiments course text. Figure 9.8 plots sentiments subtitles Annabelle: Creation produced using syuzhet::simple_plot(). degree smoothing moving average determined window argument. smoothing discrete cosine transform determined lps argument, sets size low pass filter ‚Äì lower value greater level smoothing amount noise trendline increasing size filter increases. Changing either arguments affect shape loess smoother, argument control span. Note x-axis represents narrative time lexical text, ‚Äòtime‚Äô index sentence text. first sentence text thus time 1, second sentence time 2, . , confused running time film.simplified macro shape lower panel Figure 9.8 plots scaled version discrete cosine transform normalised narrative time sampled 100 data points, values x-axis percentage narrative time data set. lps simplified macro shape size 5 controlled arguments passed simple_plot() function.\nFigure 9.8: Trends sentiment Annabelle: Creation (2017) produced using syuzhet::simple_plot().\nsimple_plot() function something black hole ‚Äì data goes function recovered output. Consequently, use fitted values smoothers subsequent analysis may wish . also limits us using plots produced function, use R‚Äôs base graphics package allows us modify plots specifying main title position legend . want produce ggplot2 style plots need access data values trendlines can visualize data . can change adding code simple_plot() function builds data frame containing data want returns data frame function exists. code simple_plot() function can read .Reading simple_plot() function see :sentence number raw sentiment data stored objects x y, respectively.discrete cosine transform stored object trans.values moving average stored object rolled.loess smoother stored object low_line.simplified macro shape scaled version discrete cosine transform, stored object normed_trans.Note data objects created using functions syuzhet package. example, discrete cosine transform produced get_dct_transform() function. use function get DCT values directly want, collect data different smoothers one go editing simple_plot() function.\nsimple_plot() function syuzhet package fits\nthree smoothers sentiments sentences ‚Äì moving average,\nloess\nsmoother, discrete\ncosine transform ‚Äì describe trend sentiments \ncourse text. cant set parameters smoothers \nsetting size low pass filter (lps) \nwindow size. can also set title plot returned\nfunction position plot legend.\n\nfunction also returns simplified macro shape plot \ndiscrete cosine transform lps = 5 sampled 100\npoints x-axis, representing normalised narrative time \ntext.\nR‚Äôs edit() function launches script editor (see Figure 9.9) lets us edit simple_plot() function create new function called sp adding lines code collect data objects identified return data frame desire. code new sp() function can read . Clicking Save script editor window add new sp() function workspace.\nFigure 9.9: script editor adapting function RStudio. ‚òùÔ∏è\nDouble-clicking function name Environment tab RStudio‚Äôs Environment pane open function Source pane. can save sp() R script Code folder project. save function R script RStudio, go File > Save ... save file project‚Äôs Scripts folder sp.R. can source file whenever need .Note new sp() function still uses functions syuzhet package still necessary load package order sp() function correctly.\nsp() function performs analysis \nsimple_plot() function still return plot\n; also add list data frames workspace \nassign object.\n\ncode changed \n\nwrap plot commands statement controlled \nplot argument, default value \nTRUE;\n\nmove calculation simplified macro shape \nlie within statement (lines 18-19); ,\n\ncollect fitted values list data frames three\nsmoothers simplified macro shape (lines 39-48). necessary\nstore separate data frames different\nnumbers data values, returning list data frames\nsimplifies process.\n\nedits made simple_plot()\nfunction include adding argument control span loess\nsmoother, adding argument control size low pass filter\nsimplified macro shape plot, adding plots list \noutputs function.\n\nalso change way function presents graphics using\nggplot2 instead R‚Äôs base graphics package, adding arguments \ncontrol appearance plots necessary.\nUsing new sp() function, can gather data want data frame. results edited function original function (compare Figure 9.10 Figure 9.8 see identical). difference now outputs accessible us data.\nFigure 9.10: sp() function returns result original syuzhet::simple_plot() function.\naccess different data frames list l_annabelle_2 index using double square brackets: [[]]. head tail values moving average smoothers data frame NA values window moving average include values. grey line moving average Figure 9.10 start end limits x-axis.Using data frame can now create plot data, allows us control many features creating data visualisations. Figure 9.11 plots discrete cosine transform sentiments Annabelle: Creation.\nFigure 9.11: ggplot2 style version output sp() function sentiments Annabelle: Creation.\n","code":"\n# Get the sentiments for each sentence\nannabelle_2_sentiments <- get_sentiment(annabelle_2_sentences)\n\n# Set the margin parameters - simple_plot() is fussy about plot margins\npar(mar = c(5,5,2,1))\n# Plot the smoothers\nsimple_plot(annabelle_2_sentiments, lps = 30, window = 0.05, \n            title = \"Annabelle: Creation\")\n# Load the syuzhet package\npacman::p_load(syuzhet)\n\n# Open the editor to create a new function call sp\nsp <- edit(simple_plot)\n# Remember to load the syuzhet package when using sp()\npacman::p_load(syuzhet)\n\n# Source the sp() function from the Scripts folder\nsource(here(\"Code\", \"sp.R\"))\n\n# Get the sentiments for each sentence\nannabelle_2_sentiments <- get_sentiment(annabelle_2_sentences)\n\n# Set the margin parameters\npar(mar = c(5,5,2,1))\n# Apply the function\nl_annabelle_2 <- sp(annabelle_2_sentiments, lps = 30, window = 0.05, \n                    title = \"Annabelle: Creation\", plot = TRUE)\n# The head of the smoothers data frame\nhead(l_annabelle_2[[1]])##   Sentence Sentiment Rolling Mean        DCT     Loess\n## 1        1      0.00           NA -0.1582106 0.5471998\n## 2        2      0.00           NA -0.1555304 0.5414273\n## 3        3      0.60           NA -0.1501918 0.5356729\n## 4        4      0.75           NA -0.1422383 0.5299364\n## 5        5      0.00           NA -0.1317350 0.5242178\n## 6        6      0.00           NA -0.1187673 0.5185168\n# The head of the simplified macro shape data frame\nhead(l_annabelle_2[[2]])##   Normalised_time Simplified\n## 1               1  1.0000000\n## 2               2  0.9924718\n## 3               3  0.9775085\n## 4               4  0.9552948\n## 5               5  0.9261044\n## 6               6  0.8902958\nggplot(data = l_annabelle_2[[1]]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", colour = \"gray\", linewidth = 1) +\n  geom_line(aes(x = Sentence, y = DCT), colour = \"#277F8E\", linewidth = 1) +\n  scale_x_continuous(expand = c(0.01, 1)) +\n  labs(x = \"Narrative time\", y = \"Scaled sentiment\") +\n  theme_light() +\n  theme(legend.position = \"none\")"},{"path":"texts.html","id":"clustering-sentiment-structure","chapter":"9 Analysing lexical texts","heading":"9.4.3 Clustering sentiment structure","text":"new sp() function allow us loop subtitles 60 films grouped data frame, gathering sentiment data dialogue film can conduct analysis temporal structure sentiments level corpus. collected sentiment data film can apply cluster analysis determine common emotional patterns Hollywood films.loop grouped data frame df_films created extract sentences using sentimentr::get_sentences() assign sentiment score sentence using syuzhet::sentiment(). create two data frames store two different data outputs produced sp() function: one smoothers (df_sentiments_complete) one data used plot simplified macro shape (df_sentiments_simplified).can check outputs loop looking head data frame created.Recalling applications cluster analysis analysing colour editing data, need data sets column equal numbers data points evenly sampled. data frame df_sentiments_complete detailed information emotional structure evident subtitles films sample; film different number subtitles. normalise narrative time subtitles unit length sample smoother even points bin normalised narrative time calculate average sentiment within bin order obtain version suitable analysis.example need take either steps already representation sentiment structure film meets requirements cluster analysis. df_sentiments_simplified contains data simplified macros shape film, comprises 100 data points representing sentiment structure sampled evenly across narrative time film. means already data set sentiment structure film suitable cluster analysis use data .need wrangle data applying cluster analysis order pivot data frame wide format transposing data row film, column represents point normalised narrative time, value cell sentiment film time. Transposing data using t() return object matrix class need change class back data.frame.Although data set contains sixty films sampled 100 data points, 6000 data points large enough make k-means clustering slow inefficient. therefore use Clustering Large Applications (CLARA) algorithm previously used construct colour palette applying partitioning medoids clustering using fpc::pamk(). allows us search across range values optimal number clusters. Del Vecchio et al. (2020) found six clusters optimal applying k-means clustering subtitles sample 6000 films, set search range two ten clusters. yet scaled data constructing palette Chapter 5, set scaling = TRUE, centre column divide root mean square. draw 250 random samples data set sample size set 30, sample drawn contains half films. Finally, use Euclidean distance metric construct clusters.optimal number clusters 9. order prepare data set plotting need data wrangling create data frame contains titles films clusters belong.use dplyr‚Äôs inner_join() function merge cluster information df_sentiments_simplified data frame. inner_join() mutating join add column df_sentiments_simplified data frame.Now can plot data, making sure group data film order plot time series sentiments, assigning colour genre. produce facet plot, facet cluster sample. Figure 9.12 plots results cluster analysis.\nFigure 9.12: Clustering sentiment structure subtitles Hollywood films. ‚òùÔ∏è\nresults larger number clusters reported Del Vecchio et al. (2020), found sentiment structure subtitles Hollywood films best described six clusters. may due range factors: much smaller data set used different methods assigning sentiments dialogue cluster analysis. similarities shapes trendlines clusters 4 6 (e.g., trendlines Dr Strange (2016) Deadpool) suggests differences clusters may relatively small. Del Vecchio et al. (2020) merged similar clusters together may also need combine clusters account differences.Figure 9.12 see clusters mirror images one another. example, peaks troughs clusters 3 8, clusters 6 7, clusters 2 5 occur opposite points normalised narrative time subtitles.narrative classes identified Del Vecchio et al. (2020) evident results. ‚Äòrags--riches‚Äô pattern characterised rise sentiment course film corresponds shapes trendlines cluster 9. ‚ÄòOedipus‚Äô pattern (fall-rise-fall) evident cluster 7; ‚ÄòCinderella‚Äô pattern (rise-fall-rise) can seen cluster 6. ‚Äòman---hole‚Äô narrative (fall-rise) evident clusters 1, 2, 4; ‚ÄòIcarus‚Äô narrative (rise-fall) cluster 5. ‚ÄòRiches--rags‚Äô narratives (falling sentiment across film) occur sporadically clusters 5 7 (e.g., Furious 7 (2015)) appear particular cluster associated class narrative. may due small sample size used . However, clusters 3 8 match narrative patterns identified previous research. Cluster 3 follows ‚ÄòOedipus‚Äô pattern rise end, indicating ending characterised positive valence; cluster 8 follows ‚ÄòCinderella‚Äô narrative fall sentiment indicating ending characterised negative valence. Del Vecchio et al. (2020) consider number acts shaping narrative form movies; results suggests look detail.interesting feature turning points trendlines often occur approximately quartiles normalised narrative time. example, trendline Avengers: Endgame (2019) cluster 3 falls beginning 26% normalised narrative time, peaks 51%, reaches lowest point 78% rising . Similar patterns can seen trendlines Jurassic World: Fallen Kingdom (2018) cluster 8 Lights (2016) cluster 3. Thompson (1999) showed four act structure common Hollywood narratives Cutting (2016) confirmed structure empirically based range formal features (editing, motion, luminance). Consequently, pattern show sentiments dialogue Hollywood films perhaps expected; nonetheless, surprising. x-axis Figure 9.12 represents running time film index sentence data set film.trendlines three turning points rather four. example, lowest point trendline Creed II cluster 7 occurs 35% peaks 69% falling . Little Women cluster 6, peak occurs 35% low 70% sentiments rises; Dr Strange cluster 4 sentiment rises 34% falls 74% rising . suggests films sample three-act structure based sentiments dialogue.cluster analysis sentiment subtitles raises interesting questions formal structure narratives Hollywood movies. films three-act structure others four-act structure? failure account number acts film mean develop model sentiment Hollywood films six classes narrative? fact three- four-act structure appears dialogue films imply running time narrative time correspond one another? sentiment structure film‚Äôs dialogue relate elements form style (cutting rate, motion, luminance) determine/reflect emotions structure? indicate emotional words dialogue tend cluster beginning /end acts film? pattern artefact methods used? simply coincidence?come across analysis sentiments related formal structure Hollywood movies answer questions Cutting (2016) consider dialogue sentiments analysis narrative form Hollywood movies. analysis sentiments dialogue Hollywood films answer questions; good example computational research can lead us frame questions form cinema otherwise consider.","code":"\n# Create some empty data frames to capture the outputs of the loop \n# for the fitted sentiments and the simplified macro shape\ndf_sentiments_complete <- data.frame()\ndf_sentiments_simplified <- data.frame()\n\nfor(i in 1:length(df_films[[1]])){\n  \n  genre <- df_films[[1]][i]\n  film <- df_films[[2]][i]\n  subs <- df_films[[3]][i]\n  \n  # Extract sentences\n  sentences <- get_sentences(subs[[1]]$subtitle)\n  # Extract sentiments and identify sentiment words\n  sentiments <- sentiment(sentences)\n  \n  # Get the sentiments but suppress the plots\n  df <- sp(sentiments$sentiment, lps = 30, window = 0.05, plot = FALSE)\n  \n  # Gather the data for the smoothers\n  df_sentiment <- df[[1]] %>% mutate(genre = rep(genre, length(Sentence)),\n                      film = rep(film, length(Sentence))) %>%\n    relocate(film) %>%\n    relocate(genre)\n  \n  # Gather the data for the simplified macro shape\n  df_simplified <- df[[2]] %>%\n    mutate(genre = rep(genre, length(Simplified)),\n           film = rep(film, length(Simplified))) %>%\n    relocate(film) %>%\n    relocate(genre) \n    \n  df_sentiments_simplified <- rbind.data.frame(df_sentiments_simplified, df_simplified)\n  \n  df_sentiments_complete <- rbind.data.frame(df_sentiments_complete, df_sentiment)\n  \n}\nhead(df_sentiments_complete)##    genre    film Sentence Sentiment Rolling Mean       DCT      Loess\n## 1 action Aquaman        1 0.0000000           NA 0.3584839 0.05942312\n## 2 action Aquaman        2 0.0000000           NA 0.3588962 0.06148228\n## 3 action Aquaman        3 0.0000000           NA 0.3597174 0.06351809\n## 4 action Aquaman        4 0.0000000           NA 0.3609409 0.06553122\n## 5 action Aquaman        5 0.1889822           NA 0.3625569 0.06752230\n## 6 action Aquaman        6 0.0000000           NA 0.3645525 0.06949198\nhead(df_sentiments_simplified)##    genre    film Normalised_time Simplified\n## 1 action Aquaman               1  0.5136773\n## 2 action Aquaman               2  0.5189481\n## 3 action Aquaman               3  0.5293844\n## 4 action Aquaman               4  0.5447769\n## 5 action Aquaman               5  0.5648160\n## 6 action Aquaman               6  0.5890965\n# Create a wide version of the data frame\ndf_sentiments_simplified_wide <- df_sentiments_simplified %>%\n  select(-genre) %>%\n  pivot_wider(names_from = \"film\", values_from = Simplified) %>% \n  select(-Normalised_time) %>% \n  t() %>% \n  data.frame()\n\ndf_sentiments_simplified_wide[1:5, 1:4]##                                          X1        X2        X3        X4\n## Aquaman                           0.5136773 0.5189481 0.5293844 0.5447769\n## Avengers Age of Ultron            1.0000000 0.9897966 0.9695039 0.9393486\n## Avengers Endgame                  0.9449720 0.9307397 0.9024946 0.8606730\n## Avengers Infinity War             1.0000000 0.9901788 0.9706619 0.9416984\n## Batman v Superman Dawn of Justice 0.6570937 0.6596318 0.6646687 0.6721266\n# Load the fpc package\npacman::p_load(fpc)\n\nclara_res <- pamk(df_sentiments_simplified_wide, krange = 2:10, \n                  criterion = \"asw\", scaling = TRUE, usepam = FALSE, \n                  samples = 250, sampsize = 30, metric = \"euclid\")\n\n# Get the optimal number of clusters\nclara_res$nc## [1] 9\ndf_sentiments_simplified_wide <- df_sentiments_simplified_wide %>% \n  mutate(cluster = paste(\"Cluster \", clara_res$pamobject$clustering)) %>%\n  select(cluster) %>%\n  rownames_to_column(var = \"film\")\n\nhead(df_sentiments_simplified_wide)##                                film    cluster\n## 1                           Aquaman Cluster  1\n## 2            Avengers Age of Ultron Cluster  2\n## 3                  Avengers Endgame Cluster  3\n## 4             Avengers Infinity War Cluster  4\n## 5 Batman v Superman Dawn of Justice Cluster  5\n## 6                     Black Panther Cluster  6\ndf_sentiments_simplified <- df_sentiments_simplified %>%\n  inner_join(df_sentiments_simplified_wide, by = \"film\") %>%\n  mutate(genre = R.utils::capitalize(genre),\n         Simplified = round(Simplified, 4))\n\nhead(df_sentiments_simplified)##    genre    film Normalised_time Simplified    cluster\n## 1 Action Aquaman               1     0.5137 Cluster  1\n## 2 Action Aquaman               2     0.5189 Cluster  1\n## 3 Action Aquaman               3     0.5294 Cluster  1\n## 4 Action Aquaman               4     0.5448 Cluster  1\n## 5 Action Aquaman               5     0.5648 Cluster  1\n## 6 Action Aquaman               6     0.5891 Cluster  1\np_sentiments <- ggplot(data = df_sentiments_simplified) +\n  geom_line(aes(x = Normalised_time, y = Simplified, \n                colour = genre, group = film,\n                text = paste(\"Film: \", film,\n                             \"<br>Normalised narrative time: \", Normalised_time,\n                             \"<br>Scaled DCT: \", Simplified))) +\n  scale_x_continuous(name = \"Normalised time\") +\n  scale_y_continuous(name = \"Scaled DCT\") +\n  scale_colour_viridis_d(name = NULL, begin = 0, end = 0.8) +\n  facet_wrap(~cluster) +\n  theme_light() +\n  theme(strip.background = element_blank(),\n        panel.spacing.y = unit(15, \"pt\"),\n        strip.text.x = element_text(colour = \"black\"))\n\n# Make the plot interactive\nggplotly(p_sentiments, tooltip = \"text\") %>%\n  layout(legend = list(orientation = 'h', x = 0.3, y = -0.1,\n                       title = list(text = '<b>Genre<\/b>'),\n                       font = list(size = 11)))"},{"path":"texts.html","id":"practical","chapter":"9 Analysing lexical texts","heading":"9.5 Practical considerations","text":"different types lexical texts present us different opportunities limitations analysis.Screenplays rich source information containing dialogue, character information, action, though may require manual annotation. Parsing screenplays data objects challenging existing tools screenplay-pdf--json Python available automate task. Screenplays available websites Internet Movie Script Database Simply Scripts often drafts necessarily final version script. may contain material included released version motion picture excluded later draft editing stage. assumes copy screenplay available film television programme interested, may case.Subtitles easy work thanks availability packages quickly parse data structures suitable analysis, lack much key information available screenplay. contain timing information makes analysing temporal structure lexical texts straightforward lack information characters actions. Byszuk (2020) points limitations using subtitles. Subtitles may accurately reflect said movie dialogue may re-phrased order fit screen given amount time. Similarly, punctuation may adjusted facilitate reading viewer rather reflecting characters‚Äô patterns speech. Byszuk describes subtitling ‚Äòconventionalised art‚Äô standards determine subtitles produced script displayed, number lines line breaks, maximum number characters line, maximum display time subtitle, relationship subtitles edits. Open access subtitles may necessarily meet professional standards may reflect range subtitling practices consistent across subtitles sample.Using subtitles source opensubtitles.org raises issues quality control. Del Vecchio et al. (2020) based quality control using materials opensubtitles.org whether submitters identified Silver, Gold, Platinum ratings. However, appears rating awarded quantity submissions (e.g., Silver rating awarded individual submitted 51 subtitles website) quality. Quality opensubtitles.org appears indicated marking submitter Trusted, Sub-Translator, Admin. downloading subtitles opensubtitles.org chapter downloaded files named submitters marked either Trusted Admin, though even possible guarantee quality subtitles timings.place subtitles, Ho≈Çobut et al. (2016) Byszuk (2020) preferred work transcripts film television programmes accurately reflect spoken contain much information subtitles, including identity speakers scene descriptions, though typically contain less information screenplays. However, transcripts pose many problems subtitles. Coverage problem, transcripts readily available types films television programmes (cult television shows dedicated fanbase) others well served. Quality control issue professional conventions films transcribed. Consequently, type amount information available may vary transcript transcript, may formatting transcript making texts difficult parse computationally.Audio descriptions similar transcripts include material beyond dialogue, descriptions body language, expressions, movements, actions extra soundtrack, descriptive information inserted lines dialogue enhance cinematic experience visually impaired viewers. Like transcripts, also lack detail available screenplays. Audio descriptions intended exhaustive, focussing information important maintain flow narrative; therefore represent edited view film (despite required avoidance subjectivity). Audio descriptions emphasise description limit emotional content may omit much key information relevant understanding film. example, Brownie (2016) noted audio description scene Full Monty (1997) described actions screen straightforward manner convey emotional response character scene, maintaining neutral descriptive stance accordance published standards audio description. Audio descriptions may available audio file require transcription analysis lexical text.Plot summaries key part narrative image film circulates prior release used audiences select films view. contain much less information content film compared forms lexical texts often basis movie recommendation systems developed using machine learning (Gawinecki et al., 2021; Guehria et al., 2021). Additionally, machine learning algorithms also used generate movie summaries (Liu et al., 2021) movie trailers (Hesham et al., 2018) automatically lexical texts. impact plot summaries shaping audiences‚Äô expectations films therefore significant key area research despite limited information contain.","code":""},{"path":"texts.html","id":"summary-7","chapter":"9 Analysing lexical texts","heading":"9.6 Summary","text":"analysis cinema‚Äôs lexical texts largely overlooked film scholars treated film purely audio-visual medium (Ho≈Çobut & Rybicki, 2020) lags behind stylometric analysis novels, plays, speeches, even tweets, fewer analysts working screenplays, subtitles, transcripts, audio descriptions (date) fewer analyses published. examples Cinelytics, ScriptBook, Largo.AI show, analysis texts key part shift data-driven filmmaking. screenplay part film exist prior pre-production decisions films green-light production based assessments screenplay ‚Äì assessments increasingly made computationally.chapter covered stylometric methods exploring class lexical texts individual films groups films. learnt create tidy data set sample .srt files, applying regular expressions stringr package filter replace text. cleaned data set, used tidy text approach Silge & Robinson (2017) count word frequencies proportions example extending method look relationships among texts using heatmap dendrogram. Using sentimentr syuzhet packages performed sentiment analysis movie subtitles sample look similarities dialogue films different genres based frequency words assigned different emotion occur evolution valence sentiments course narrative time film. Crucially, learnt can edit R functions order adapt existing functions suit goals, thereby expanding scope can achieve applying computational methods analysis motion pictures.","code":""},{"path":"references.html","id":"references","chapter":"10 References","heading":"10 References","text":"","code":""},{"path":"references.html","id":"works-cited","chapter":"10 References","heading":"10.1 Works cited","text":"","code":""},{"path":"references.html","id":"r-packages","chapter":"10 References","heading":"10.2 R Packages","text":"","code":""},{"path":"colophon.html","id":"colophon","chapter":"Colophon","heading":"Colophon","text":"","code":""},{"path":"colophon.html","id":"version","chapter":"Colophon","heading":"Version","text":"version 0.9.010 Computational Film Analysis R published 24 November 2023.","code":""},{"path":"colophon.html","id":"change-log","chapter":"Colophon","heading":"Change log","text":"full change log book available .","code":""},{"path":"colophon.html","id":"source","chapter":"Colophon","heading":"Source","text":"source code book available https://github.com/DrNickRedfern/CFA--R","code":""},{"path":"colophon.html","id":"build","chapter":"Colophon","heading":"Build","text":"book built :","code":"## ‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n##  setting  value\n##  version  R version 4.2.1 (2022-06-23 ucrt)\n##  os       Windows 10 x64 (build 19045)\n##  system   x86_64, mingw32\n##  ui       RTerm\n##  language (EN)\n##  collate  English_United Kingdom.utf8\n##  ctype    English_United Kingdom.utf8\n##  tz       Europe/London\n##  date     2023-11-24\n##  pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n## \n## ‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n##  package       * version date (UTC) lib source\n##  askpass         1.1     2019-01-13 [1] CRAN (R 4.2.1)\n##  backports       1.4.1   2021-12-13 [1] CRAN (R 4.2.0)\n##  base64enc       0.1-3   2015-07-28 [1] CRAN (R 4.2.0)\n##  bit             4.0.5   2022-11-15 [1] CRAN (R 4.2.2)\n##  bit64           4.0.5   2020-08-30 [1] CRAN (R 4.2.1)\n##  blob            1.2.4   2023-03-17 [1] CRAN (R 4.2.3)\n##  broom           1.0.5   2023-06-09 [1] CRAN (R 4.2.3)\n##  bslib           0.5.1   2023-08-11 [1] CRAN (R 4.2.3)\n##  cachem          1.0.8   2023-05-01 [1] CRAN (R 4.2.3)\n##  callr           3.7.3   2022-11-02 [1] CRAN (R 4.2.2)\n##  cellranger      1.1.0   2016-07-27 [1] CRAN (R 4.2.1)\n##  cli             3.6.1   2023-03-23 [1] CRAN (R 4.2.3)\n##  clipr           0.8.0   2022-02-22 [1] CRAN (R 4.2.1)\n##  colorspace      2.1-0   2023-01-23 [1] CRAN (R 4.2.3)\n##  conflicted      1.2.0   2023-02-01 [1] CRAN (R 4.2.3)\n##  cpp11           0.4.6   2023-08-10 [1] CRAN (R 4.2.3)\n##  crayon          1.5.2   2022-09-29 [1] CRAN (R 4.2.1)\n##  curl            5.0.2   2023-08-14 [1] CRAN (R 4.2.1)\n##  data.table      1.14.8  2023-02-17 [1] CRAN (R 4.2.3)\n##  DBI             1.1.3   2022-06-18 [1] CRAN (R 4.2.1)\n##  dbplyr          2.3.3   2023-07-07 [1] CRAN (R 4.2.3)\n##  digest          0.6.33  2023-07-07 [1] CRAN (R 4.2.3)\n##  dplyr         * 1.1.2   2023-04-20 [1] CRAN (R 4.2.3)\n##  dtplyr          1.3.1   2023-03-22 [1] CRAN (R 4.2.3)\n##  ellipsis        0.3.2   2021-04-29 [1] CRAN (R 4.2.1)\n##  evaluate        0.21    2023-05-05 [1] CRAN (R 4.2.3)\n##  fansi           1.0.4   2023-01-22 [1] CRAN (R 4.2.3)\n##  farver        * 2.1.1   2022-07-06 [1] CRAN (R 4.2.1)\n##  fastmap         1.1.1   2023-02-24 [1] CRAN (R 4.2.3)\n##  fontawesome     0.5.1   2023-04-18 [1] CRAN (R 4.2.3)\n##  forcats       * 1.0.0   2023-01-29 [1] CRAN (R 4.2.3)\n##  fs              1.6.3   2023-07-20 [1] CRAN (R 4.2.3)\n##  gargle          1.5.2   2023-07-20 [1] CRAN (R 4.2.3)\n##  generics        0.1.3   2022-07-05 [1] CRAN (R 4.2.1)\n##  ggplot2       * 3.4.3   2023-08-14 [1] CRAN (R 4.2.1)\n##  glue            1.6.2   2022-02-24 [1] CRAN (R 4.2.1)\n##  googledrive     2.1.1   2023-06-11 [1] CRAN (R 4.2.3)\n##  googlesheets4   1.1.1   2023-06-11 [1] CRAN (R 4.2.3)\n##  gtable          0.3.3   2023-03-21 [1] CRAN (R 4.2.3)\n##  haven           2.5.3   2023-06-30 [1] CRAN (R 4.2.3)\n##  highr           0.10    2022-12-22 [1] CRAN (R 4.2.2)\n##  hms             1.1.3   2023-03-21 [1] CRAN (R 4.2.3)\n##  htmltools       0.5.6   2023-08-10 [1] CRAN (R 4.2.3)\n##  httr            1.4.7   2023-08-15 [1] CRAN (R 4.2.1)\n##  ids             1.0.1   2017-05-31 [1] CRAN (R 4.2.1)\n##  isoband         0.2.7   2022-12-20 [1] CRAN (R 4.2.2)\n##  jquerylib       0.1.4   2021-04-26 [1] CRAN (R 4.2.1)\n##  jsonlite        1.8.7   2023-06-29 [1] CRAN (R 4.2.3)\n##  knitr           1.42    2023-01-25 [1] CRAN (R 4.2.1)\n##  labeling        0.4.2   2020-10-20 [1] CRAN (R 4.2.0)\n##  lattice         0.20-45 2021-09-22 [2] CRAN (R 4.2.1)\n##  lifecycle       1.0.3   2022-10-07 [1] CRAN (R 4.2.1)\n##  lubridate     * 1.9.2   2023-02-10 [1] CRAN (R 4.2.3)\n##  magrittr      * 2.0.3   2022-03-30 [1] CRAN (R 4.2.1)\n##  MASS            7.3-60  2023-05-04 [1] CRAN (R 4.2.3)\n##  Matrix          1.6-1   2023-08-14 [1] CRAN (R 4.2.1)\n##  memoise         2.0.1   2021-11-26 [1] CRAN (R 4.2.1)\n##  mgcv            1.8-40  2022-03-29 [2] CRAN (R 4.2.1)\n##  mime            0.12    2021-09-28 [1] CRAN (R 4.2.0)\n##  modelr          0.1.11  2023-03-22 [1] CRAN (R 4.2.3)\n##  munsell         0.5.0   2018-06-12 [1] CRAN (R 4.2.1)\n##  nlme            3.1-163 2023-08-09 [1] CRAN (R 4.2.3)\n##  openssl         2.1.0   2023-07-15 [1] CRAN (R 4.2.3)\n##  pillar          1.9.0   2023-03-22 [1] CRAN (R 4.2.3)\n##  pkgconfig       2.0.3   2019-09-22 [1] CRAN (R 4.2.1)\n##  prettyunits     1.1.1   2020-01-24 [1] CRAN (R 4.2.1)\n##  processx        3.8.2   2023-06-30 [1] CRAN (R 4.2.3)\n##  progress        1.2.2   2019-05-16 [1] CRAN (R 4.2.1)\n##  ps              1.7.5   2023-04-18 [1] CRAN (R 4.2.3)\n##  purrr         * 1.0.2   2023-08-10 [1] CRAN (R 4.2.3)\n##  R6              2.5.1   2021-08-19 [1] CRAN (R 4.2.1)\n##  ragg            1.2.5   2023-01-12 [1] CRAN (R 4.2.1)\n##  rappdirs        0.3.3   2021-01-31 [1] CRAN (R 4.2.1)\n##  RColorBrewer    1.1-3   2022-04-03 [1] CRAN (R 4.2.0)\n##  readr         * 2.1.4   2023-02-10 [1] CRAN (R 4.2.3)\n##  readxl          1.4.3   2023-07-06 [1] CRAN (R 4.2.3)\n##  rematch         1.0.1   2016-04-21 [1] CRAN (R 4.2.1)\n##  rematch2        2.1.2   2020-05-01 [1] CRAN (R 4.2.1)\n##  reprex          2.0.2   2022-08-17 [1] CRAN (R 4.2.1)\n##  rlang           1.1.1   2023-04-28 [1] CRAN (R 4.2.3)\n##  rmarkdown       2.24    2023-08-14 [1] CRAN (R 4.2.1)\n##  rstudioapi      0.15.0  2023-07-07 [1] CRAN (R 4.2.3)\n##  rvest           1.0.3   2022-08-19 [1] CRAN (R 4.2.1)\n##  sass            0.4.7   2023-07-15 [1] CRAN (R 4.2.3)\n##  scales          1.2.1   2022-08-20 [1] CRAN (R 4.2.1)\n##  selectr         0.4-2   2019-11-20 [1] CRAN (R 4.2.1)\n##  stringi         1.7.12  2023-01-11 [1] CRAN (R 4.2.2)\n##  stringr       * 1.5.0   2022-12-02 [1] CRAN (R 4.2.2)\n##  sys             3.4.2   2023-05-23 [1] CRAN (R 4.2.3)\n##  systemfonts     1.0.4   2022-02-11 [1] CRAN (R 4.2.1)\n##  textshaping     0.3.6   2021-10-13 [1] CRAN (R 4.2.1)\n##  tibble        * 3.2.1   2023-03-20 [1] CRAN (R 4.2.3)\n##  tidyr         * 1.3.0   2023-01-24 [1] CRAN (R 4.2.3)\n##  tidyselect      1.2.0   2022-10-10 [1] CRAN (R 4.2.2)\n##  tidyverse     * 2.0.0   2023-02-22 [1] CRAN (R 4.2.3)\n##  timechange      0.2.0   2023-01-11 [1] CRAN (R 4.2.2)\n##  tinytex         0.46    2023-08-09 [1] CRAN (R 4.2.3)\n##  tzdb            0.4.0   2023-05-12 [1] CRAN (R 4.2.3)\n##  utf8            1.2.3   2023-01-31 [1] CRAN (R 4.2.3)\n##  uuid            1.1-0   2022-04-19 [1] CRAN (R 4.2.0)\n##  vctrs           0.6.3   2023-06-14 [1] CRAN (R 4.2.3)\n##  viridisLite   * 0.4.2   2023-05-02 [1] CRAN (R 4.2.3)\n##  vroom           1.6.3   2023-04-28 [1] CRAN (R 4.2.3)\n##  withr         * 2.5.0   2022-03-03 [1] CRAN (R 4.2.1)\n##  xfun            0.40    2023-08-09 [1] CRAN (R 4.2.3)\n##  xml2            1.3.5   2023-07-06 [1] CRAN (R 4.2.3)\n##  yaml            2.3.7   2023-01-23 [1] CRAN (R 4.2.3)\n## \n##  [1] C:/Users/nickr/AppData/Local/R/win-library/4.2\n##  [2] C:/Program Files/R/R-4.2.1/library\n## \n## ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"},{"path":"reader-statistics.html","id":"reader-statistics","chapter":"Reader statistics","heading":"Reader statistics","text":"page can access readership statistics book original 13 September 2022 day. Data updated every 12 hours. View data full screen mode clicking full screen icon (‚õ∂) bottom page.","code":""}]
